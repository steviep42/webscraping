# More Real Life Examples {#Moreexamples}

```{r loadlibs, message=FALSE,echo=FALSE}
library(rvest)
library(tidytext)
library(dplyr)
library(ggplot2)
library(rtweet)
library(tidyr)
library(wordcloud)
library(tidyquant)
```


Okay. This is a tour of some sites that will serve as important examples on how to parse sites. Let's check the price of bitcoins. You want to be rich don't you ? 

## BitCoin Prices


![](./PICS/bc4.png){}

The challenge here is that it's all one big table and it's not clear how to adress it. And the owners of the web site will ususally change the format or start using Javascript or HTML5 which will mess things up in the future. One solid approach I frequently use is to simply pull out all the tables and, by experimentation, try to figure out which one has the information I want. This always require some work.

```{r bitcoin1}
library(rvest)
url <- "https://coinmarketcap.com/all/views/all/"
bc <- read_html(url)

bc_table <- bc %>% 
  html_nodes('table') %>% 
  html_table() %>% .[[3]]
 # We get back a one element list that is a data frame
 str(bc_table,0)
 bc_table <- bc_table[,c(2:3,5)]
 head(bc_table)
```

Everything is a character at this point so we have to go in an do some surgery on the data frame to turn the Price into an actual numeric.

```{r bitcoin2}
# The data is "dirty" and has characers in it that need cleaning
bc_table <- bc_table %>% mutate(Price=gsub("\\$","",Price))
bc_table <- bc_table %>% mutate(Price=gsub(",","",Price))
bc_table <- bc_table %>% mutate(Price=round(as.numeric(Price),2))

# There are four rows wherein the Price is missing NA
bc_table <- bc_table %>% filter(complete.cases(bc_table))

# Let's get the Crypto currencies with the Top 10 highest prices 
top_10 <- bc_table %>% arrange(desc(Price)) %>% head(10)
top_10
```

Let's make a barplot of the top 10 crypto currencies. 

```{r bitbar}
# Next we want to make a barplot of the Top 10
ylim=c(0,max(top_10$Price)+10000)
main="Top 10 Crypto Currencies in Terms of Price"
bp <- barplot(top_10$Price,col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

So that didn't work out so well since one of the crypto currencies dominates the others in terms of price. So let's create a log transformed verion of the plot.

```{r bitbarlog}
# Let's take the log of the price
ylim=c(0,max(log(top_10$Price))+5)
main="Top 10 Crypto Currencies in Terms of log(Price)"
bp <- barplot(log(top_10$Price),col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

## IMDB 

Look at this example from IMDb (Internet Movie Database). According to Wikipedia:

IMDb (Internet Movie Database)[2] is an online database of   information related to films, television programs, home    videos, video games, and streaming content online â€“ including cast, production crew and personal biographies, plot summaries, trivia, fan and critical reviews, and ratings. We can search or refer to specific movies by URL if we wanted. For example, consider the following link to the "Lego Movie":  http://www.imdb.com/title/tt1490017/ 

![](./PICS/legom.png)

In terms of scraping information from this site we could do that using the **rvest** package. Let's say that we wanted to capture the rating information which is 7.8 out of 10. We could use the xPath Tool or the Selector gadet tool to zone in on this information. According to selector gadget we have the following xpath expression:

![](./PICS/sg.png)

```{r}
url <- "http://www.imdb.com/title/tt1490017/"
lego_movie <- read_html(url)

# Scrape the website for the movie rating
rating <- lego_movie %>%
  html_nodes(".ratingValue span") %>%
  html_text() 
```

So that gives us what we need albeit in character form. Now it's a simple matter of parsing out the first rating value:

```{r}
(rating <- as.numeric(rating[1]))
```

That wasn't so bad. Let's see what using the xPath plugin gives us:

![](./PICS/xpp.png)

We get a much longer xpath expression which should provide us with direct access to the value. 

```{r}

xp <- "/html/body/div[3]/div/div[2]/div[5]/div[1]/div[2]/div/div[1]/div[2]/div/div[1]/div[1]/div[1]/strong/span"

# Scrape the website for the movie rating
rating <- lego_movie %>%
  html_nodes(xpath=xp) %>%
  html_text() %>% as.numeric()

rating
```

Let's access the summary section of the link. We could use Selector Gadget or the xPath plugin. I'll use the former.

![](./PICS/summary.png)

![](./PICS/sumxt.png)

```{r}
mov_summary <- lego_movie %>%
  html_nodes(".summary_text") %>%
  html_text() 

mov_summary
```


## Faculty Salaries

In this example we have to parse the main table associated with the results page. 

![Salary](.//PICS/salary.png)

![Salary](.//PICS/sal_page.png)

```{r sal1}
url <- "https://www.insidehighered.com/aaup-compensation-survey"
df <- read_html(url) %>% html_table() %>% `[[`(1)
intost <- c("Institution","Category","State")
salary <- df %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
salary
```


So we could process multiple pages


```{r sal2,eval=FALSE}
# So now we could process multiple pages

url <- 'https://www.insidehighered.com/aaup-compensation-survey?institution-name=&professor-category=1591&page=1'
str1 <- "https://www.insidehighered.com/aaup-compensation-survey?"
str2 <- "institution-name=&professor-category=1591&page="
intost <- c("Institution","Category","State")

salary <- data.frame()
for (ii in 1:2) {
  nurl  <- paste(str1,str2,ii,sep="")
  df <- read_html(nurl)
  tmp <- df %>% html_table() %>% `[[`(1)
  tmp <- tmp %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
  salary <- rbind(salary,tmp)
}

salary

```


Look at the URLs at the bottom of the main page to find beginning and ending page numbers. Visually this is easy. Programmatically we could  do something like the following:


```{r sal3, eval=FALSE}
# https://www.insidehighered.com/aaup-compensation-survey?page=1
# https://www.insidehighered.com/aaup-compensation-survey?page=94

# What is the last page number ? We already know the answer - 94

lastnum <- df %>% html_nodes(xpath='//a') %>% 
  html_attr("href") %>% '['(103) %>%
  strsplit(.,"page=") %>% '[['(1) %>% '['(2) %>% as.numeric(.)

# So now we could get all pages of the survey

str1 <- "https://www.insidehighered.com/aaup-compensation-survey?"
str2 <- "institution-name=&professor-category=1591&page="
intost <- c("Institution","Category","State")

salary <- data.frame()

for (ii in 1:lastnum) {
  nurl  <- paste(str1,str2,ii,sep="")
  df <- read_html(nurl)
  tmp <- df %>% html_table() %>% `[[`(1)
  tmp <- tmp %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
  salary <- rbind(salary,tmp)
  Sys.sleep(1)
}
names(salary) <- c("Institution","Category","State","AvgSalFP","AvgChgFP",
                   "CntFP","AvgTotCompFP","SalEquityFP")

salary <- salary %>% 
  mutate(AvgSalFP=as.numeric(gsub("\\$|,","",salary$AvgSalFP))) %>%
  mutate(AvgTotCompFP=as.numeric(gsub("\\$|,","",salary$AvgTotCompFP)))

salary %>% group_by(State,Category) %>% 
  summarize(avg=mean(AvgSalFP)) %>% 
  arrange(desc(avg))
```

There are some problems:

* Data is large and scattered across multiple pages
* We could use above techniques to move from page to page
* There is a form we could use to narrow criteria
* But we have to programmatically submit the form
* rvest (and other packages) let you do this

## Filling Out Forms From a Program

![Salary](./PICS/salary_form.png){width=250px}

Let's find salaries between $ 150,000 and the default max ($ 244,000)

* Find the element name associated with "Average Salary"
* Establish a connection with the form (usually the url of the page)
* Get a local copy of the form
* Fill in the value for the "Average Salary"
* Submit the  lled in form
* Get the results and parse them like above
`

So finding the correct element is more challenging. I use Chrome to do this. Just highlight the area over the form and right click to "Insepct" the element. This opens up the developer tools. You have to dig down to find the corrext form and the element name. Here is a screen shot of my activity:

![Salary](./PICS/form.png){width=550px}

```{r sal4,eval=FALSE}

url <- "https://www.insidehighered.com/aaup-compensation-survey"

# Establish a session
mysess <- html_session(url)

# Get the form

form_unfilled <- mysess %>% html_node("form") %>% html_form()
form_filled   <- form_unfilled %>% set_values("range-from"=150000)

# Submit form

results <- submit_form(mysess,form_filled)
first_page <- results %>% html_nodes(xpath=expr) %>% html_table()

first_page
```


## PubMed

Pubmed provides a rich source of information on published scientific literature. There are tutorials on how to leverage its capabilities but one thing to consider is that MESH terms are a good starting place since the search is index-based. MeSH (Medical Subject Headings) is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed. It's faster and more accurate so you can first use the MESH browser to generate the appropriate search terms and add that into the Search interface. The MESH browser can be found at https://www.ncbi.nlm.nih.gov/mesh/

![](./PICS/pubmed1.png){width=550px}


![](./PICS/mesh.png){width=550px}

What we do here is get the links associated with each publication so we can then process each of those and get the abstract associated with each publication. 

```{r hemo}

# "hemodialysis, home" [MeSH Terms] 

url<-"https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D"

# 
# The results from the search will be of the form:
# https://www.ncbi.nlm.nih.gov/pubmed/30380542

results <- read_html(url) %>% 
  html_nodes("a") %>% 
  html_attr("href") %>%
  grep("/pubmed/[0-9]{1,6}",.,value=TRUE) %>% unique(.)

results
```

![](./PICS/pubmed_results.png){width=550px}

So now we could loop through these links and get the abstracts for these results. It looks that there are approximately 20 results per page. As before we would have to dive in to the underlying structure of the page to get the correct HTML pathnames or we could just look for Paragraph elements and pick out the links that way. 

```{r abs1}

text.vec <- vector()

for (ii in 1:length(results)) {
  string <- paste0("https://www.ncbi.nlm.nih.gov",results[ii])
  text.vec[ii] <- read_html(string) %>% html_nodes("p") %>% `[[`(11) %>% html_text()
}

# Eliminate lines with newlines characters

final.vec <- text.vec[grep("^\n",text.vec,invert=TRUE)]

final.vec
```


Well that was tedious. And we processed only the first page of results. How do we "progrmmatically" hit the "Next" Button at the bottom of the page ? This is complicated by the fact that there appears to be some Javascript at work that we would have to somehow interact with to get the URL for the next page. Unlike with the school salary example it isn't obvious how to do this. If we hove over the "Next" button we don't get an associated link.

