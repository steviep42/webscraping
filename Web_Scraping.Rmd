--- 
title: "Web Scraping with R"
author: "Steve Pittard"
date: "`r Sys.Date()`"
bibliography:
- book.bib
- packages.bib
description: This is in support of my talk for the Data Science group
documentclass: book
link-citations: yes
site: bookdown::bookdown_site
biblio-style: apalike
---


# Motivations



## Lots of Data For The Taking ? 

The web hosts lots of interesting data that you can ”scrape”. Some of it is stashed in data bases, behind APIs, or in free form text. Lots of people want to grab information of of Twitter or from user forums to see what people are thinking. There is a lot of valuable information out there for the taking although some web sites have "caught on" and either block programmatic access or they setup "pay walls" that require you to subscribe to an API for access. The New York Times does this. But there are lots of opportunities to get data. 

![](./PICS/ws.png)

 | 
------|--------------------------------
tables|Fetch tables like from Wikipedia
forms|You can submit forms and fetch the results
css|You can access parts of a web site using style or css selectors
Tweets|Process tweets including emojis
Web Sites| User forums have lots of content
Instagram| Yes you can "scrape" photos also


## Web Scraping Can Be Ugly

Depending on what web sites you want to scrape the process can be involved and quite tedious. Many websites are very much aware that people are scraping so they offer Application Programming Interfaces (APIs) to make requests for information easier for the user and easier for the server administrators to control access. Most times the user must apply for a "key" to gain access. 

For premium sites, the key costs money. Some sites like Google and Wunderground (a popular weather site) allow some number of free accesses before they start charging you. Even so the results are typically returned in XML or JSON which then requires you to parse the result to get the information you want. In the best situation there is an R package that will wrap in the parsing and will return lists or data frames. 

Here is a summary:

* First. Always try to find an R package that will access a site (e.g. New York Times, Wunderground, PubMed). These packages (e.g. omdbapi, easyPubMed, RBitCoin, rtimes) provide a programmatic search interface and return data frames with little to no effort on your part. 

* If no package exists then hopefully there is an API that allows you to query the website and get results back in JSON or XML. I prefer JSON because it's "easier" and the packages for parsing JSON return lists which are native data structures to R. So you can easily turn results into data frames. You will ususally use the *rvest* package in conjunction with XML, and the RSJONIO packages. 

* If the Web site doesn't have an API then you will need to scrape text. This isn't hard but it is tedious. You will need to use *rvest* to parse HMTL elements. If you want to parse mutliple pages then you will need to use *rvest* to move to the other pages and possibly fill out forms. If there is a lot of Javascript then you might need to use RSelenium to programmatically manage the web page. 

## Understanding The Language of The Web

The Web has its own languages: HTML, CSS, Javascript

```{r eval=FALSE}
<h1>, <h2>, ..., <h6> Heading 1 and so on
<p> Paragraph elements
<ul> Unordered List
<ol> Ordered List
<li> List Element
<div> Division / Section
<table> Tables
<form> Web forms
```

So to be productive at scraping requires you to have some familiarity with HMTL XML, and CSS. Here we look at a very basic HTML file. Refer to See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html
 for a basic introductory session on HTML and webscraping with R


```{r eval=FALSE}
<!DOCTYPE html>
<html>
  <body>
    <h1>My First Heading</h1>
    <p>My first paragraph.</p>
   </body>
</html>
```

![](./PICS/first_heading.png){width=450px}

\   

And you could apply some styling to this courtest of the CSS language which allows you to inject styles into plain HTML:

![](.//PICS/css.png){width=450px}

\  

![](.//PICS/css_page.png){width=450px}

### Useful tools 

There are a number of tools that allow us to inspect web pages and see "what is under the hood". Warning - I just discovered that one of my favorite browser plugins (firebug) to find the xpaths and/or css paths of page elements is no longer supported under Firefox or Chrome. I've found a couple of replacements but they don't work as well. I'll research it more. The way that **Selector Gadget** and **xPath** work is that you install them into your browswer and then activate them whenever you need to identify the **selector** associated with a part of a web page. 

\   

| 
------|--------------------------------
Selector Gadget | http://selectorgadget.com/
Firebug | https://getfirebug.com/ (now integrated into a version of Firefox)
xPath | https://addons.mozilla.org/en-US/firefox/addon/xpath_finder/ 
Google Chrome | Right click to inspect a page element 
Google Chrome | View Developer - Developer Tools
Oxygen Editor | Can obtain via the Emory Software Express Site

## Useful Packages

You will use the following three primary packages to help you get data from various web pages: *rvest*, *XML*, and *RJSONIO*. Note that you won't always use them simultaneously but you might use them in pairs or individually depending on the task at hand. 

## Quick **rvest** tutorial

Now let's do a quick *rvest* tutorial. There are several steps involved in using **rvest** which are conceptually quite straightforward:

1) Identify a URL to be examined for content
2) Use Selector Gadet, xPath, or Google Insepct to identify the "selector"
   This will be a paragraph, table, hyper links, images
3) Load rvest
4) Use **read_html** to "read" the URL
5) Pass the result to **html_nodes** to get the selectors identified in step number 2
6) Get the text or table content



```{r}
library(rvest)
url <- "https://en.wikipedia.org/wiki/World_population"

(paragraphs <- read_html(url) %>% html_nodes("p"))

```


Then we might want to actually parse out those paragraphs into text:

```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
paragraphs <- read_html(url) %>% html_nodes("p") %>% html_text()
paragraphs[1:10]
```


Get some other types of HTML obejects. Let's get all the hyperlinks to other pages

```{r}

read_html(url) %>% html_nodes("a") 
```


What about tables ? 


```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
tables <- read_html(url) %>% html_nodes("table") 
tables
```


## Example: Parsing A Table From Wikipedia

Look at the [Wikipedia Page](https://en.wikipedia.org/wiki/World_population) for world population:

https://en.wikipedia.org/wiki/World_population

* We can get any table we want using rvest
* We might have to experiment to figure out which one 
* Get the one that lists the ten most populous countries 
* I think this might be the 4th or 5th table on the page
* How do we get this ?

![](./PICS/worldpop.png){width=450px}


First we will load packages that will help us throughout this session.

```{r message=F, echo=FALSE}
library(rvest)
library(tidytext)
library(dplyr)
library(ggplot2)
library(rtweet)
library(tidyr)
library(wordcloud)
library(tidyquant)
```

In this case we'll need to figure out what number table it is we want. We could fetch
all the tables and then experiment to find the precise one. 

```{r}
library(rvest)
library(tidyr)
library(dplyr)
library(ggplot2)

# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 
  
ten_most_populous <- ten_most_df %>% 
  html_nodes("table") %>% `[[`(6) %>% html_table()

# Let's get just the first three columns
ten_most_populous <- ten_most_populous[,2:4]

# Get some content - Change the column names
names(ten_most_populous) <- c("Country_Territory","Population","Date")

# Do reformatting on the columns to be actual numerics where appropriate
ten_most_populous %>% 
  mutate(Population=gsub(",","",Population)) %>% 
  mutate(Population=round(as.numeric(Population)/1e+06))  %>%
  ggplot(aes(x=Country_Territory,y=Population)) + geom_point() + 
  labs(y = "Population / 1,000,000") + coord_flip() +
  ggtitle("Top 10 Most Populous Countries")

```

In the above example we leveraged the fact that we were looking specifically for a table element and it became a project to locate the correct table number. This isn't always the case with more complicated websites in that the element we are trying to grab or scrape is contained within a nested structure that doesn't correspond neatly to a paragraph, link, heading, or table. This can be the case if the page is heavily styled with CSS or Javascript. We might have to work harder. But it's okay to try to use simple elements and then try to refine the search some more.


```{r}
# Could have use the xPath plugin to help

url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 
  
ten_most_populous <- ten_most_df %>% 
  html_nodes(xpath="/html/body/div[3]/div[3]/div[4]/div/table[5]") %>% html_table()
```


## Scraping Patient Dialysis Stories

Here is an example relating to the experiences of dialysis patients with a specific dialysis provider. It might be more useful to find a support forum that is managed by dialysis patients to get more general opinions but this example is helpful in showing you what is involved. Check out this website:

```{r eval=FALSE}
https://www.americanrenal.com/dialysis-centers/patient-stories

```

![](./PICS/dia1.png){}


### Getting More Detail 

In looking at this page you will see that there are a number of patient stories. Actually, there is a summary line followed by a "Read More" link that provides more detail on the patient experience. Our goal is to get the full content as opposed to only the summary. How would we do this ? 


![](./PICS/dia2.png){}


![](./PICS/dia3.png){width=550}

### Writing Some Code 

Let's use our new found knowledge of **rvest** to help us get these detailed stories. Maybe we want to do some sentiment analysis on this. If you hover over the **Read More** link on the website it will provide a specific link for each patient. For example, 

```{r john,eval=FALSE}
https://www.americanrenal.com/dialysis-centers/patient-stories/john-baguchinsky
```




What we want to do is first get a list of all these links from the main page after which we can loop over each of the patient specific links and capture that information into a vector. Each element of the vector will be the content of a specific patient's story. 


```{r dialysis}
library(rvest)

burl <- "https://www.americanrenal.com/dialysis-centers/patient-stories"

# Setup an empty vector to which we will add the content of each story
workVector <- vector()


# Grab the links from the site that relate patient stories
  
  links <- read_html(burl) %>% 
    html_nodes("a") %>% 
    html_attr("href") %>% 
    grep("stories",.,value=TRUE)
  
links


```


Some of these links do not correspond directly to a specific patient name so we need to filter those out. 


```{r}
# Get only the ones that seem to have actual names associated with them

 storiesLinks <- links[-grep("stories$",links)] 

 storiesLinks
```


Next we will visit each of these pages and scrape the text information. We'll step through this in class so you can see this in action but here is the code. We will get each story and place each paragrpah of the story into a vector element. After that we will eliminate blank lines and some junk lines that begin with a new line character. Then we will collapse all of the vector text into a single paragraph and store it into a list element. Let's step through it for the first link.


```{r}
# This corresponds to the first link
# "http://www.americanrenal.com/dialysis-centers/patient-stories/randal-beatty" 

tmpResult <- read_html(storiesLinks[1]) %>% 
                       html_nodes("p") %>% html_text()

tmpResult
```

Okay, that has some junk in it like blank lines and lines that begin with new line characters. 

```{r}
# Get rid of elements that are a blank line
    
    tmpResult <- tmpResult[tmpResult!=""]
  
  # Get rid of elements that begin with a newline character "\n"
    
    newlines_begin <- sum(grepl("^\n",tmpResult))

    if (newlines_begin > 0) {
      tmpResult <- tmpResult[-grep("^\n",tmpResult)]
    }

    tmpResult
```

Next, let's create a more compact version of the data. We'll cram it all into a single element. 


```{r}
(tmpResult <- paste(tmpResult,collapse=""))
```


So we could put this logic into a loop and process each of the links programmatically. 


```{r}
 # Now go to these pages and scrape the text necessary to
  # build a corpus

  tmpResult <- vector()
  textList <- list()
  
  for (ii in 1:length(storiesLinks)) {
    tmpResult <- read_html(storiesLinks[ii]) %>% 
                       html_nodes("p") %>% html_text()
  
  # Get rid of elements that are a blank line
    
    tmpResult <- tmpResult[tmpResult!=""]
  
  # Get rid of elements that begin with a newline character "\n"
    
    newlines_begin <- sum(grepl("^\n",tmpResult))

    if (newlines_begin > 0) {
      tmpResult <- tmpResult[-grep("^\n",tmpResult)]
    }
      
    # Let's collpase all the elements into a single element and then store
    # it into a list element so we can maintain each patient story separately
    # This is not necessary but until we figure out what we want to do with
    # the data then this gives us some options
    
    tmpResult <- paste(tmpResult,collapse="")
    textList[[ii]] <- tmpResult
    
  }
```

If we did our job correctly then each element of the **textList** will have text in it corresponding to each patient


```{r}
textList[[1]]
```



## Summary

* Need some basic HTML and CSS knowledge to find correct elements
* How to extract text from common elements
* How to extract text from specific elements
* Always have to do some text cleanup of data
* It usually takes multiple times to get it right

See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html


<!--chapter:end:index.Rmd-->

# XML and JSON {#xml}

This is where things get a little dicey because some web pages will return XML and JSON in response to inquiries and while these formats seem complicated they are actually doing you a really big favor by doing this since these formats can ususally be easily parsed using various packges. XML is a bit hard to get your head around and JSON is the new kid on the block which is easier to use. 

Since this isn't a full-on course lecture I'll keep it short as to how and why you would want to use these but any time you spend trying to better understand JSON (and XML) the better of you will be when parsing web pages. It's not such a big deal if all you are going to be parsing is raw text since the mthods we use to do that avoid XML and JSON although cleaning up raw text has its own problems. Let's revisit the Wikipedia example from the previous section.

```{r wiki}
library(rvest)

# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 
  
ten_most_populous <- ten_most_df %>% 
  html_nodes("table") %>% `[[`(6) %>% html_table()
```

![](./PICS/xml.png){width=650px}

![](./PICS/xmls.png){width=450px}


Let's look at an XML file that has some basic content:
\   

```{r xmlchunk, eval=FALSE}
<?xml version="1.0" encoding="UTF-8"?>
<bookstore>
  <book category="COOKING">
    <title lang="en">Everyday Italian</title>
    <author>Giada De Laurentiis</author>
    <year>2005</year>
    <price>30.00</price>
  </book>
  <book category="CHILDREN">
    <title lang="en">Harry Potter</title>
    <author>J K. Rowling</author>
    <year>2005</year>
    <price>29.99</price>
  </book>
  <book category="WEB">
    <title lang="en">Learning XML</title>
    <author>Erik T. Ray</author>
    <year>2003</year>
    <price>39.95</price>
  </book>
</bookstore>

```
Well we pulled out all tables and then, by experimentation, we isolated table 6 and got the content corresponding to that. But. Is there a more direct way to find the content ? There is. It requires us to install some helper plugins such as the xPath Finder for Firefox and Chrome. In reality there are a number of ways to find the XML Path or CSS Path for an element within a web page but this is a good one to start. 



![](./PICS/xpath_finder.png){width=350px}

Remeber that we want to find the table corresponding to the "10 Most Populous Countries". So we activate the *xPath* finder plugin and the highlight the element of interest. This takes some practice to get it right. Once you highlight the desired elment you will see the corresponding XPATH. Here is a screenshot of what I did. We can use the resulting path to directly access the table without first having to first pull out all tables and then trying to find the right one

![](./PICS/xpath_pop.png){width=350px}

\   
 
```{r populous}
# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_populous <- read_html(url) 

ten_most_df <- ten_most_populous %>% 
  html_nodes(xpath='/html/body/div[3]/div[3]/div[4]/div/table[5]') %>%
  html_table() 

# We have to get the first element of the list. 
ten_most_df <- ten_most_df[[1]]

ten_most_df
```

## Finding XPaths

![](./PICS/xml_path_1.png){width=650px}

\   

In addition to Browser Plugins there are standalone tools such as the Oxygen XML Editor which is availabel through the Emory Software Express Website. This is a comprehensive XML editor that will allow you to parse XML and develop paths to locate specific nodes within an XML document. If you find yourself working with websites with lots of XML then this will be useful. The Oxygen editor is free. 

![](./PICS/xml_path_10.png){width=350px}

Let's look at an XML file that has some basic content:
\   

```{r book, eval=FALSE}
<?xml version="1.0" encoding="UTF-8"?>
<bookstore>
  <book category="COOKING">
    <title lang="en">Everyday Italian</title>
    <author>Giada De Laurentiis</author>
    <year>2005</year>
    <price>30.00</price>
  </book>
  <book category="CHILDREN">
    <title lang="en">Harry Potter</title>
    <author>J K. Rowling</author>
    <year>2005</year>
    <price>29.99</price>
  </book>
  <book category="WEB">
    <title lang="en">Learning XML</title>
    <author>Erik T. Ray</author>
    <year>2003</year>
    <price>39.95</price>
  </book>
</bookstore>

```


![](./PICS/xml_path_2.png){width=650px}

\   

![](./PICS/xml_path_3.png){width=550px}


\  

![](./PICS/xml_path_4.png){width=550px}

\  

![](./PICS/xml_path_4.png){width=550px}

\  

![](./PICS/xml_path_6.png){width=550px}

![](./PICS/xml_path_7.png){width=550px}


![](./PICS/xml_path_8.png){width=550px}

![](./PICS/xml_path_9.png){width=550px}


## Example: GeoCoding With Google 

Let's run through an example of using the GeoCoding API with Google. They used to provide free access to this service but no more. You have to sign up for an account and get an API key. If you are currently taking one of my classes I probably have arranged for cloud credits that you can use to do Google Geocoding for free. 

![](./PICS/google_api2.png){}

So one way to do this is to create a URL according to the specification given in the Google Geocoding documentation. We need 1) the base Google URL for the Geocoding service, 2) the format of the desired output (XML or JSON), 3) and address for which we want to find the latitude and longitude, and 4) the API key we create at the Google API service. Here is a fully functional URL you can paste into your browser:

https://maps.googleapis.com/maps/api/geocode/xml?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyDPwt1Ya79b7lhsZkh75BjCz-GpMKC9ZYw

If you paste this into Chome then you get something back like this:

![](./PICS/geocode1.png){width=550x}


So I could create an R function to take care of this kind of thing so I could maybe pass in arbitrary addressess to be geocided. Let's run through this example and then look at how I parsed the XML file that is returned by the Google GeoCoding API. We will stick with the *1510 Clifton Rd, Atlanta, GA* address which corresponds to the Rollins Research Building. 

First we will see an example of what Google returns in terms of XML. We can use some tools like Oxygen Editor (available free via Emory Software Express) to develop an appropriate XPATH expression to parse out the latitude and longitude information. 

```{r mygeo, message=FALSE, eval=FALSE}

# https://maps.googleapis.com/maps/api/geocode/xml?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyDPwt1Ya79b7lhsZkh75BjCz-GpMKC9ZYw

# https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyDPwt1Ya79b7lhsZkh75BjCz-GpMKC9ZYw

myGeo <- function(address="1510 Clifton Rd Atlanta GA",form="xml") {
  library(XML)
  library(RCurl)
  
  geourl <- "https://maps.googleapis.com/maps/api/geocode/"
  
  # You will need to replace this with your OWN key !
  key <- "AIzaSyA3ereIVEjA0gPrxLupTPLO3GH_v98KpMA"
  
  address <- gsub(" ","+",address)
  
  add <- paste0(geourl,form,sep="?address=")
  add <- paste0(add,address,"&key=")
  geourl <- paste0(add,key)
  
  locale <- getURL(geourl)
  plocal <- xmlParse(locale,useInternalNodes=TRUE)
  
  # Okay let's extract the lat and lon
  latlon <- getNodeSet(plocal,"/GeocodeResponse/result/geometry/location/descendant::*")
  lat  <- as.numeric(xmlSApply(latlon,xmlValue))[1]
  lon  <- as.numeric(xmlSApply(latlon,xmlValue))[2]
  
  return(c(lat=lat,lng=lon))  
}
mylocs <- myGeo()
```

```{r eval=FALSE}
lat       lng 
 33.79667 -84.32319 
```


Now. We could have saved the report to a file on our local computer and open it up with Oxygen editor and figure out what the approproate XPATH would be. This is basically what I did. Here is a screenshot of the session. I picked an XPATH expression of //location/descendant::*



![](./PICS/oxygen2.png){width=650px}

\  
We could expand this considerable to process a number of addresses. This is a great example of how once you get a single example working then you can generalize this into a function that will allow you to do the same thing for a much larger numnber of addressess. 

```{r echo=FALSE}
cities  <- readRDS(file="cities.rds")
```

```{r geoapply, message=FALSE, eval=FALSE}
namevec <- c("Atlanta GA", 
             "Birmingham AL", 
             "Seattle WA", 
             "Sacramento CA",
             "Denver CO", 
             "LosAngeles CA", 
             "Rochester NY")

cityList <- lapply(namevec,myGeo,eval=FALSE)

# Or to get a data frame

cities <- data.frame(city=namevec,do.call(rbind,cityList),
                     stringsAsFactors = FALSE)
```



```{r}
cities

# Let's create a Map

library(leaflet)
m <- leaflet(data=cities)
m <- addTiles(m)
m <- addMarkers(m,popup=cities$city)

# Put up the Map - click on the markers
m
```


## Using JSON 

JSON is fast becoming the primary interchange format over XML although XML is still well supported. R has a number of packages to ease the parsing of JSON/ documents returned by web pages. Ususally you get back a *list* which is a native data type in R that can easily be manipulated into a data frame. Most web APIs provide an option for JSON or XML although some only provide JSON. 


![](./PICS/json.png){width=650px}

There are rules and regulations about how JSON is formed and we will learn them by example but you can look at the numerous tutotorials on the web to locate definitive references. See http://www.w3schools.com/json/  Here is an XML file that describes some employees. 

```{r employee, eval=FALSE}
<employees>
     <employee>
         <firstName>John</firstName>
         <lastName>Doe</lastName>
     </employee>
     <employee>
         <firstName>Anna</firstName>
         <lastName>Smith</lastName>
     </employee>
     <employee>
         <firstName>Peter</firstName>
         <lastName>Jones</lastName>
     </employee>
 </employees>
```


And here is the corresposning JSON file:

```{r json, eval=FALSE}
{
  "employees":[
    {"firstName":"John", "lastName":"Doe"},
    {"firstName":"Anna", "lastName":"Smith"},
    {"firstName":"Peter","lastName":"Jones"}
] }
```


* It is important to note that the actual information in the document, things like city name, county name, latitude, and longitude are the same as they would be in the comparable XML document.

* JSON documents are at the heart of the NoSQL“database”called MongoDB

* JSON can be found within many webpages since it is closely related to JavaScript which is a language strongly related to web pages.

* JSON is very compact and lighweight which has made it a natural followon to XML so much so that it appears to be replacing XML


See http:..www.json.org/ for a full description of the specification

* An object is an unordered set of name/value pairs. An object begins with (left brace) and ends with (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma).

* An array is an ordered collection of values. An array begins with [ (left bracket) and ends with ] (right bracket). Values are separated by , (comma).

* A value can be a string in double quotes, or a number, or true or false or null, or an object or an array. These structures can be nested.

* A string is a sequence of zero or more Unicode characters, wrapped in double quotes, using backslash escapes. A character is represented as a single character string.


Do you remember the Google Geocoding example from before ? We can tell Google to send us back JSON instead of XML just by adjusting the URL accordingly:

```{r}
url <- "https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Rd+Atlanta+GA&key=AIzaSyD0zIyn2ijIqb7OKYTGnAnchXY7zt3VB9Y"
```


![](./PICS/json_google.png){width=650px}

## Using the RJSONIO Package

To read/parse this in R we use a package called RJSONIO. There are other packages but this is the one we will be using. Download and install it.

There is a function called fromJSON which will parse the JSON file and return a list to contain the data.

So we parse lists instead of using XPath. Many people feel this to be easier than trying to construct XPath statments. You will have to decide for yourself.

```{r echo=FALSE}
geo <- readRDS(file="geo.rds")
```


```{r eval=FALSE}
library(RJSONIO)

url <- "https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyD0zIyn2ijIqb7OKYTGnAnchXY7zt3VB9Y"
geo <- fromJSON(url)

```

Since what we get back is a list we can directly access whatever we want. We just index into the list. No need for complicated XPATHS.

```{r}
str(geo,3)
```


```{r georesult}
geo$results[[1]]$geometry$location
```

Let's put this into a function that helps us get the information for a number of addresses

```{r eval=FALSE}
myGeo <- function(address="1510 Clifton Rd Atlanta GA",form="json") {
  library(RJSONIO)
  
  geourl <- "https://maps.googleapis.com/maps/api/geocode/"
  
  # You will need to replace this with your OWN key !
  key <- "AIzaSyA3ereIVEjA0gPrxLupTWRPOLH_v89DpMA"
  
  address <- gsub(" ","+",address)
  
  add <- paste0(geourl,form,sep="?address=")
  add <- paste0(add,address,"&key=")
  geourl <- paste0(add,key)
  
  geo <- fromJSON(geourl)
  
  
  lat  <- geo$results[[1]]$geometry$location[1]
  lng  <- geo$results[[1]]$geometry$location[2]
  
  return(c(lat,lng))  
}
```


Consider the following:


```{r geoapplyjson, message=FALSE, eval=FALSE}
namevec <- c("Atlanta GA", 
             "Birmingham AL", 
             "Seattle WA", 
             "Sacramento CA",
             "Denver CO", 
             "LosAngeles CA", 
             "Rochester NY")

cityList <- lapply(namevec,myGeo)

# Or to get a data frame

cities <- data.frame(city=namevec,do.call(rbind,cityList),
                     stringsAsFactors = FALSE)

```

Now we can check out the geocoding cities and then make a map

```{r echo=FALSE}
cities  <- readRDS(file="cities.rds")
```


```{r}
cities
# Let's create a Map

library(leaflet)
m <- leaflet(data=cities)
m <- addTiles(m)
m <- addMarkers(m,popup=cities$city)

# Put up the Map - click on the markers
m
```


<!--chapter:end:01-xmljson.Rmd-->

# More Real Life Examples {#Moreexamples}

```{r loadlibs, message=FALSE,echo=FALSE}
library(rvest)
library(tidytext)
library(dplyr)
library(ggplot2)
library(rtweet)
library(tidyr)
library(wordcloud)
library(tidyquant)
```


Okay. This is a tour of some sites that will serve as important examples on how to parse sites. Let's check the price of bitcoins. You want to be rich don't you ? 

## BitCoin Prices


![](./PICS/bc4.png){}

The challenge here is that it's all one big table and it's not clear how to adress it. And the owners of the web site will ususally change the format or start using Javascript or HTML5 which will mess things up in the future. One solid approach I frequently use is to simply pull out all the tables and, by experimentation, try to figure out which one has the information I want. This always require some work.

```{r bitcoin1}
library(rvest)
url <- "https://coinmarketcap.com/all/views/all/"
bc <- read_html(url)

bc_table <- bc %>% 
  html_nodes('table') %>% 
  html_table() %>% .[[3]]
 # We get back a one element list that is a data frame
 str(bc_table,0)
 bc_table <- bc_table[,c(2:3,5)]
 head(bc_table)
```

Everything is a character at this point so we have to go in an do some surgery on the data frame to turn the Price into an actual numeric.

```{r bitcoin2}
# The data is "dirty" and has characers in it that need cleaning
bc_table <- bc_table %>% mutate(Price=gsub("\\$","",Price))
bc_table <- bc_table %>% mutate(Price=gsub(",","",Price))
bc_table <- bc_table %>% mutate(Price=round(as.numeric(Price),2))

# There are four rows wherein the Price is missing NA
bc_table <- bc_table %>% filter(complete.cases(bc_table))

# Let's get the Crypto currencies with the Top 10 highest prices 
top_10 <- bc_table %>% arrange(desc(Price)) %>% head(10)
top_10
```

Let's make a barplot of the top 10 crypto currencies. 

```{r bitbar}
# Next we want to make a barplot of the Top 10
ylim=c(0,max(top_10$Price)+10000)
main="Top 10 Crypto Currencies in Terms of Price"
bp <- barplot(top_10$Price,col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

So that didn't work out so well since one of the crypto currencies dominates the others in terms of price. So let's create a log transformed verion of the plot.

```{r bitbarlog}
# Let's take the log of the price
ylim=c(0,max(log(top_10$Price))+5)
main="Top 10 Crypto Currencies in Terms of log(Price)"
bp <- barplot(log(top_10$Price),col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

## IMDB 

Look at this example from IMDb (Internet Movie Database). According to Wikipedia:

IMDb (Internet Movie Database)[2] is an online database of   information related to films, television programs, home    videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, fan and critical reviews, and ratings. We can search or refer to specific movies by URL if we wanted. For example, consider the following link to the "Lego Movie":  http://www.imdb.com/title/tt1490017/ 

![](./PICS/legom.png)

In terms of scraping information from this site we could do that using the **rvest** package. Let's say that we wanted to capture the rating information which is 7.8 out of 10. We could use the xPath Tool or the Selector gadet tool to zone in on this information. According to selector gadget we have the following xpath expression:

![](./PICS/sg.png)

```{r}
url <- "http://www.imdb.com/title/tt1490017/"
lego_movie <- read_html(url)

# Scrape the website for the movie rating
rating <- lego_movie %>%
  html_nodes(".ratingValue span") %>%
  html_text() 
```

So that gives us what we need albeit in character form. Now it's a simple matter of parsing out the first rating value:

```{r}
(rating <- as.numeric(rating[1]))
```

That wasn't so bad. Let's see what using the xPath plugin gives us:

![](./PICS/xpp.png)

We get a much longer xpath expression which should provide us with direct access to the value. 

```{r}

xp <- "/html/body/div[3]/div/div[2]/div[5]/div[1]/div[2]/div/div[1]/div[2]/div/div[1]/div[1]/div[1]/strong/span"

# Scrape the website for the movie rating
rating <- lego_movie %>%
  html_nodes(xpath=xp) %>%
  html_text() %>% as.numeric()

rating
```

Let's access the summary section of the link. We could use Selector Gadget or the xPath plugin. I'll use the former.

![](./PICS/summary.png)

![](./PICS/sumxt.png)

```{r}
mov_summary <- lego_movie %>%
  html_nodes(".summary_text") %>%
  html_text() 

mov_summary
```


## Faculty Salaries

In this example we have to parse the main table associated with the results page. 

![Salary](.//PICS/salary.png)

![Salary](.//PICS/sal_page.png)

```{r sal1}
url <- "https://www.insidehighered.com/aaup-compensation-survey"
df <- read_html(url) %>% html_table() %>% `[[`(1)
intost <- c("Institution","Category","State")
salary <- df %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
salary
```


So the default is 10 listings per page but there are many more pages we could process to get more information. If we look at the bottom of the page we can get some clues as to what the URLs are. Here we'll just process the first two pages since it will be quick and won't burden the server. 


```{r sal2,eval=FALSE}
# So now we could process multiple pages

url <- 'https://www.insidehighered.com/aaup-compensation-survey?institution-name=&professor-category=1591&page=1'
str1 <- "https://www.insidehighered.com/aaup-compensation-survey?"
str2 <- "institution-name=&professor-category=1591&page="
intost <- c("Institution","Category","State")

salary <- data.frame()

# We'll get just the first two pages

for (ii in 1:2) {
  nurl  <- paste(str1,str2,ii,sep="")
  df <- read_html(nurl)
  tmp <- df %>% html_table() %>% `[[`(1)
  tmp <- tmp %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
  salary <- rbind(salary,tmp)
}

salary

```


Look at the URLs at the bottom of the main page to find beginning and ending page numbers. Visually this is easy. Programmatically we could  do something like the following:


```{r sal3, eval=FALSE}
# https://www.insidehighered.com/aaup-compensation-survey?page=1
# https://www.insidehighered.com/aaup-compensation-survey?page=94

# What is the last page number ? We already know the answer - 94

lastnum <- df %>% html_nodes(xpath='//a') %>% 
  html_attr("href") %>% '['(103) %>%
  strsplit(.,"page=") %>% '[['(1) %>% '['(2) %>% as.numeric(.)

# So now we could get all pages of the survey

str1 <- "https://www.insidehighered.com/aaup-compensation-survey?"
str2 <- "institution-name=&professor-category=1591&page="
intost <- c("Institution","Category","State")

salary <- data.frame()

for (ii in 1:lastnum) {
  nurl  <- paste(str1,str2,ii,sep="")
  df <- read_html(nurl)
  tmp <- df %>% html_table() %>% `[[`(1)
  tmp <- tmp %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
  salary <- rbind(salary,tmp)
  Sys.sleep(1)
}
names(salary) <- c("Institution","Category","State","AvgSalFP","AvgChgFP",
                   "CntFP","AvgTotCompFP","SalEquityFP")

salary <- salary %>% 
  mutate(AvgSalFP=as.numeric(gsub("\\$|,","",salary$AvgSalFP))) %>%
  mutate(AvgTotCompFP=as.numeric(gsub("\\$|,","",salary$AvgTotCompFP)))

salary %>% group_by(State,Category) %>% 
  summarize(avg=mean(AvgSalFP)) %>% 
  arrange(desc(avg))
```

There are some problems:

* Data is large and scattered across multiple pages
* We could use above techniques to move from page to page
* There is a form we could use to narrow criteria
* But we have to programmatically submit the form
* rvest (and other packages) let you do this

## Filling Out Forms From a Program

![Salary](./PICS/salary_form.png){width=250px}

Let's find salaries between $ 150,000 and the default max ($ 244,000)

* Find the element name associated with "Average Salary"
* Establish a connection with the form (usually the url of the page)
* Get a local copy of the form
* Fill in the value for the "Average Salary"
* Submit the  lled in form
* Get the results and parse them like above
`

So finding the correct element is more challenging. I use Chrome to do this. Just highlight the area over the form and right click to "Insepct" the element. This opens up the developer tools. You have to dig down to find the corrext form and the element name. Here is a screen shot of my activity:

![Salary](./PICS/form.png){width=550px}

```{r sal4,eval=FALSE}

url <- "https://www.insidehighered.com/aaup-compensation-survey"

# Establish a session
mysess <- html_session(url)

# Get the form

form_unfilled <- mysess %>% html_node("form") %>% html_form()
form_filled   <- form_unfilled %>% set_values("range-from"=150000)

# Submit form

results <- submit_form(mysess,form_filled)
first_page <- results %>% html_nodes(xpath=expr) %>% html_table()

first_page
```


## PubMed

Pubmed provides a rich source of information on published scientific literature. There are tutorials on how to leverage its capabilities but one thing to consider is that MESH terms are a good starting place since the search is index-based. MeSH (Medical Subject Headings) is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed. It's faster and more accurate so you can first use the MESH browser to generate the appropriate search terms and add that into the Search interface. The MESH browser can be found at https://www.ncbi.nlm.nih.gov/mesh/

![](./PICS/pubmed1.png){width=550px}


![](./PICS/mesh.png){width=550px}

What we do here is get the links associated with each publication so we can then process each of those and get the abstract associated with each publication. 

```{r hemo}

# "hemodialysis, home" [MeSH Terms] 

url<-"https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D"

# 
# The results from the search will be of the form:
# https://www.ncbi.nlm.nih.gov/pubmed/30380542

results <- read_html(url) %>% 
  html_nodes("a") %>% 
  html_attr("href") %>%
  grep("/pubmed/[0-9]{1,6}",.,value=TRUE) %>% unique(.)

results
```

![](./PICS/pubmed_results.png){width=550px}

So now we could loop through these links and get the abstracts for these results. It looks that there are approximately 20 results per page. As before we would have to dive in to the underlying structure of the page to get the correct HTML pathnames or we could just look for Paragraph elements and pick out the links that way. 

```{r abs1}

text.vec <- vector()

for (ii in 1:length(results)) {
  string <- paste0("https://www.ncbi.nlm.nih.gov",results[ii])
  text.vec[ii] <- read_html(string) %>% html_nodes("p") %>% `[[`(11) %>% html_text()
}

# Eliminate lines with newlines characters

final.vec <- text.vec[grep("^\n",text.vec,invert=TRUE)]

final.vec
```


Well that was tedious. And we processed only the first page of results. How do we "progrmmatically" hit the "Next" Button at the bottom of the page ? This is complicated by the fact that there appears to be some Javascript at work that we would have to somehow interact with to get the URL for the next page. Unlike with the school salary example it isn't obvious how to do this. If we hove over the "Next" button we don't get an associated link.


<!--chapter:end:02-moreexamples.Rmd-->

# APIs {#APIs}


## OMDB 
Let's look at the IMDB page whic catalogues lots of information about movies. Just got to the web site and search although here is an example link. https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2 In this case we would like to get the summary information for the movie. So we would use Selector Gadget or some other method to find the XPATH or CSS associated with this element. 

![](./PICS/suspiria_77.png){width=450px}

This pretty easy and doesn't present much of a problem although for large scale mining of movie data we would run into trouble because IMDB doesn't really like you to scrape their pages. They have an API that they would like for you to use. 

```{r imdb, eval=FALSE}
url <- 'https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2'
summary <- read_html(url) %>% 
  html_nodes(".summary_text") %>%
  html_text()

summary
```

But here we go again. We have to parse the desired elements on this page and then what if we wanted to follow other links or set up a general function to search IMDB for other movies of various genres, titles, directors, etc. 


![](./PICS/omdb.png){width=550px}


![](./PICS/omdb_api.png){width=450px}


![](./PICS/omdb_instruc.png){width=450px}


***
So as an example on how this works. Paste the URL into any web browser. You must supply your key for this to work. What you get back is a JSON formatted entry corresponding to ”The GodFather”movie.

***

```{r godfather, eval=FALSE}
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=The+Godfather"


```

![](./PICS/omdb_result.png){width=650px}


```{r jsongf}
library(RJSONIO)

url <- "http://www.omdbapi.com/?apikey=f7c004c&t=The+Godfather"

# Fetch the URL via fromJSON
movie <- fromJSON("http://www.omdbapi.com/?apikey=f7c004c&t=The+Godfather")

# We get back a list which is much easier to process than raw JSON or XML
str(movie)
```


```{r movie}
movie$Plot

sapply(movie$Ratings,unlist)
```

Let’s Get all the Episodes for Season 1 of Game of Thrones

```{r got}
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=Game%20of%20Thrones&Season=1"
movie <- fromJSON(url)
str(movie,1)
episodes <- data.frame(do.call(rbind,movie$Episodes),stringsAsFactors = FALSE)
episodes

```


## The omdbapi package 

Wait a minute. Looks like someone created an R package that wraps all this for us. It is called omdbapi

![](./PICS/omdbapi.png){width=650px}


```{r omdbapi, eval=FALSE}
# Use devtools to install
devtools::install_github("hrbrmstr/omdbapi")
```

```{r searchombd}
library(omdbapi)
# The first time you use this you will be prompted to enter your
 # API key
movie_df <- search_by_title("Star Wars", page = 2)
(movie_df <- movie_df[,-5])


# Get lots of info on The GodFather

(gf <- find_by_title("The GodFather"))

# Get the actors from the GodFather
get_actors((gf))
```

![](./PICS/api_summary.png){width=550px}

## RSelenium

Sometimes we interact with websites that use Javascript to load more text or comments in a user forum. Here is an example of that. Look at https://www.dailystrength.org/group/dialysis which is a website associated with people wanting to share information about dialysis. 

![](./PICS/diasupport.png)

If you check the bottom of the pag you will see a button. 

![](./PICS/showmore.png)


```{r rselen, eval=FALSE}
# https://www.dailystrength.org/group/dialysis

library(RSelenium)
library(rvest)
library(tm)
library(SentimentAnalysis)
library(wordcloud)

url <- "https://www.dailystrength.org/group/dialysis"

# The website has a "show more" button that hides most of the patient posts
# If we don't find a way to programmatically "click" this button then we can
# only get a few of the posts and their responses. To do this we need to
# use the RSelenium package which does a lot of behind the scenes work

# See https://cran.r-project.org/web/packages/RSelenium/RSelenium.pdf
# http://brazenly.blogspot.com/2016/05/r-advanced-web-scraping-dynamic.html

# Open up a connection 

# rD <- rsDriver()
# So, you might have to specify the version of chrome you are using
# For someone reason this seems now to be necessary (11/4/19)

rD <- rsDriver(browser=c("chrome"),chromever="78.0.3904.70")
remDr <- rD[["client"]]
remDr$navigate(url)

loadmorebutton <- remDr$findElement(using = 'css selector', "#load-more-discussions")

# Do this a number of times to get more links

loadmorebutton$clickElement()

# Now get the page with more comments and questions

page_source <- remDr$getPageSource()

# So let's parse the contents

comments <- read_html(page_source[[1]])

cumulative_comments <- vector()

links <- comments %>% html_nodes(css=".newsfeed__description")  %>% 
  html_node("a") %>% html_attr("href")

full_links <- paste0("https://www.dailystrength.org",links)

if (length(grep("NA",full_links)) > 0) {
  full_links <- full_links[-grep("NA",full_links)]
}

ugly_xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "comments__comment-text", " " ))] | //p'

for (ii in 1:length(full_links)) {
  text <- read_html(full_links[ii]) %>% 
    html_nodes(xpath=ugly_xpath) %>% 
    html_text() 
  length(text) <- length(text) - 1
  text <- text[-1]
  
  text
  
  cumulative_comments <- c(cumulative_comments,text)
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

```


## EasyPubMed

So there is an R package called *EasyPubMed* that helps ease the access of data on the Internet. The idea behind this package is to be able to query NCBI Entrez and retrieve PubMed records in XML or TXT format.  


![](./PICS/easypubmed.png)

The PubMed records can be downloaded and saved as XML or text files if desired.  According to the package authours, "Data integrity is enforced during data download, allowing to retrieve and save very large number of records effortlessly". The bottom line is that you can do what you want after that. Let's look at an example involving home hemodialysis


```{r ezpub}
library(easyPubMed)
```

Let's do some searching

```{r echo=FALSE}
my_abstracts <- readRDS("my_abstracts")

```


```{r ezpubmed, eval=FALSE}
my_query <- 'hemodialysis, home" [MeSH Terms]'
my_entrez_id <- get_pubmed_ids(my_query)

my_abstracts <- fetch_pubmed_data(my_entrez_id)
my_abstracts <- custom_grep(my_abstracts,"AbstractText","char")


my_abstracts[1:3]

[1] "Assisted PD (assPD) is an option of home dialysis treatment for dependent 
end-stage renal patients and worldwide applied in different countries since more 
than 40 years. China and Germany shares similar trends in demographic development 
with a growing proportion of elderly referred to dialysis treatment. So far number 
of patients treated by assPD is low in both countries. We analyze experiences in 
the implementation process, barriers, and benefits of ass PD in the aging population 
to provide a model for sustainable home dialysis treatment with PD in both countries. Differences and similarities of different factors (industrial, patient and facility 
based) which affect utilization of assPD are discussed. AssPD should be promoted in 
China and Germany to realize the benefits of home dialysis for the aging population 
by providing a structured model of implementation and quality assurance."                                                                                                


[2] "End-stage renal disease (ESRD) is the final stage of chronic kidney disease 
in which the kidney is not sufficient to meet the needs of daily life. It is necessary 
to understand the role of genes expression involved in ESRD patient responses to 
nocturnal hemodialysis (NHD) and to improve the immunity responsiveness. The aim of 
this study was to investigate novel immune-associated genes that may play important 
roles in patients with ESRD.The microarray expression profiles of peripheral blood 
in patients with ESRD before and after NHD were analyzed by network-based approaches, 
and then using Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes 
pathway analysis to explore the biological process and molecular functions of 
differentially expressed genes. Subsequently, a transcriptional regulatory network 
of the core genes and the connected transcriptional regulators was constructed. 
We found that NHD had a significant effect on neutrophil activation and immune 
response in patients with ESRD.In addition, Our findings suggest that MAPKAPK3, 
RHOA, ARRB2, FLOT1, MYH9, PRKCD, RHOG, PTPN6, MAPK3, CNPY3, PI3KCG, and PYGL 
genes maybe potential targets regulated by core transcriptional factors, 
including ARNT, C/EBPalpha, CEBPA, CREB1, PSG1, DAND5, SP1, GATA1, MYC, EGR2, 
and EGR3." 

[3] "Only a minority of patients with chronic kidney disease treated by 
hemodialysis are currently treated at home. Until relatively recently, the 
only type of hemodialysis machine available for these patients was a slightly 
smaller version of the standard machines used for in-center dialysis treatments. 
Areas covered: There are now an alternative generation of dialysis machines 
specifically designed for home hemodialysis. The home dialysis patient wants 
a smaller machine, which is intuitive to use, easy to trouble shoot, robust 
and reliable, quick to setup and put away, requiring minimal waste disposal. 
The machines designed for home dialysis have some similarities in terms of 
touch-screen patient interfaces, and using pre-prepared cartridges to speed 
up setting up the machine. On the other hand, they differ in terms of whether 
they use slower or standard dialysate flows, prepare batches of dialysis fluid, 
require separate water purification equipment, or whether this is integrated, 
or use pre-prepared sterile bags of dialysis fluid. Expert commentary: Dialysis 
machine complexity is one of the hurdles reducing the number of patients opting 
for home hemodialysis and the introduction of the newer generation of dialysis 
machines designed for ease of use will hopefully increase the number of patients 
opting for home hemodialysis."

```




<!--chapter:end:03-apis.Rmd-->

# Bag of Words Sentiment Analysis {#bagofwords}

One we have a collection of text it's interesting to figure out what it might mean or infer - if anything at all. In text analysis and NLP (Natural Language Processing) we talk about "Bag of Words" to describe a collection or "corpus" of unstructured text. What do we do with a "bag of words" ? 

* Extract meaning from collections of text (without reading !) 
* Detect and analyze patterns in unstructured textual collections 
* Use Natural Language Processing techniques to reach conclusions 
* Discover what ideas occur in text and how they might be linked
* Determine if the discovered patterns be used to predict behavior ? 
* Identify interesting ideas that might otherwise be ignored


## Workflow

* Identify and Obtain text (e.g. websites, Twitter, Databases, PDFs, surveys) 
* Create a text ”Corpus”- a structure that contains the raw text
* Apply transformations:
    + Normalize case (convert to lower case)
    + Remove puncutation and stopwords
    + Remove domain specific stopwords
* Perform Analysis and Visualizations (word frequency, tagging, wordclouds) 
* Do Sentiment Analysis


R has Packages to Help. These are just some of them:

* QDAP - Quantitative Discourse Package
* tm - text mining applications within R
* tidytext - Text Mining using ddplyr and ggplot and tidyverse tools 
* SentimentAnalysis - For Sentiment Analysis

However, consider that:

* Some of these are easier to use than others
* Some can be kind of a problem to install (e.g. qdap) 
* They all offer similar capabilities
* We’ll look at tidytext

```{r echo=FALSE}
library(dplyr)
library(ggplot2)
```

## Simple Example

![](./PICS/prez.png){width=650px}


Find the URL for Lincoln’s March 4, 1865 Speech:

```{r bow1}
url <- "https://millercenter.org/the-presidency/presidential-speeches/march-4-1865-second-inaugural-address"
library(rvest)
lincoln_doc <- read_html(url) %>%
                    html_nodes(".view-transcript") %>%
                    html_text()
lincoln_doc

```


There are probably lots of words that don't really "matter" or contribute to the "real" meaning of the speech. 

```{r bow2}
word_vec <- unlist(strsplit(lincoln_doc," "))
word_vec[1:20]
sort(table(word_vec),decreasing = TRUE)[1:10]
```

How do we remove all the uninteresting words ? We could do it manaully

```{r bow3}
# Remove all punctuation marks
word_vec <- gsub("[[:punct:]]","",word_vec)
stop_words <- c("the","to","and","of","the","for","in","it",
                "a","this","which","by","is","an","hqs","from",
                "that","with","as")
for (ii in 1:length(stop_words)) {
    for (jj in 1:length(word_vec)) {
      if (stop_words[ii] == word_vec[jj]) {
          word_vec[jj] <- ""
} }
}
word_vec <- word_vec[word_vec != ""]
sort(table(word_vec),decreasing = TRUE)[1:10]
word_vec[1:30]
```

## tidytext

So the tidytext package provides some accomodations to convert your body of text into individual **tokens** which then simplfies the removal of less meaningful words and the creation of word frequency counts. The first thing you do is to create a data frame where the there is one line for each body of text. In this case we have only one long string of text this will be a one line data frame.


```{r}
library(tidytext)
library(tidyr)
text_df <- data_frame(line = 1:length(lincoln_doc), text = lincoln_doc)

text_df
```

The next step is to breakup each of text lines (we have only 1) into invdividual rows, each with it's own line. We also want to count the number of times that each word appears. This is known as **tokenizing** the data frame.

```{r bow4}
token_text <- text_df %>%
  unnest_tokens(word, text)

# Let's now count them

token_text %>% count(word,sort=TRUE)

```

But we need to get rid of the "stop words". It's a good thing that the **tidytext** package has a way to filter out the common words that do not significantly contribute to the meaning of the overall text. The **stop_words** data frame is built into **tidytext**. Take a look to see some of the words contained therein:

```{r}
data(stop_words)

# Sample 40 random stop words

stop_words %>% sample_n(40)
```



```{r bow5}
# Now remove stop words from the document

tidy_text <- token_text %>%
  anti_join(stop_words)

# This could also be done by the following. I point this out only because some people react
# negatively to "joins" although fully understanding what joins are can only help you since
# much of what the dplyr package does is based on SQL type joins. 

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

tidy_text %>% count(word,sort=TRUE)
```

```{r bow6}
tidy_text %>% count(word,sort=TRUE)

tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 2) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

## Back To The PubMed Example

We have around 935 abstracts that we mess with based on our work using the **easyPubMed** package

```{r echo=FALSE}
my_abstracts <- readRDS("my_abstracts")
```



```{r bow7,message=FALSE}
# Create a data frame out of the cleaned up abstracts
library(tidytext)
library(dplyr)
text_df <- data_frame(line = 1:length(my_abstracts), text = my_abstracts)
token_text <- text_df %>%
  unnest_tokens(word, text)

# Many of these words aren't helpful 
token_text %>% count(total=word,sort=TRUE)

# Now remove stop words
data(stop_words)
tidy_text <- token_text %>%
  anti_join(stop_words)

# This could also be done by the following. I point this out only because some people react
# negatively to "joins" although fully understanding what joins are can only help you since
# much of what the dplyr package does is based on SQL type joins. 

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

# Arrange the text by descending word frequency 

tidy_text %>%
  count(word, sort = TRUE) 
```

Some of the most frequently occurring words are in fact "dialysis", "patients" so maybe we should consider them to be stop words also since we already know quite well that the overall theme is, well, dialysis and kidneys. There are also synonymns and abbreviations that are somewhat redundant such as "pdd","pd","hhd" so let's eliminate them also. 


```{r bow8}
tidy_text <- token_text %>%
   filter(!word %in% c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","disease","treatment"))

tidy_text %>%
  count(word, sort = TRUE) 
```


Let's do some plotting of these words


```{r bow9}
library(ggplot2)
tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 120) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```



Okay, it looks like there are numbers in there which might be useful. I suspect that the "95" is probably associated with the idea of a confidence interval. But there are other references to numbers. 

```{r bow10}
grep("^[0-9]{1,3}$",tidy_text$word)[1:20]

tidy_text_nonum <- tidy_text[grep("^[0-9]{1,3}$",tidy_text$word,invert=TRUE),]

```


Okay well I think maybe we have some reasonable data to examine. As you might have realized by now, manipulating data to get it "clean" can be tedious and frustrating though it is an inevitable part of the process. 

```{r bow11}
tidy_text_nonum %>%
  count(word, sort = TRUE) %>%
  filter(n > 120) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

### How Do You Feel ?

The next step is to explore what some of these words might mean. The **tidytext** package has four dictionaries that help you figure out what sentiment is being expressed by your data frame. 


```{r}
# NRC Emotion Lexicon from Saif Mohammad and Peter Turney
get_sentiments("nrc") %>% sample_n(20)

# the sentiment lexicon from Bing Liu and collaborators
get_sentiments("bing") %>% sample_n(20)

# Tim Loughran and Bill McDonald
get_sentiments("loughran") %>% sample_n(20)

# Pull out words that correspond to joy
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

nrc_joy
```

So we will use the **nrc** sentiment dictionary to see the "sentiment" expressed in our abstracts.

```{r bow12}

bing_word_counts <- tidy_text_nonum %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(word,sentiment,sort=TRUE)
```


t the positive vs negative words

```{r bow13}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```


Let's create a word cloud


```{r bow14,message=FALSE, warning=FALSE}
library(wordcloud)
#

tidy_text_nonum %>%  
  count(word) %>%
  with(wordcloud(word,n,max.words=90,scale=c(4,.5),colors=brewer.pal(8,"Dark2")))
```

## BiGrams

Let's look at bigrams. We need to go back to the cleaned abstracts and pair words to get phrase that might be suggestive of some sentiment


```{r bow15}
text_df <- data_frame(line = 1:length(my_abstracts), text = my_abstracts)
dialysis_bigrams <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

dialysis_bigrams %>%
  count(bigram, sort = TRUE)

```


But we have to filter out stop words

```{r bow16,message=FALSE}
library(tidyr)
bigrams_sep <- dialysis_bigrams %>% 
  separate(bigram,c("word1","word2"),sep=" ")

stop_list <- c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","treatment","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","esrd","lt","95","0.001")

bigrams_filtered <- bigrams_sep %>% 
  filter(!word1 %in% stop_list) %>%
  filter(!word2 %in% stop_list)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>%  count(bigram, sort = TRUE) %>% print(n=25)
```



```{r bow17,message=FALSE}
library(tidyquant)
bigram_counts %>%
  filter(n > 30) %>%
  ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) +
    geom_tile(alpha = 0.8, color = "white") +
    scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) +
    coord_flip() +
    theme_tq() +
    theme(legend.position = "right") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "first word in pair",
         y = "second word in pair")
```


<!--chapter:end:04-bow.Rmd-->

# Final Words

We have finished a nice book.

<!--chapter:end:05-summary.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

