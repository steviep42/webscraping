--- 
title: "Web Scraping with R"
author: "Steve Pittard"
date: "`r Sys.Date()`"
bibliography:
- book.bib
- packages.bib
description: This is in support of my talk for the Data Science group
documentclass: book
link-citations: yes
site: bookdown::bookdown_site
biblio-style: apalike
---


# Motivations



## Lots of Data For The Taking ? 

The web hosts lots of interesting data that you can ”scrape”. Some of it is stashed in data bases, behind APIs, or in free form text. Lots of people want to grab information of of Twitter or from user forums to see what people are thinking. There is a lot of valuable information out there for the taking although some web sites have "caught on" and either block programmatic access or they setup "pay walls" that require you to subscribe to an API for access. The New York Times does this. But there are lots of opportunities to get data. 

 | 
------|--------------------------------
tables|Fetch tables like from Wikipedia
forms|You can submit forms and fetch the results
css|You can access parts of a web site using style or css selectors
Tweets|Process tweets including emojis
Web Sites| User forums have lots of content
Instagram| Yes you can "scrape" photos also



## Web Scraping Can Be Ugly

Depending on what web sites you want to scrape the process can be involved and quite tedious. Many websites are very much aware that people are scraping so they offer Application Programming Interfaces (APIs) to make requests for information easier for the user and easier for the server administrators to control access. Most times the user must apply for a "key" to gain access. For premium sites, the key costs money. Some sites like Google and Wunderground (a popular weather site) allow some number of free accesses before they start charging you. Even so the results are typically returned in XML or JSON which then requires you to parse the result to get the information you want. In the best situation there is an R package that will wrap in the parsing and will return lists or data frames. 

Here is a summary:

* First. Always try to find an R package that will access a site (e.g. New York Times, Wunderground, PubMed). These packages (e.g. omdbapi, easyPubMed, RBitCoin, rtimes) provide a programmatic search interface and return data frames with little to no effort on your part. 

* If no package exists then hopefully there is an API that allows you to query the website and get results back in JSON or XML. I prefer JSON because it's "easier" and the packages for parsing JSON return lists which are native data structures to R. So you can easily turn results into data frames. You will ususally use the *rvest* package in conjunction with XML, and the RSJONIO packages. 

* If the Web site doesn't have an API then you will need to scrape text. This isn't hard but it is tedious. You will need to use *rvest* to parse HMTL elements. If you want to parse mutliple pages then you will need to use *rvest* to move to the other pages and possibly fill out forms. If there is a lot of Javascript then you might need to use RSelenium to programmatically manage the web page. 

## Understanding The Language of The Web

The Web has its own languages: HTML, CSS, Javascript

```{r eval=FALSE}
<h1>, <h2>, ..., <h6> Heading 1 and so on
<p> Paragraph elements
<ul> Unordered List
<ol> Ordered List
<li> List Element
<div> Division / Section
<table> Tables
<form> Web forms
```

So to be productive at scraping requires you to have some familiairty with HMTL XML, and CSS. Here we look at a very basic HTML file


```{r eval=FALSE}
<!DOCTYPE html>
<html>
  <body>
    <h1>My First Heading</h1>
    <p>My first paragraph.</p>
   </body>
</html>
```

![](./PICS/first_heading.png){width=450px}

\   

And you could apply some styling to this courtest of the CSS language which allows you to inject styles into plain HTML:

![](.//PICS/css.png){width=450px}

\  

![](.//PICS/css_page.png){width=450px}

There are a number of tools that allow us to inspect web pages and see "what is under the hood". Warning - I just discovered that one of my favorite browser plugins to find the xpaths and/or css paths of page elements is no longer supported under Firefox or Chrome. I've found a couple of replacements but they don't work as well. I'll research it more. 

\   

| 
------|--------------------------------
Selector Gadget | http://selectorgadget.com/
Firebug | https://getfirebug.com/ (now integrated into a version of Firefox)
Google Chrome | Right click to inspect a page element 
Google Chrome | View Developer - Developer Tools
Oxygen Editor | Can obtain via the Emory Software Express Site

## Useful Packages

You will use the following three primary packages to help you get data from various web pages: *rvest*, *XML*, and *RJSONIO*. Note that you won't always use them simultaneously but you might use them in pairs or individually depending on the task at hand. 

## Quick **rvest** tutorial

Now let's do a quick *rvest* tutorial:


```{r}
library(rvest)
url <- "https://en.wikipedia.org/wiki/World_population"

(paragraphs <- read_html(url) %>% html_nodes("p"))

```


Then we might want to actually parse out those paragraphs into text:

```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
paragraphs <- read_html(url) %>% html_nodes("p") %>% html_text()
paragraphs[1:10]
```


Get some other types of HTML obejects. Let's get all the "h2" HTML elements:

```{r}

read_html(url) %>% html_nodes("h2") %>% html_text()
```

What about tables ? 


```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
tables <- read_html(url) %>% html_nodes("table") 
tables
```


## Example: Parsing A Table From Wikipedia

Look at the [Wikipedia Page](https://en.wikipedia.org/wiki/World_population) for world population:

https://en.wikipedia.org/wiki/World_population

* We can get any table we want using rvest
* We might have to experiment to figure out which one 
* Get the one that lists the ten most populous countries 
* I think this might be the 4th or 5th table on the page
* How do we get this ?

![](./PICS/worldpop.png){width=450px}


First we will load packages that will help us throughout this session.

```{r message=F, echo=FALSE}
library(rvest)
library(tidytext)
library(dplyr)
library(ggplot2)
library(rtweet)
library(tidyr)
library(wordcloud)
library(tidyquant)
```

In this case we'll need to figure out what number table it is we want. We could fetch
all the tables and then experiment to find the precise one. 

```{r}
library(rvest)
library(tidyr)
library(dplyr)
library(ggplot2)

# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 
  
ten_most_populous <- ten_most_df %>% 
  html_nodes("table") %>% `[[`(6) %>% html_table()

# Let's get just the first three columns
ten_most_populous <- ten_most_populous[,2:4]

# Get some content - Change the column names
names(ten_most_populous) <- c("Country_Territory","Population","Date")

# Do reformatting on the columns to be actual numerics where appropriate
ten_most_populous %>% 
  mutate(Population=gsub(",","",Population)) %>% 
  mutate(Population=round(as.numeric(Population)/1e+06))  %>%
  ggplot(aes(x=Country_Territory,y=Population)) + geom_point() + 
  labs(y = "Population / 1,000,000") + coord_flip() +
  ggtitle("Top 10 Most Populous Countries")

```

In the above example we leveraged the fact that we were looking specifically for a table element and it became a project to locate the correct table number. This isn't always the case with more complicated websites in that the element we are trying to grab or scrape is contained within a nested structure that doesn't correspond neatly to a paragraph, link, heading, or table. This can be the case if the page is heavily styled with CSS or Javascript. We might have to work harder. But it's okay to try to use simple elements and then try to refine the search some more.

## Summary

* Need some basic HTML and CSS knowledge to find correct elements
* How to extract text from common elements
* How to extract text from specific elements
* Always have to do some text cleanup of data
* It usually takes multiple times to get it right

See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# XML and JSON {#xml}

This is where things get a little dicey because some web pages will return XML and JSON in response to inquiries and while these formats seem complicated they are actually doing you a really big favor by doing this since these formats can ususally be easily parsed using various packges. XML is a bit hard to get your head around and JSON is the new kid on the block which is easier to use. 

Since this isn't a full-on course lecture I'll keep it short as to how and why you would want to use these but any time you spend trying to better understand JSON (and XML) the better of you will be when parsing web pages. It's not such a big deal if all you are going to be parsing is raw text since the mthods we use to do that avoid XML and JSON although cleaning up raw text has its own problems. Let's revisit the Wikipedia example from the previous section.

```{r}
library(rvest)

# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 
  
ten_most_populous <- ten_most_df %>% 
  html_nodes("table") %>% `[[`(6) %>% html_table()
```

![](./PICS/xml.png){width=650px}

![](./PICS/xmls.png){width=450px}


Let's look at an XML file that has some basic content:
\   

```{r eval=FALSE}
<?xml version="1.0" encoding="UTF-8"?>
<bookstore>
  <book category="COOKING">
    <title lang="en">Everyday Italian</title>
    <author>Giada De Laurentiis</author>
    <year>2005</year>
    <price>30.00</price>
  </book>
  <book category="CHILDREN">
    <title lang="en">Harry Potter</title>
    <author>J K. Rowling</author>
    <year>2005</year>
    <price>29.99</price>
  </book>
  <book category="WEB">
    <title lang="en">Learning XML</title>
    <author>Erik T. Ray</author>
    <year>2003</year>
    <price>39.95</price>
  </book>
</bookstore>

```
Well we pulled out all tables and then, by experimentation, we isolated table 6 and got the content corresponding to that. But. Is there a more direct way to find the content ? There is. It requires us to install some helper plugins such as the xPath Finder for Firefox and Chrome. In reality there are a number of ways to find the XML Path or CSS Path for an element within a web page but this is a good one to start. 



![](./PICS/xpath_finder.png){width=350px}

Remeber that we want to find the table corresponding to the "10 Most Populous Countries". So we activate the *xPath* finder plugin and the highlight the element of interest. This takes some practice to get it right. Once you highlight the desired elment you will see the corresponding XPATH. Here is a screenshot of what I did. We can use the resulting path to directly access the table without first having to first pull out all tables and then trying to find the right one

![](./PICS/xpath_pop.png){width=350px}

\   
 
```{r}
# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_populous <- read_html(url) 

ten_most_df <- ten_most_populous %>% 
  html_nodes(xpath='/html/body/div[3]/div[3]/div[4]/div/table[5]') %>%
  html_table() 

# We have to get the first element of the list. 
ten_most_df <- ten_most_df[[1]]

ten_most_df
```

## Finding XPaths

![](./PICS/xml_path_1.png){width=650px}

\   

In addition to Browser Plugins there are standalone tools such as the Oxygen XML Editor which is availabel through the Emory Software Express Website. This is a comprehensive XML editor that will allow you to parse XML and develop paths to locate specific nodes within an XML document. If you find yourself working with websites with lots of XML then this will be useful. The Oxygen editor is free. 

![](./PICS/xml_path_10.png){width=350px}

Let's look at an XML file that has some basic content:
\   

```{r eval=FALSE}
<?xml version="1.0" encoding="UTF-8"?>
<bookstore>
  <book category="COOKING">
    <title lang="en">Everyday Italian</title>
    <author>Giada De Laurentiis</author>
    <year>2005</year>
    <price>30.00</price>
  </book>
  <book category="CHILDREN">
    <title lang="en">Harry Potter</title>
    <author>J K. Rowling</author>
    <year>2005</year>
    <price>29.99</price>
  </book>
  <book category="WEB">
    <title lang="en">Learning XML</title>
    <author>Erik T. Ray</author>
    <year>2003</year>
    <price>39.95</price>
  </book>
</bookstore>

```


![](./PICS/xml_path_2.png){width=650px}

\   

![](./PICS/xml_path_3.png){width=550px}


\  

![](./PICS/xml_path_4.png){width=550px}

\  

![](./PICS/xml_path_4.png){width=550px}

\  

![](./PICS/xml_path_6.png){width=550px}

![](./PICS/xml_path_7.png){width=550px}


![](./PICS/xml_path_8.png){width=550px}

![](./PICS/xml_path_9.png){width=550px}


## Example: GeoCoding With Google 

Let's run through an example of using the GeoCoding API with Google. They used to provide free access to this service and they still do although you are limited to a certain number. You have to sign up for an account and get an API key. Anyway, let's run through this example and then look at how I parsed the XML file that is returned by the Google GeoCoding API. Let's say that we wanted to get the latitude and longitude associated with the the address *1510 Clifton Rd, Atlanta, GA* which corresponds to the Rollins Research Building. 

First we will see an example of what Google returns in terms of XML. We can use some tools like Oxygen Editor (available free via Emory Software Express) to develop an appropriate XPATH expression to parse out the latitude and longitude information. 

```{r message=FALSE}

# https://maps.googleapis.com/maps/api/geocode/xml?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM

# https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM

myGeo <- function(address="1510 Clifton Rd Atlanta GA",form="xml") {
  library(XML)
  library(RCurl)
  
  geourl <- "https://maps.googleapis.com/maps/api/geocode/"
  key <- "AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM"
  address <- gsub(" ","+",address)
  
  add <- paste0(geourl,form,sep="?address=")
  add <- paste0(add,address,"&key=")
  geourl <- paste0(add,key)
  
  locale <- getURL(geourl)
  plocal <- xmlParse(locale,useInternalNodes=TRUE)
  
  # Okay let's extract the lat and lon
  latlon <- getNodeSet(plocal,"/GeocodeResponse/result/geometry/location/descendant::*")
  lat  <- as.numeric(xmlSApply(latlon,xmlValue))[1]
  lon  <- as.numeric(xmlSApply(latlon,xmlValue))[2]
  
  return(c(lat=lat,lng=lon))  
}

myGeo()
```
\  
Now. We could have saved the report to a file on our local computer and open it up with Oxygen editor and figure out what the approproate XPATH would be. This is basically what I did. Here is a screenshot of the session:



![](./PICS/oxygen2.png){width=650px}

\  
We could expand this considerable to process a number of addresses:


```{r message=FALSE}
namevec <- c("Atlanta GA", 
             "Birmingham AL", 
             "Seattle WA", 
             "Sacramento CA",
             "Denver CO", 
             "LosAngeles CA", 
             "Rochester NY")

cityList <- lapply(namevec,myGeo)

# Or to get a data frame

cities <- data.frame(city=namevec,do.call(rbind,cityList),
                     stringsAsFactors = FALSE)
cities


# Let's create a Map

library(leaflet)
m <- leaflet(data=cities)
m <- addTiles(m)
m <- addMarkers(m,popup=cities$city)

# Put up the Map - click on the markers
m
```

## Using JSON 

JSON is fast becoming the primary interchange format over XML although XML is still well supported. R has a number of packages to ease the parsing of JSON/ documents returned by web pages. Ususally you get back a *list* which is a native data type in R that can easily be manipulated into a data frame. Most web APIs provide an option for JSON or XML although some only provide JSON. 


![](./PICS/json.png){width=650px}

There are rules and regulations about how JSON is formed and we will learn them by example but you can look at the numerous tutotorials on the web to locate definitive references. See http://www.w3schools.com/json/  Here is an XML file that describes some employees. 

```{r eval=FALSE}
<employees>
     <employee>
         <firstName>John</firstName>
         <lastName>Doe</lastName>
     </employee>
     <employee>
         <firstName>Anna</firstName>
         <lastName>Smith</lastName>
     </employee>
     <employee>
         <firstName>Peter</firstName>
         <lastName>Jones</lastName>
     </employee>
 </employees>
```


And here is the corresposning JSON file:

```{r eval=FALSE}
{
  "employees":[
    {"firstName":"John", "lastName":"Doe"},
    {"firstName":"Anna", "lastName":"Smith"},
    {"firstName":"Peter","lastName":"Jones"}
] }
```


* It is important to note that the actual information in the document, things like city name, county name, latitude, and longitude are the same as they would be in the comparable XML document.

* JSON documents are at the heart of the NoSQL“database”called MongoDB

* JSON can be found within many webpages since it is closely related to JavaScript which is a language strongly related to web pages.

* JSON is very compact and lighweight which has made it a natural followon to XML so much so that it appears to be replacing XML


See http:..www.json.org/ for a full description of the specification

* An object is an unordered set of name/value pairs. An object begins with (left brace) and ends with (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma).

* An array is an ordered collection of values. An array begins with [ (left bracket) and ends with ] (right bracket). Values are separated by , (comma).

* A value can be a string in double quotes, or a number, or true or false or null, or an object or an array. These structures can be nested.

* A string is a sequence of zero or more Unicode characters, wrapped in double quotes, using backslash escapes. A character is represented as a single character string.


Do you remember the Google Geocding example from before ? We can tell Google to send us back JSON instead of XML just by adjusting the URL accordingly:

```{r}
url <- "https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Rd+Atlanta+GA&key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM"
```


![](./PICS/json_google.png){width=650px}

## Using the RJSONIO Package

To read/parse this in R we use a package called RJSONIO. There are other packages but this is the one we will be using. Download and install it.

There is a function called fromJSON which will parse the JSON file and return a list to contain the data.

So we parse lists instead of using XPath. Many people feel this to be easier than trying to construct XPath statments. You will have to decide for yourself.


```{r}
library(RJSONIO)

url <- "https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM"
geo <- fromJSON(url)
str(geo,3)
```

Since what we get back is a list we can directly access whatever we want. We just index into the list. No need for complicated XPATHS.

```{r}
geo$results[[1]]$geometry$location
```


<!--chapter:end:01-xmljson.Rmd-->

# More Real Life Examples {#Moreexamples}

```{r message=FALSE,echo=FALSE}
library(rvest)
library(tidytext)
library(dplyr)
library(ggplot2)
library(rtweet)
library(tidyr)
library(wordcloud)
library(tidyquant)
```


Okay. This is a tour of some sites that will serve as important examples on how to parse sites. Let's check the price of bitcoins. You want to be rich don't you ? 

## BitCoin Prices


![](./PICS/crypto.png){width=650px}

![](./PICS/view_source.png){width=750px}

```{r}
library(rvest)
url <- "https://coinmarketcap.com/all/views/all/"
bc <- read_html(url)

path <- '//*[@id="currencies-all"]'
 bc_table <- bc %>% html_nodes(xpath=path) %>% html_table()
 # We get back a one element list that is a data frame
 str(bc_table,0)
 bc_table <- bc_table[[1]]
 head(bc_table[,3:5])
```

```{r}
# The data is "dirty" and has characers in it that need cleaning
bc_table <- bc_table %>% select(Name,Symbol,Price)
bc_table <- bc_table %>% mutate(Name=gsub("\n"," ",Name))
bc_table <- bc_table %>% mutate(Name=gsub("\\.+","",Name))
bc_table <- bc_table %>% mutate(Price=gsub("\\$","",Price))
bc_table <- bc_table %>% mutate(Price=round(as.numeric(Price),2))

# There are four rows wherein the Price is missing NA
bc_table <- bc_table %>% filter(complete.cases(bc_table))

# Let's get the Crypto currencies with the Top 10 highest prices 
top_10 <- bc_table %>% arrange(desc(Price)) %>% head(10)

# Next we want to make a barplot of the Top 10
ylim=c(0,max(top_10$Price)+10000)
main="Top 10 Crypto Currencies in Terms of Price"
bp <- barplot(top_10$Price,col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
# Let's take the log of the price
ylim=c(0,max(log(top_10$Price))+5)
main="Top 10 Crypto Currencies in Terms of log(Price)"
bp <- barplot(log(top_10$Price),col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

## Faculty Salaries

In this example we have to parse the main table associated with the results page. 

![Salary](.//PICS/salary.png)

![Salary](.//PICS/sal_page.png)

```{r}
url <- "https://www.insidehighered.com/aaup-compensation-survey"
df <- read_html(url) %>% html_table() %>% `[[`(1)
intost <- c("Institution","Category","State")
salary <- df %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
salary
```


So we could process multiple pages


```{r eval=FALSE}
# So now we could process multiple pages

url <- 'https://www.insidehighered.com/aaup-compensation-survey?institution-name=&professor-category=1591&page=1'
str1 <- "https://www.insidehighered.com/aaup-compensation-survey?"
str2 <- "institution-name=&professor-category=1591&page="
intost <- c("Institution","Category","State")

salary <- data.frame()
for (ii in 1:2) {
  nurl  <- paste(str1,str2,ii,sep="")
  df <- read_html(nurl)
  tmp <- df %>% html_table() %>% `[[`(1)
  tmp <- tmp %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
  salary <- rbind(salary,tmp)
}

salary

```


Look at the URLs at the bottom of the main page to find beginning and ending page numbers. Visually this is easy. Programmatically we could  do something like the following:


```{r eval=FALSE}
# https://www.insidehighered.com/aaup-compensation-survey?page=1
# https://www.insidehighered.com/aaup-compensation-survey?page=94

# What is the last page number ? We already know the answer - 94

lastnum <- df %>% html_nodes(xpath='//a') %>% 
  html_attr("href") %>% '['(103) %>%
  strsplit(.,"page=") %>% '[['(1) %>% '['(2) %>% as.numeric(.)

# So now we could get all pages of the survey

str1 <- "https://www.insidehighered.com/aaup-compensation-survey?"
str2 <- "institution-name=&professor-category=1591&page="
intost <- c("Institution","Category","State")

salary <- data.frame()

for (ii in 1:lastnum) {
  nurl  <- paste(str1,str2,ii,sep="")
  df <- read_html(nurl)
  tmp <- df %>% html_table() %>% `[[`(1)
  tmp <- tmp %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
  salary <- rbind(salary,tmp)
  Sys.sleep(1)
}
names(salary) <- c("Institution","Category","State","AvgSalFP","AvgChgFP",
                   "CntFP","AvgTotCompFP","SalEquityFP")

salary <- salary %>% 
  mutate(AvgSalFP=as.numeric(gsub("\\$|,","",salary$AvgSalFP))) %>%
  mutate(AvgTotCompFP=as.numeric(gsub("\\$|,","",salary$AvgTotCompFP)))

salary %>% group_by(State,Category) %>% 
  summarize(avg=mean(AvgSalFP)) %>% 
  arrange(desc(avg))
```

There are some problems:

* Data is large and scattered across multiple pages
* We could use above techniques to move from page to page
* There is a form we could use to narrow criteria
* But we have to programmatically submit the form
* rvest (and other packages) let you do this

## Filling Out Forms From a Program

![Salary](./PICS/salary_form.png){width=250px}

Let's find salaries between $ 150,000 and the default max ($ 244,000)

* Find the element name associated with "Average Salary"
* Establish a connection with the form (usually the url of the page)
* Get a local copy of the form
* Fill in the value for the "Average Salary"
* Submit the  lled in form
* Get the results and parse them like above
`

So finding the correct element is more challenging. I use Chrome to do this. Just highlight the area over the form and right click to "Insepct" the element. This opens up the developer tools. You have to dig down to find the corrext form and the element name. Here is a screen shot of my activity:

![Salary](./PICS/form.png){width=550px}

```{r}

url <- "https://www.insidehighered.com/aaup-compensation-survey"

# Establish a session
mysess <- html_session(url)

# Get the form

form_unfilled <- mysess %>% html_node("form") %>% html_form()
form_filled   <- form_unfilled %>% set_values("range-from"=150000)

# Submit form

results <- submit_form(mysess,form_filled)
first_page <- results %>% html_nodes(xpath=expr) %>% html_table()

first_page
```


## PubMed

Pubmed provides a rich source of information on published scientific literature. There are tutorials on how to leverage its capabilities but one thing to consider is that MESH terms are a good starting place since the search is index-based. MeSH (Medical Subject Headings) is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed. It's faster and more accurate so you can first use the MESH browser to generate the appropriate search terms and add that into the Search interface. The MESH browser can be found at https://www.ncbi.nlm.nih.gov/mesh/

![](./PICS/pubmed1.png){width=550px}


![](./PICS/mesh.png){width=550px}

What we do here is get the links associated with each publication so we can then process each of those and get the abstract associated with each publication. 

```{r}

# "hemodialysis, home" [MeSH Terms] 

url <- "https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D"

# 

results <- read_html(url) %>% 
  html_nodes("a") %>% 
  html_attr("href") %>%
  grep("/pubmed/[0-9]{1,6}",.,value=TRUE) %>% unique(.)

results
```

![](./PICS/pubmed_results.png){width=550px}

So now we could loop through these links and get the abstracts for these results. It looks that there are approximately 10 results per page. As before we would have to dive in to the underlying structure of the page to get the correct XML pathnames or we could look for Paragraph elements and pick out the links that way. 

```{r}

text.vec <- vector()

for (ii in 1:length(results)) {
  string <- paste0("https://www.ncbi.nlm.nih.gov",results[ii])
  text.vec[ii] <- read_html(string) %>% html_nodes("p") %>% `[[`(10) %>% html_text()
}

# Eliminate lines with newlines characters

final.vec <- text.vec[grep("^\n",text.vec,invert=TRUE)]

final.vec
```


Well that was a lot of work and it's very tedious. And we processed only the first page of results. How do we "progrmmatically" hit the "Next" Button at the bottom of the page ? This is complicated by the fact that there appears to be some Javascript at work that we would have to somehow interact with to get the URL for the next page. Unlike with the school salary example it isn't obvious how to do this. If we hove over the "Next" button we don't get an associated link.


<!--chapter:end:02-moreexamples.Rmd-->

# APIs {#APIs}


## OMDB 
Let's look at the IMDB page whic catalogues lots of information about movies. Just got to the web site and search although here is an example link. https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2 In this case we would like to get the summary information for the movie. So we would use Selector Gadget or some other method to find the XPATH or CSS associated with this element. 

![](./PICS/suspiria_77.png){width=450px}

This pretty easy and doesn't present much of a problem although for large scale mining of movie data we would run into trouble because IMDB doesn't really like you to scrape their pages. They have an API that they would like for you to use. 

```{r eval=FALSE}
url <- 'https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2'
summary <- read_html(url) %>% 
  html_nodes(".summary_text") %>%
  html_text()
```

But here we go again. We have to parse the desired elements on this page and then what if we wanted to follow other links or set up a general function to search IMDB for other movies of various genres, titles, directors, etc. 


![](./PICS/omdb.png){width=550px}


![](./PICS/omdb_api.png){width=450px}


![](./PICS/omdb_instruc.png){width=450px}


***
So as an example on how this works. Paste the URL into any web browser. You must supply your key for this to work. What you get back is a JSON formatted entry corresponding to ”The GodFather”movie.

***

```{r eval=FALSE}
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=The+Godfather"


```

![](./PICS/omdb_result.png){width=650px}


```{r}
library(RJSONIO)

url <- "http://www.omdbapi.com/?apikey=f7c004c&t=The+Godfather"

# Fetch the URL via fromJSON
movie <- fromJSON("http://www.omdbapi.com/?apikey=f7c004c&t=The+Godfather")

# We get back a list which is much easier to process than raw JSON or XML
str(movie)
```


```{r}
movie$Plot

sapply(movie$Ratings,unlist)
```

Let’s Get all the Episodes for Season 1 of Game of Thrones

```{r}
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=Game%20of%20Thrones&Season=1"
movie <- fromJSON(url)
str(movie,1)
episodes <- data.frame(do.call(rbind,movie$Episodes),stringsAsFactors = FALSE)
episodes

```


## The omdbapi package 

Wait a minute. Looks like someone created an R package that wraps all this for us. It is called omdbapi

![](./PICS/omdbapi.png){width=650px}


```{r eval=FALSE}
# Use devtools to install
devtools::install_github("hrbrmstr/omdbapi")
```

```{r}
library(omdbapi)
# The first time you use this you will be prompted to enter your
 # API key
movie_df <- search_by_title("Star Wars", page = 2)
(movie_df <- movie_df[,-5])


# Get lots of info on The GodFather

(gf <- find_by_title("The GodFather"))

# Get the actors from the GodFather
get_actors((gf))
```

![](./PICS/api_summary.png){width=550px}

## RSelenium

Sometimes we interact with websites that use Javascript to load more text or comments in a user forum. Here is an example of that. Look at https://www.dailystrength.org/group/dialysis which is a website associated with people wanting to share information about dialysis. 

![](./PICS/diasupport.png)

If you check the bottom of the pag you will see a button. 

![](./PICS/showmore.png)


```{r eval=FALSE}
# https://www.dailystrength.org/group/dialysis

library(RSelenium)
library(rvest)
library(tm)
library(SentimentAnalysis)
library(wordcloud)

url <- "https://www.dailystrength.org/group/dialysis"

# The website has a "show more" button that hides most of the patient posts
# If we don't find a way to programmatically "click" this button then we can
# only get a few of the posts and their responses. To do this we need to
# use the RSelenium package which does a lot of behind the scenes work

# See https://cran.r-project.org/web/packages/RSelenium/RSelenium.pdf
# http://brazenly.blogspot.com/2016/05/r-advanced-web-scraping-dynamic.html

# Open up a connection 

rD <- rsDriver()
remDr <- rD[["client"]]
remDr$navigate(url)

loadmorebutton <- remDr$findElement(using = 'css selector', "#load-more-discussions")

# Do this a number of times to get more links

loadmorebutton$clickElement()

# Now get the page with more comments and questions

page_source <- remDr$getPageSource()

# So let's parse the contents

comments <- read_html(page_source[[1]])

cumulative_comments <- vector()

links <- comments %>% html_nodes(css=".newsfeed__description")  %>% 
  html_node("a") %>% html_attr("href")

full_links <- paste0("https://www.dailystrength.org",links)

if (length(grep("NA",full_links)) > 0) {
  full_links <- full_links[-grep("NA",full_links)]
}

ugly_xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "comments__comment-text", " " ))] | //p'

for (ii in 1:length(full_links)) {
  text <- read_html(full_links[ii]) %>% 
    html_nodes(xpath=ugly_xpath) %>% 
    html_text() 
  length(text) <- length(text) - 1
  text <- text[-1]
  
  text
  
  cumulative_comments <- c(cumulative_comments,text)
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

```


## EasyPubMed

So there is an R package called *EasyPubMed* that helps ease the access of data on the Internet. The idea behind this package is to be able to query NCBI Entrez and retrieve PubMed records in XML or TXT format.  The PubMed records can be downloaded and saved as XML or text files if desired.  According to the package authours, "Data integrity is enforced during data download, allowing to retrieve and save very large number of records effortlessly". The bottom line is that you can do what you want after that. Let's look at an example involving home hemodialysis


```{r}
library(easyPubMed)
```

Let's do some searching

```{r}
my_query <- 'hemodialysis, home" [MeSH Terms]'
my_entrez_id <- get_pubmed_ids(my_query)
my_abstracts_txt <- fetch_pubmed_data(my_entrez_id, format = "abstract")
my_abstracts_txt[1:20]

my_abstracts_xml <- fetch_pubmed_data(my_entrez_id)

```


Well the results aren't bad but we still have to parse out some junk but nothing like before. It's a minor problem. In this case the API gives us some XML back that we then have to parse. 

```{r}
my_abstracts <- unlist(xpathApply(my_abstracts_xml, "//AbstractText", saveXML))

# Still have to clear out some junk in each abstract

my_abstracts <- gsub('<(.*)">',"",my_abstracts)
my_abstracts <- gsub('<AbstractText>|</AbstractText>',"",my_abstracts)

length(my_abstracts)

```

But we have around 935 abstracts that we can now mess with. The benefit of this approach is that we dont' have to do all the text conversions and manipulations that we would have with the manual approach. Plus, we are able to get a lot more results more conveniently. 

```{r}
my_abstracts[1:3]
```




<!--chapter:end:03-apis.Rmd-->

# Bag of Words Sentiment Analysis {#bagofwords}

One we have a collection of text it's interesting to figure out what it might mean or infer - if anything at all. In text analysis and NLP (Natural Language Processing) we talk about "Bag of Words" to describe a collection or "corpus" of unstructured text. What do we do with a "bag of words" ? 

* Extract meaning from collections of text (without reading !) 
* Detect and analyze patterns in unstructured textual collections 
* Use Natural Language Processing techniques to reach conclusions 
* Discover what ideas occur in text and how they might be linked
* Determine if the discovered patterns be used to predict behavior ? 
* Identify interesting ideas that might otherwise be ignored


## Workflow

* Identify and Obtain text (e.g. websites, Twitter, Databases, PDFs, surveys) 
* Create a text ”Corpus”- a structure that contains the raw text
* Apply transformations:
    + Normalize case (convert to lower case)
    + Remove puncutation and stopwords
    + Remove domain specific stopwords
* Perform Analysis and Visualizations (word frequency, tagging, wordclouds) 
* Do Sentiment Analysis


R has Packages to Help. These are just some of them:

* QDAP - Quantitative Discourse Package
* tm - text mining applications within R
* tidytext - Text Mining using ddplyr and ggplot and tidyverse tools 
* SentimentAnalysis - For Sentiment Analysis

However, consider that:

* Some of these are easier to use than others
* Some can be kind of a problem to install (e.g. qdap) 
* They all offer similar capabilities
* We’ll look at tidytext


## Simple Example

![](./PICS/prez.png){width=650px}


Find the URL for Lincoln’s March 4, 1865 Speech:

```{r}
url <- "https://millercenter.org/the-presidency/presidential-speeches/march-4-1865-second-inaugural-address"
library(rvest)
lincoln_doc <- read_html(url) %>%
                    html_nodes(".view-transcript") %>%
                    html_text()
lincoln_doc

```


There are probably lots of words that don't really "matter" or contribute to the "real" meaning of the speech. 

```{r}
word_vec <- unlist(strsplit(lincoln_doc," "))
word_vec[1:20]
sort(table(word_vec),decreasing = TRUE)[1:10]
```

How do we remove all the uninteresting words ? We could do it manaully

```{r}
# Remove all punctuation marks
word_vec <- gsub("[[:punct:]]","",word_vec)
stop_words <- c("the","to","and","of","the","for","in","it",
                "a","this","which","by","is","an","hqs","from",
                "that","with","as")
for (ii in 1:length(stop_words)) {
    for (jj in 1:length(word_vec)) {
      if (stop_words[ii] == word_vec[jj]) {
          word_vec[jj] <- ""
} }
}
word_vec <- word_vec[word_vec != ""]
sort(table(word_vec),decreasing = TRUE)[1:10]
word_vec[1:30]
```

## tidytext

A better way would be to use the tidytext package. First we need to create a data frame out of the text. Then we "tokenize" the text which means we have one line per word. 

```{r}
library(tidytext)

text_df <- data_frame(line = 1:length(lincoln_doc), text = lincoln_doc)
token_text <- text_df %>%
  unnest_tokens(word, text)

token_text %>% count(word,sort=TRUE)

```

But we need to get rid of the "stop words"


```{r}
# Now remove stop words
data(stop_words)
tidy_text <- token_text %>%
  anti_join(stop_words)

# This could also be done by the following. I point this out only because some people react
# negatively to "joins" although fully understanding what joins are can only help you since
# much of what the dplyr package does is based on SQL type joins. 

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

tidy_text %>% count(word,sort=TRUE)
```

```{r}
tidy_text %>% count(word,sort=TRUE)

tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 2) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

## Back To The PubMed Example

We have around 935 abstracts that we can now mess with

```{r}
# Create a data frame out of the cleaned up abstracts

text_df <- data_frame(line = 1:length(my_abstracts), text = my_abstracts)
token_text <- text_df %>%
  unnest_tokens(word, text)

# Many of these words aren't helpful 
token_text %>% count(total=word,sort=TRUE)

# Now remove stop words
data(stop_words)
tidy_text <- token_text %>%
  anti_join(stop_words)

# This could also be done by the following. I point this out only because some people react
# negatively to "joins" although fully understanding what joins are can only help you since
# much of what the dplyr package does is based on SQL type joins. 

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

# Arrange the text by descending word frequency 

tidy_text %>%
  count(word, sort = TRUE) 
```

Some of the most frequently occurring words are in fact "dialysis", "patients" so maybe we should consider them to be stop words also since we already know quite well that the overall theme is, well, dialysis and kidneys. There are also synonymns and abbreviations that are somewhat redundant such as "pdd","pd","hhd" so let's eliminate them also. 


```{r}
tidy_text <- token_text %>%
   filter(!word %in% c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","disease"))

tidy_text %>%
  count(word, sort = TRUE) 
```


Let's do some plotting of these words


```{r}
tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 120) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```



Okay, it looks like there are numbers in there which might be useful. I suspect that the "95" is probably associated with the idea of a confidence interval. But there are other references to numbers. 

```{r}
grep("^[0-9]{1,3}$",tidy_text$word)[1:20]

tidy_text_nonum <- tidy_text[grep("^[0-9]{1,3}$",tidy_text$word,invert=TRUE),]

```


Okay well I think maybe we have some reasonable data to examine. As you might have realized by now, manipulating data to get it "clean" can be tedious and frustrating though it is an inevitable part of the process. 

```{r}
tidy_text_nonum %>%
  count(word, sort = TRUE) %>%
  filter(n > 120) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```



```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

bing_word_counts <- tidy_text_nonum %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(word,sentiment,sort=TRUE)
```


t the positive vs negative words

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```


Let's create a word cloud


```{r message=FALSE, warning=FALSE}
library(wordcloud)
#

tidy_text_nonum %>%  
  count(word) %>%
  with(wordcloud(word,n,max.words=100,colors=brewer.pal(8,"Dark2")))
```

## BiGrams

Let's look at bigrams. We need to go back to the cleaned abstracts and pair words to get phrase that might be suggestive of some sentiment


```{r}
text_df <- data_frame(line = 1:length(my_abstracts), text = my_abstracts)
dialysis_bigrams <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

dialysis_bigrams %>%
  count(bigram, sort = TRUE)

```


But we have to filter out stop words

```{r}
bigrams_sep <- dialysis_bigrams %>% 
  separate(bigram,c("word1","word2"),sep=" ")

stop_list <- c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","esrd","lt","95","0.001")

bigrams_filtered <- bigrams_sep %>% 
  filter(!word1 %in% stop_list) %>%
  filter(!word2 %in% stop_list)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>%  count(bigram, sort = TRUE) %>% print(n=25)
```



```{r}
bigram_counts %>%
  filter(n > 30) %>%
  ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) +
    geom_tile(alpha = 0.8, color = "white") +
    scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) +
    coord_flip() +
    theme_tq() +
    theme(legend.position = "right") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "first word in pair",
         y = "second word in pair")
```


<!--chapter:end:04-bow.Rmd-->

# Final Words

We have finished a nice book.

<!--chapter:end:05-summary.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

--- 
title: "Web Scraping with R"
author: "Steve Pittard"
date: "`r Sys.Date()`"
bibliography:
- book.bib
- packages.bib
description: This is in support of my talk for the Data Science group
documentclass: book
link-citations: yes
site: bookdown::bookdown_site
biblio-style: apalike
---

# Motivations

## Lots of Data For The Taking ? 

The web hosts lots of interesting data that you can ”scrape”. Some of it is stashed in data bases, behind APIs, or in free form text. Lots of people want to grab information of of Twitter or from user forums to see what people are thinking. There is a lot of valuable information out there for the taking although some web sites have "caught on" and either block programmatic access or they setup "pay walls" that require you to subscribe to an API for access. The New York Times does this. But there are lots of opportunities to get data. 

 | 
------|--------------------------------
tables|Fetch tables like from Wikipedia
forms|You can submit forms and fetch the results
css|You can access parts of a web site using style or css selectors
Tweets|Process tweets including emojis
Web Sites| User forums have lots of content
Instagram| Yes you can "scrape" photos also

## Web Scraping Can Be Ugly

Depending on what web sites you want to scrape the process can be involved and quite tedious. Many websites are very much aware that people are scraping so they offer Application Programming Interfaces (APIs) to make requests for information easier for the user and easier for the server administrators to control access. Most times the user must apply for a "key" to gain access. For premium sites, the key costs money. Some sites like Google and Wunderground (a popular weather site) allow some number of free accesses before they start charging you. Even so the results are typically returned in XML or JSON which then requires you to parse the result to get the information you want. In the best situation there is an R package that will wrap in the parsing and will return lists or data frames. 

Here is a summary:

* First. Always try to find an R package that will access a site (e.g. New York Times, Wunderground, PubMed). These packages (e.g. omdbapi, easyPubMed, RBitCoin, rtimes) provide a programmatic search interface and return data frames with little to no effort on your part. 

* If no package exists then hopefully there is an API that allows you to query the website and get results back in JSON or XML. I prefer JSON because it's "easier" and the packages for parsing JSON return lists which are native data structures to R. So you can easily turn results into data frames. You will ususally use the *rvest* package in conjunction with XML, and the RSJONIO packages. 

* If the Web site doesn't have an API then you will need to scrape text. This isn't hard but it is tedious. You will need to use *rvest* to parse HMTL elements. If you want to parse mutliple pages then you will need to use *rvest* to move to the other pages and possibly fill out forms. If there is a lot of Javascript then you might need to use RSelenium to programmatically manage the web page. 

## Understanding The Language of The Web

The Web has its own languages: HTML, CSS, Javascript

```{r eval=FALSE}
<h1>, <h2>, ..., <h6> Heading 1 and so on
<p> Paragraph elements
<ul> Unordered List
<ol> Ordered List
<li> List Element
<div> Division / Section
<table> Tables
<form> Web forms
```

So to be productive at scraping requires you to have some familiairty with HMTL XML, and CSS. Here we look at a very basic HTML file


```{r eval=FALSE}
<!DOCTYPE html>
<html>
  <body>
    <h1>My First Heading</h1>
    <p>My first paragraph.</p>
   </body>
</html>
```

![](./PICS/first_heading.png){width=450px}

\   

And you could apply some styling to this courtest of the CSS language which allows you to inject styles into plain HTML:

![](.//PICS/css.png){width=450px}

\  

![](.//PICS/css_page.png){width=450px}

There are a number of tools that allow us to inspect web pages and see "what is under the hood". Warning - I just discovered that one of my favorite browser plugins to find the xpaths and/or css paths of page elements is no longer supported under Firefox or Chrome. I've found a couple of replacements but they don't work as well. I'll research it more. 

\   

| 
------|--------------------------------
Selector Gadget | http://selectorgadget.com/
Firebug | https://getfirebug.com/ (now integrated into a version of Firefox)
Google Chrome | Right click to inspect a page element 
Google Chrome | View Developer - Developer Tools
Oxygen Editor | Can obtain via the Emory Software Express Site

## Useful Packages

You will use the following three primary packages to help you get data from various web pages: *rvest*, *XML*, and *RJSONIO*. Note that you won't always use them simultaneously but you might use them in pairs or individually depending on the task at hand. 

## Quick **rvest** tutorial

Now let's do a quick *rvest* tutorial:


```{r}
url <- "https://en.wikipedia.org/wiki/World_population"

(paragraphs <- read_html(url) %>% html_nodes("p"))

```


Then we might want to actually parse out those paragraphs into text:

```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
paragraphs <- read_html(url) %>% html_nodes("p") %>% html_text()
paragraphs[1:10]
```


Get some other types of HTML obejects. Let's get all the "h2" HTML elements:

```{r}

read_html(url) %>% html_nodes("h2") %>% html_text()
```

What about tables ? 


```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
tables <- read_html(url) %>% html_nodes("table") 
tables
```


## Example: Parsing A Table From Wikipedia

Look at the [Wikipedia Page](https://en.wikipedia.org/wiki/World_population) for world population:

https://en.wikipedia.org/wiki/World_population

* We can get any table we want using rvest
* We might have to experiment to figure out which one 
* Get the one that lists the ten most populous countries 
* I think this might be the 4th or 5th table on the page
* How do we get this ?

![](./PICS/worldpop.png){width=450px}


First we will load packages that will help us throughout this session.

```{r message=F, echo=FALSE}
library(rvest)
library(tidytext)
library(dplyr)
library(ggplot2)
library(rtweet)
library(tidyr)
library(wordcloud)
library(tidyquant)
```

In this case we'll need to figure out what number table it is we want. We could fetch
all the tables and then experiment to find the precise one. 

```{r}
library(rvest)

# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 
  
ten_most_populous <- ten_most_df %>% 
  html_nodes("table") %>% `[[`(6) %>% html_table()

# Let's get just the first three columns
ten_most_populous <- ten_most_populous[,2:4]

# Get some content - Change the column names
names(ten_most_populous) <- c("Country_Territory","Population","Date")

# Do reformatting on the columns to be actual numerics where appropriate
ten_most_populous %>% 
  mutate(Population=gsub(",","",Population)) %>% 
  mutate(Population=round(as.numeric(Population)/1e+06))  %>%
  ggplot(aes(x=Country_Territory,y=Population)) + geom_point() + 
  labs(y = "Population / 1,000,000") + coord_flip() +
  ggtitle("Top 10 Most Populous Countries")

```

In the above example we leveraged the fact that we were looking specifically for a table element and it became a project to locate the correct table number. This isn't always the case with more complicated websites in that the element we are trying to grab or scrape is contained within a nested structure that doesn't correspond neatly to a paragraph, link, heading, or table. This can be the case if the page is heavily styled with CSS or Javascript. We might have to work harder. But it's okay to try to use simple elements and then try to refine the search some more.

## Summary

* Need some basic HTML and CSS knowledge to find correct elements
* How to extract text from common elements
* How to extract text from specific elements
* Always have to do some text cleanup of data
* It usually takes multiple times to get it right

See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# XML and JSON {#xml}

This is where things get a little dicey because some web pages will return XML and JSON in response to inquiries and while these formats seem complicated they are actually doing you a really big favor by doing this since these formats can ususally be easily parsed using various packges. XML is a bit hard to get your head around and JSON is the new kid on the block which is easier to use. 

Since this isn't a full-on course lecture I'll keep it short as to how and why you would want to use these but any time you spend trying to better understand JSON (and XML) the better of you will be when parsing web pages. It's not such a big deal if all you are going to be parsing is raw text since the mthods we use to do that avoid XML and JSON although cleaning up raw text has its own problems. Let's revisit the Wikipedia example from the previous section.

```{r}
library(rvest)

# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 
  
ten_most_populous <- ten_most_df %>% 
  html_nodes("table") %>% `[[`(6) %>% html_table()
```

![](./PICS/xml.png){width=650px}

![](./PICS/xmls.png){width=450px}


Let's look at an XML file that has some basic content:
\   

```{r eval=FALSE}
<?xml version="1.0" encoding="UTF-8"?>
<bookstore>
  <book category="COOKING">
    <title lang="en">Everyday Italian</title>
    <author>Giada De Laurentiis</author>
    <year>2005</year>
    <price>30.00</price>
  </book>
  <book category="CHILDREN">
    <title lang="en">Harry Potter</title>
    <author>J K. Rowling</author>
    <year>2005</year>
    <price>29.99</price>
  </book>
  <book category="WEB">
    <title lang="en">Learning XML</title>
    <author>Erik T. Ray</author>
    <year>2003</year>
    <price>39.95</price>
  </book>
</bookstore>

```
Well we pulled out all tables and then, by experimentation, we isolated table 6 and got the content corresponding to that. But. Is there a more direct way to find the content ? There is. It requires us to install some helper plugins such as the xPath Finder for Firefox and Chrome. In reality there are a number of ways to find the XML Path or CSS Path for an element within a web page but this is a good one to start. 



![](./PICS/xpath_finder.png){width=350px}

Remeber that we want to find the table corresponding to the "10 Most Populous Countries". So we activate the *xPath* finder plugin and the highlight the element of interest. This takes some practice to get it right. Once you highlight the desired elment you will see the corresponding XPATH. Here is a screenshot of what I did. We can use the resulting path to directly access the table without first having to first pull out all tables and then trying to find the right one

![](./PICS/xpath_pop.png){width=350px}

\   
 
```{r}
# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_populous <- read_html(url) 

ten_most_df <- ten_most_populous %>% 
  html_nodes(xpath='/html/body/div[3]/div[3]/div[4]/div/table[5]') %>%
  html_table() 

# We have to get the first element of the list. 
ten_most_df <- ten_most_df[[1]]

ten_most_df
```

## Finding XPaths

![](./PICS/xml_path_1.png){width=650px}

\   

In addition to Browser Plugins there are standalone tools such as the Oxygen XML Editor which is availabel through the Emory Software Express Website. This is a comprehensive XML editor that will allow you to parse XML and develop paths to locate specific nodes within an XML document. If you find yourself working with websites with lots of XML then this will be useful. The Oxygen editor is free. 

![](./PICS/xml_path_10.png){width=350px}

Let's look at an XML file that has some basic content:
\   

```{r eval=FALSE}
<?xml version="1.0" encoding="UTF-8"?>
<bookstore>
  <book category="COOKING">
    <title lang="en">Everyday Italian</title>
    <author>Giada De Laurentiis</author>
    <year>2005</year>
    <price>30.00</price>
  </book>
  <book category="CHILDREN">
    <title lang="en">Harry Potter</title>
    <author>J K. Rowling</author>
    <year>2005</year>
    <price>29.99</price>
  </book>
  <book category="WEB">
    <title lang="en">Learning XML</title>
    <author>Erik T. Ray</author>
    <year>2003</year>
    <price>39.95</price>
  </book>
</bookstore>

```


![](./PICS/xml_path_2.png){width=650px}

\   

![](./PICS/xml_path_3.png){width=550px}


\  

![](./PICS/xml_path_4.png){width=550px}

\  

![](./PICS/xml_path_4.png){width=550px}

\  

![](./PICS/xml_path_6.png){width=550px}

![](./PICS/xml_path_7.png){width=550px}


![](./PICS/xml_path_8.png){width=550px}

![](./PICS/xml_path_9.png){width=550px}


## Example: GeoCoding With Google 

Let's run through an example of using the GeoCoding API with Google. They used to provide free access to this service and they still do although you are limited to a certain number. You have to sign up for an account and get an API key. Anyway, let's run through this example and then look at how I parsed the XML file that is returned by the Google GeoCoding API. Let's say that we wanted to get the latitude and longitude associated with the the address *1510 Clifton Rd, Atlanta, GA* which corresponds to the Rollins Research Building. 

First we will see an example of what Google returns in terms of XML. We can use some tools like Oxygen Editor (available free via Emory Software Express) to develop an appropriate XPATH expression to parse out the latitude and longitude information. 

```{r message=FALSE}

# https://maps.googleapis.com/maps/api/geocode/xml?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM

# https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM

myGeo <- function(address="1510 Clifton Rd Atlanta GA",form="xml") {
  library(XML)
  library(RCurl)
  
  geourl <- "https://maps.googleapis.com/maps/api/geocode/"
  key <- "AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM"
  address <- gsub(" ","+",address)
  
  add <- paste0(geourl,form,sep="?address=")
  add <- paste0(add,address,"&key=")
  geourl <- paste0(add,key)
  
  locale <- getURL(geourl)
  plocal <- xmlParse(locale,useInternalNodes=TRUE)
  
  # Okay let's extract the lat and lon
  latlon <- getNodeSet(plocal,"/GeocodeResponse/result/geometry/location/descendant::*")
  lat  <- as.numeric(xmlSApply(latlon,xmlValue))[1]
  lon  <- as.numeric(xmlSApply(latlon,xmlValue))[2]
  
  return(c(lat=lat,lng=lon))  
}

myGeo()
```
\  
Now. We could have saved the report to a file on our local computer and open it up with Oxygen editor and figure out what the approproate XPATH would be. This is basically what I did. Here is a screenshot of the session:



![](./PICS/oxygen2.png){width=650px}

\  
We could expand this considerable to process a number of addresses:


```{r message=FALSE}
namevec <- c("Atlanta GA", 
             "Birmingham AL", 
             "Seattle WA", 
             "Sacramento CA",
             "Denver CO", 
             "LosAngeles CA", 
             "Rochester NY")

cityList <- lapply(namevec,myGeo)

# Or to get a data frame

cities <- data.frame(city=namevec,do.call(rbind,cityList),
                     stringsAsFactors = FALSE)
cities


# Let's create a Map

library(leaflet)
m <- leaflet(data=cities)
m <- addTiles(m)
m <- addMarkers(m,popup=cities$city)

# Put up the Map - click on the markers
m
```

## Using JSON 

JSON is fast becoming the primary interchange format over XML although XML is still well supported. R has a number of packages to ease the parsing of JSON/ documents returned by web pages. Ususally you get back a *list* which is a native data type in R that can easily be manipulated into a data frame. Most web APIs provide an option for JSON or XML although some only provide JSON. 


![](./PICS/json.png){width=650px}

There are rules and regulations about how JSON is formed and we will learn them by example but you can look at the numerous tutotorials on the web to locate definitive references. See http://www.w3schools.com/json/  Here is an XML file that describes some employees. 

```{r eval=FALSE}
<employees>
     <employee>
         <firstName>John</firstName>
         <lastName>Doe</lastName>
     </employee>
     <employee>
         <firstName>Anna</firstName>
         <lastName>Smith</lastName>
     </employee>
     <employee>
         <firstName>Peter</firstName>
         <lastName>Jones</lastName>
     </employee>
 </employees>
```


And here is the corresposning JSON file:

```{r eval=FALSE}
{
  "employees":[
    {"firstName":"John", "lastName":"Doe"},
    {"firstName":"Anna", "lastName":"Smith"},
    {"firstName":"Peter","lastName":"Jones"}
] }
```


* It is important to note that the actual information in the document, things like city name, county name, latitude, and longitude are the same as they would be in the comparable XML document.

* JSON documents are at the heart of the NoSQL“database”called MongoDB

* JSON can be found within many webpages since it is closely related to JavaScript which is a language strongly related to web pages.

* JSON is very compact and lighweight which has made it a natural followon to XML so much so that it appears to be replacing XML


See http:..www.json.org/ for a full description of the specification

* An object is an unordered set of name/value pairs. An object begins with (left brace) and ends with (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma).

* An array is an ordered collection of values. An array begins with [ (left bracket) and ends with ] (right bracket). Values are separated by , (comma).

* A value can be a string in double quotes, or a number, or true or false or null, or an object or an array. These structures can be nested.

* A string is a sequence of zero or more Unicode characters, wrapped in double quotes, using backslash escapes. A character is represented as a single character string.


Do you remember the Google Geocding example from before ? We can tell Google to send us back JSON instead of XML just by adjusting the URL accordingly:

```{r}
url <- "https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Rd+Atlanta+GA&key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM"
```


![](./PICS/json_google.png){width=650px}

## Using the RJSONIO Package

To read/parse this in R we use a package called RJSONIO. There are other packages but this is the one we will be using. Download and install it.

There is a function called fromJSON which will parse the JSON file and return a list to contain the data.

So we parse lists instead of using XPath. Many people feel this to be easier than trying to construct XPath statments. You will have to decide for yourself.


```{r}
library(RJSONIO)

url <- "https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM"
geo <- fromJSON(url)
str(geo,3)
```

Since what we get back is a list we can directly access whatever we want. We just index into the list. No need for complicated XPATHS.

```{r}
geo$results[[1]]$geometry$location
```


<!--chapter:end:01-xmljson.Rmd-->

# More Real Life Examples {#Moreexamples}

```{r message=FALSE,echo=FALSE}
library(rvest)
library(tidytext)
library(dplyr)
library(ggplot2)
library(rtweet)
library(tidyr)
library(wordcloud)
library(tidyquant)
```


Okay. This is a tour of some sites that will serve as important examples on how to parse sites. Let's check the price of bitcoins. You want to be rich don't you ? 

## BitCoin Prices


![](./PICS/crypto.png){width=650px}

![](./PICS/view_source.png){width=750px}

```{r}
library(rvest)
url <- "https://coinmarketcap.com/all/views/all/"
bc <- read_html(url)

path <- '//*[@id="currencies-all"]'
 bc_table <- bc %>% html_nodes(xpath=path) %>% html_table()
 # We get back a one element list that is a data frame
 str(bc_table,0)
 bc_table <- bc_table[[1]]
 head(bc_table[,3:5])
```

```{r}
# The data is "dirty" and has characers in it that need cleaning
bc_table <- bc_table %>% select(Name,Symbol,Price)
bc_table <- bc_table %>% mutate(Name=gsub("\n"," ",Name))
bc_table <- bc_table %>% mutate(Name=gsub("\\.+","",Name))
bc_table <- bc_table %>% mutate(Price=gsub("\\$","",Price))
bc_table <- bc_table %>% mutate(Price=round(as.numeric(Price),2))

# There are four rows wherein the Price is missing NA
bc_table <- bc_table %>% filter(complete.cases(bc_table))

# Let's get the Crypto currencies with the Top 10 highest prices 
top_10 <- bc_table %>% arrange(desc(Price)) %>% head(10)

# Next we want to make a barplot of the Top 10
ylim=c(0,max(top_10$Price)+10000)
main="Top 10 Crypto Currencies in Terms of Price"
bp <- barplot(top_10$Price,col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
# Let's take the log of the price
ylim=c(0,max(log(top_10$Price))+5)
main="Top 10 Crypto Currencies in Terms of log(Price)"
bp <- barplot(log(top_10$Price),col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

## Faculty Salaries

In this example we have to parse the main table associated with the results page. 

![Salary](.//PICS/salary.png)

![Salary](.//PICS/sal_page.png)

```{r}
url <- "https://www.insidehighered.com/aaup-compensation-survey"
df <- read_html(url) %>% html_table() %>% `[[`(1)
intost <- c("Institution","Category","State")
salary <- df %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
salary
```


So we could process multiple pages


```{r eval=FALSE}
# So now we could process multiple pages

url <- 'https://www.insidehighered.com/aaup-compensation-survey?institution-name=&professor-category=1591&page=1'
str1 <- "https://www.insidehighered.com/aaup-compensation-survey?"
str2 <- "institution-name=&professor-category=1591&page="
intost <- c("Institution","Category","State")

salary <- data.frame()
for (ii in 1:2) {
  nurl  <- paste(str1,str2,ii,sep="")
  df <- read_html(nurl)
  tmp <- df %>% html_table() %>% `[[`(1)
  tmp <- tmp %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
  salary <- rbind(salary,tmp)
}

salary

```


Look at the URLs at the bottom of the main page to find beginning and ending page numbers. Visually this is easy. Programmatically we could  do something like the following:


```{r eval=FALSE}
# https://www.insidehighered.com/aaup-compensation-survey?page=1
# https://www.insidehighered.com/aaup-compensation-survey?page=94

# What is the last page number ? We already know the answer - 94

lastnum <- df %>% html_nodes(xpath='//a') %>% 
  html_attr("href") %>% '['(103) %>%
  strsplit(.,"page=") %>% '[['(1) %>% '['(2) %>% as.numeric(.)

# So now we could get all pages of the survey

str1 <- "https://www.insidehighered.com/aaup-compensation-survey?"
str2 <- "institution-name=&professor-category=1591&page="
intost <- c("Institution","Category","State")

salary <- data.frame()

for (ii in 1:lastnum) {
  nurl  <- paste(str1,str2,ii,sep="")
  df <- read_html(nurl)
  tmp <- df %>% html_table() %>% `[[`(1)
  tmp <- tmp %>% separate(InstitutionCategoryState,into=intost,sep="\n") 
  salary <- rbind(salary,tmp)
  Sys.sleep(1)
}
names(salary) <- c("Institution","Category","State","AvgSalFP","AvgChgFP",
                   "CntFP","AvgTotCompFP","SalEquityFP")

salary <- salary %>% 
  mutate(AvgSalFP=as.numeric(gsub("\\$|,","",salary$AvgSalFP))) %>%
  mutate(AvgTotCompFP=as.numeric(gsub("\\$|,","",salary$AvgTotCompFP)))

salary %>% group_by(State,Category) %>% 
  summarize(avg=mean(AvgSalFP)) %>% 
  arrange(desc(avg))
```

There are some problems:

* Data is large and scattered across multiple pages
* We could use above techniques to move from page to page
* There is a form we could use to narrow criteria
* But we have to programmatically submit the form
* rvest (and other packages) let you do this

## Filling Out Forms From a Program

![Salary](./PICS/salary_form.png){width=250px}

Let's find salaries between $ 150,000 and the default max ($ 244,000)

* Find the element name associated with "Average Salary"
* Establish a connection with the form (usually the url of the page)
* Get a local copy of the form
* Fill in the value for the "Average Salary"
* Submit the  lled in form
* Get the results and parse them like above
`

So finding the correct element is more challenging. I use Chrome to do this. Just highlight the area over the form and right click to "Insepct" the element. This opens up the developer tools. You have to dig down to find the corrext form and the element name. Here is a screen shot of my activity:

![Salary](./PICS/form.png){width=550px}

```{r}

url <- "https://www.insidehighered.com/aaup-compensation-survey"

# Establish a session
mysess <- html_session(url)

# Get the form

form_unfilled <- mysess %>% html_node("form") %>% html_form()
form_filled   <- form_unfilled %>% set_values("range-from"=150000)

# Submit form

results <- submit_form(mysess,form_filled)
first_page <- results %>% html_nodes(xpath=expr) %>% html_table()

first_page
```


## PubMed

Pubmed provides a rich source of information on published scientific literature. There are tutorials on how to leverage its capabilities but one thing to consider is that MESH terms are a good starting place since the search is index-based. MeSH (Medical Subject Headings) is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed. It's faster and more accurate so you can first use the MESH browser to generate the appropriate search terms and add that into the Search interface. The MESH browser can be found at https://www.ncbi.nlm.nih.gov/mesh/

![](./PICS/pubmed1.png){width=550px}


![](./PICS/mesh.png){width=550px}

What we do here is get the links associated with each publication so we can then process each of those and get the abstract associated with each publication. 

```{r}

# "hemodialysis, home" [MeSH Terms] 

url <- "https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D"

# 

results <- read_html(url) %>% 
  html_nodes("a") %>% 
  html_attr("href") %>%
  grep("/pubmed/[0-9]{1,6}",.,value=TRUE) %>% unique(.)

results
```

![](./PICS/pubmed_results.png){width=550px}

So now we could loop through these links and get the abstracts for these results. It looks that there are approximately 10 results per page. As before we would have to dive in to the underlying structure of the page to get the correct XML pathnames or we could look for Paragraph elements and pick out the links that way. 

```{r}

text.vec <- vector()

for (ii in 1:length(results)) {
  string <- paste0("https://www.ncbi.nlm.nih.gov",results[ii])
  text.vec[ii] <- read_html(string) %>% html_nodes("p") %>% `[[`(10) %>% html_text()
}

# Eliminate lines with newlines characters

final.vec <- text.vec[grep("^\n",text.vec,invert=TRUE)]

final.vec
```


Well that was a lot of work and it's very tedious. And we processed only the first page of results. How do we "progrmmatically" hit the "Next" Button at the bottom of the page ? This is complicated by the fact that there appears to be some Javascript at work that we would have to somehow interact with to get the URL for the next page. Unlike with the school salary example it isn't obvious how to do this. If we hove over the "Next" button we don't get an associated link.


<!--chapter:end:02-moreexamples.Rmd-->

# APIs {#APIs}


## OMDB 
Let's look at the IMDB page whic catalogues lots of information about movies. Just got to the web site and search although here is an example link. https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2 In this case we would like to get the summary information for the movie. So we would use Selector Gadget or some other method to find the XPATH or CSS associated with this element. 

![](./PICS/suspiria_77.png){width=450px}

This pretty easy and doesn't present much of a problem although for large scale mining of movie data we would run into trouble because IMDB doesn't really like you to scrape their pages. They have an API that they would like for you to use. 

```{r eval=FALSE}
url <- 'https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2'
summary <- read_html(url) %>% 
  html_nodes(".summary_text") %>%
  html_text()
```

But here we go again. We have to parse the desired elements on this page and then what if we wanted to follow other links or set up a general function to search IMDB for other movies of various genres, titles, directors, etc. 


![](./PICS/omdb.png){width=550px}


![](./PICS/omdb_api.png){width=450px}


![](./PICS/omdb_instruc.png){width=450px}


***
So as an example on how this works. Paste the URL into any web browser. You must supply your key for this to work. What you get back is a JSON formatted entry corresponding to ”The GodFather”movie.

***

```{r eval=FALSE}
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=The+Godfather"


```

![](./PICS/omdb_result.png){width=650px}


```{r}
library(RJSONIO)

url <- "http://www.omdbapi.com/?apikey=f7c004c&t=The+Godfather"

# Fetch the URL via fromJSON
movie <- fromJSON("http://www.omdbapi.com/?apikey=f7c004c&t=The+Godfather")

# We get back a list which is much easier to process than raw JSON or XML
str(movie)
```


```{r}
movie$Plot

sapply(movie$Ratings,unlist)
```

Let’s Get all the Episodes for Season 1 of Game of Thrones

```{r}
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=Game%20of%20Thrones&Season=1"
movie <- fromJSON(url)
str(movie,1)
episodes <- data.frame(do.call(rbind,movie$Episodes),stringsAsFactors = FALSE)
episodes

```


## The omdbapi package 

Wait a minute. Looks like someone created an R package that wraps all this for us. It is called omdbapi

![](./PICS/omdbapi.png){width=650px}


```{r eval=FALSE}
# Use devtools to install
devtools::install_github("hrbrmstr/omdbapi")
```

```{r}
library(omdbapi)
# The first time you use this you will be prompted to enter your
 # API key
movie_df <- search_by_title("Star Wars", page = 2)
(movie_df <- movie_df[,-5])


# Get lots of info on The GodFather

(gf <- find_by_title("The GodFather"))

# Get the actors from the GodFather
get_actors((gf))
```

![](./PICS/api_summary.png){width=550px}

## RSelenium

Sometimes we interact with websites that use Javascript to load more text or comments in a user forum. Here is an example of that. Look at https://www.dailystrength.org/group/dialysis which is a website associated with people wanting to share information about dialysis. 

![](./PICS/diasupport.png)

If you check the bottom of the pag you will see a button. 

![](./PICS/showmore.png)


```{r eval=FALSE}
# https://www.dailystrength.org/group/dialysis

library(RSelenium)
library(rvest)
library(tm)
library(SentimentAnalysis)
library(wordcloud)

url <- "https://www.dailystrength.org/group/dialysis"

# The website has a "show more" button that hides most of the patient posts
# If we don't find a way to programmatically "click" this button then we can
# only get a few of the posts and their responses. To do this we need to
# use the RSelenium package which does a lot of behind the scenes work

# See https://cran.r-project.org/web/packages/RSelenium/RSelenium.pdf
# http://brazenly.blogspot.com/2016/05/r-advanced-web-scraping-dynamic.html

# Open up a connection 

rD <- rsDriver()
remDr <- rD[["client"]]
remDr$navigate(url)

loadmorebutton <- remDr$findElement(using = 'css selector', "#load-more-discussions")

# Do this a number of times to get more links

loadmorebutton$clickElement()

# Now get the page with more comments and questions

page_source <- remDr$getPageSource()

# So let's parse the contents

comments <- read_html(page_source[[1]])

cumulative_comments <- vector()

links <- comments %>% html_nodes(css=".newsfeed__description")  %>% 
  html_node("a") %>% html_attr("href")

full_links <- paste0("https://www.dailystrength.org",links)

if (length(grep("NA",full_links)) > 0) {
  full_links <- full_links[-grep("NA",full_links)]
}

ugly_xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "comments__comment-text", " " ))] | //p'

for (ii in 1:length(full_links)) {
  text <- read_html(full_links[ii]) %>% 
    html_nodes(xpath=ugly_xpath) %>% 
    html_text() 
  length(text) <- length(text) - 1
  text <- text[-1]
  
  text
  
  cumulative_comments <- c(cumulative_comments,text)
}

remDr$close()
# stop the selenium server
rD[["server"]]$stop()

```


## EasyPubMed

So there is an R package called *EasyPubMed* that helps ease the access of data on the Internet. The idea behind this package is to be able to query NCBI Entrez and retrieve PubMed records in XML or TXT format.  The PubMed records can be downloaded and saved as XML or text files if desired.  According to the package authours, "Data integrity is enforced during data download, allowing to retrieve and save very large number of records effortlessly". The bottom line is that you can do what you want after that. Let's look at an example involving home hemodialysis


```{r}
library(easyPubMed)
```

Let's do some searching

```{r}
my_query <- 'hemodialysis, home" [MeSH Terms]'
my_entrez_id <- get_pubmed_ids(my_query)
my_abstracts_txt <- fetch_pubmed_data(my_entrez_id, format = "abstract")
my_abstracts_txt[1:20]

my_abstracts_xml <- fetch_pubmed_data(my_entrez_id)

```


Well the results aren't bad but we still have to parse out some junk but nothing like before. It's a minor problem. In this case the API gives us some XML back that we then have to parse. 

```{r}
my_abstracts <- unlist(xpathApply(my_abstracts_xml, "//AbstractText", saveXML))

# Still have to clear out some junk in each abstract

my_abstracts <- gsub('<(.*)">',"",my_abstracts)
my_abstracts <- gsub('<AbstractText>|</AbstractText>',"",my_abstracts)

length(my_abstracts)

```

But we have around 935 abstracts that we can now mess with. The benefit of this approach is that we dont' have to do all the text conversions and manipulations that we would have with the manual approach. Plus, we are able to get a lot more results more conveniently. 

```{r}
my_abstracts[1:3]
```




<!--chapter:end:03-apis.Rmd-->

# Bag of Words Sentiment Analysis {#bagofwords}

One we have a collection of text it's interesting to figure out what it might mean or infer - if anything at all. In text analysis and NLP (Natural Language Processing) we talk about "Bag of Words" to describe a collection or "corpus" of unstructured text. What do we do with a "bag of words" ? 

* Extract meaning from collections of text (without reading !) 
* Detect and analyze patterns in unstructured textual collections 
* Use Natural Language Processing techniques to reach conclusions 
* Discover what ideas occur in text and how they might be linked
* Determine if the discovered patterns be used to predict behavior ? 
* Identify interesting ideas that might otherwise be ignored


## Workflow

* Identify and Obtain text (e.g. websites, Twitter, Databases, PDFs, surveys) 
* Create a text ”Corpus”- a structure that contains the raw text
* Apply transformations:
    + Normalize case (convert to lower case)
    + Remove puncutation and stopwords
    + Remove domain specific stopwords
* Perform Analysis and Visualizations (word frequency, tagging, wordclouds) 
* Do Sentiment Analysis


R has Packages to Help. These are just some of them:

* QDAP - Quantitative Discourse Package
* tm - text mining applications within R
* tidytext - Text Mining using ddplyr and ggplot and tidyverse tools 
* SentimentAnalysis - For Sentiment Analysis

However, consider that:

* Some of these are easier to use than others
* Some can be kind of a problem to install (e.g. qdap) 
* They all offer similar capabilities
* We’ll look at tidytext


## Simple Example

![](./PICS/prez.png){width=650px}


Find the URL for Lincoln’s March 4, 1865 Speech:

```{r}
url <- "https://millercenter.org/the-presidency/presidential-speeches/march-4-1865-second-inaugural-address"
library(rvest)
lincoln_doc <- read_html(url) %>%
                    html_nodes(".view-transcript") %>%
                    html_text()
lincoln_doc

```


There are probably lots of words that don't really "matter" or contribute to the "real" meaning of the speech. 

```{r}
word_vec <- unlist(strsplit(lincoln_doc," "))
word_vec[1:20]
sort(table(word_vec),decreasing = TRUE)[1:10]
```

How do we remove all the uninteresting words ? We could do it manaully

```{r}
# Remove all punctuation marks
word_vec <- gsub("[[:punct:]]","",word_vec)
stop_words <- c("the","to","and","of","the","for","in","it",
                "a","this","which","by","is","an","hqs","from",
                "that","with","as")
for (ii in 1:length(stop_words)) {
    for (jj in 1:length(word_vec)) {
      if (stop_words[ii] == word_vec[jj]) {
          word_vec[jj] <- ""
} }
}
word_vec <- word_vec[word_vec != ""]
sort(table(word_vec),decreasing = TRUE)[1:10]
word_vec[1:30]
```

## tidytext

A better way would be to use the tidytext package. First we need to create a data frame out of the text. Then we "tokenize" the text which means we have one line per word. 

```{r}
library(tidytext)

text_df <- data_frame(line = 1:length(lincoln_doc), text = lincoln_doc)
token_text <- text_df %>%
  unnest_tokens(word, text)

token_text %>% count(word,sort=TRUE)

```

But we need to get rid of the "stop words"


```{r}
# Now remove stop words
data(stop_words)
tidy_text <- token_text %>%
  anti_join(stop_words)

# This could also be done by the following. I point this out only because some people react
# negatively to "joins" although fully understanding what joins are can only help you since
# much of what the dplyr package does is based on SQL type joins. 

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

tidy_text %>% count(word,sort=TRUE)
```

```{r}
tidy_text %>% count(word,sort=TRUE)

tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 2) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

## Back To The PubMed Example

We have around 935 abstracts that we can now mess with

```{r}
# Create a data frame out of the cleaned up abstracts

text_df <- data_frame(line = 1:length(my_abstracts), text = my_abstracts)
token_text <- text_df %>%
  unnest_tokens(word, text)

# Many of these words aren't helpful 
token_text %>% count(total=word,sort=TRUE)

# Now remove stop words
data(stop_words)
tidy_text <- token_text %>%
  anti_join(stop_words)

# This could also be done by the following. I point this out only because some people react
# negatively to "joins" although fully understanding what joins are can only help you since
# much of what the dplyr package does is based on SQL type joins. 

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

# Arrange the text by descending word frequency 

tidy_text %>%
  count(word, sort = TRUE) 
```

Some of the most frequently occurring words are in fact "dialysis", "patients" so maybe we should consider them to be stop words also since we already know quite well that the overall theme is, well, dialysis and kidneys. There are also synonymns and abbreviations that are somewhat redundant such as "pdd","pd","hhd" so let's eliminate them also. 


```{r}
tidy_text <- token_text %>%
   filter(!word %in% c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","disease"))

tidy_text %>%
  count(word, sort = TRUE) 
```


Let's do some plotting of these words


```{r}
tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 120) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```



Okay, it looks like there are numbers in there which might be useful. I suspect that the "95" is probably associated with the idea of a confidence interval. But there are other references to numbers. 

```{r}
grep("^[0-9]{1,3}$",tidy_text$word)[1:20]

tidy_text_nonum <- tidy_text[grep("^[0-9]{1,3}$",tidy_text$word,invert=TRUE),]

```


Okay well I think maybe we have some reasonable data to examine. As you might have realized by now, manipulating data to get it "clean" can be tedious and frustrating though it is an inevitable part of the process. 

```{r}
tidy_text_nonum %>%
  count(word, sort = TRUE) %>%
  filter(n > 120) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```



```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

bing_word_counts <- tidy_text_nonum %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(word,sentiment,sort=TRUE)
```


t the positive vs negative words

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```


Let's create a word cloud


```{r message=FALSE, warning=FALSE}
library(wordcloud)
#

tidy_text_nonum %>%  
  count(word) %>%
  with(wordcloud(word,n,max.words=100,colors=brewer.pal(8,"Dark2")))
```

## BiGrams

Let's look at bigrams. We need to go back to the cleaned abstracts and pair words to get phrase that might be suggestive of some sentiment


```{r}
text_df <- data_frame(line = 1:length(my_abstracts), text = my_abstracts)
dialysis_bigrams <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

dialysis_bigrams %>%
  count(bigram, sort = TRUE)

```


But we have to filter out stop words

```{r}
bigrams_sep <- dialysis_bigrams %>% 
  separate(bigram,c("word1","word2"),sep=" ")

stop_list <- c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","esrd","lt","95","0.001")

bigrams_filtered <- bigrams_sep %>% 
  filter(!word1 %in% stop_list) %>%
  filter(!word2 %in% stop_list)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>%  count(bigram, sort = TRUE) %>% print(n=25)
```



```{r}
bigram_counts %>%
  filter(n > 30) %>%
  ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) +
    geom_tile(alpha = 0.8, color = "white") +
    scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) +
    coord_flip() +
    theme_tq() +
    theme(legend.position = "right") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "first word in pair",
         y = "second word in pair")
```


<!--chapter:end:04-bow.Rmd-->

# Final Words

We have finished a nice book.

<!--chapter:end:05-summary.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->


<!--chapter:end:WebScraping.Rmd-->

