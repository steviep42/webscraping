[
["index.html", "Web Scraping with R Chapter 1 Motivations 1.1 Lots of Data For The Taking ? 1.2 Web Scraping Can Be Ugly 1.3 Understanding The Language of The Web 1.4 Useful Packages 1.5 Quick rvest tutorial 1.6 Example: Parsing A Table From Wikipedia 1.7 Scraping Patient Dialysis Stories 1.8 Summary", " Web Scraping with R Steve Pittard 2020-02-08 Chapter 1 Motivations 1.1 Lots of Data For The Taking ? The web hosts lots of interesting data that you can ”scrape”. Some of it is stashed in data bases, behind APIs, or in free form text. Lots of people want to grab information of of Twitter or from user forums to see what people are thinking. There is a lot of valuable information out there for the taking although some web sites have “caught on” and either block programmatic access or they setup “pay walls” that require you to subscribe to an API for access. The New York Times does this. But there are lots of opportunities to get data. tables Fetch tables like from Wikipedia forms You can submit forms and fetch the results css You can access parts of a web site using style or css selectors Tweets Process tweets including emojis Web Sites User forums have lots of content Instagram Yes you can “scrape” photos also 1.2 Web Scraping Can Be Ugly Depending on what web sites you want to scrape the process can be involved and quite tedious. Many websites are very much aware that people are scraping so they offer Application Programming Interfaces (APIs) to make requests for information easier for the user and easier for the server administrators to control access. Most times the user must apply for a “key” to gain access. For premium sites, the key costs money. Some sites like Google and Wunderground (a popular weather site) allow some number of free accesses before they start charging you. Even so the results are typically returned in XML or JSON which then requires you to parse the result to get the information you want. In the best situation there is an R package that will wrap in the parsing and will return lists or data frames. Here is a summary: First. Always try to find an R package that will access a site (e.g. New York Times, Wunderground, PubMed). These packages (e.g. omdbapi, easyPubMed, RBitCoin, rtimes) provide a programmatic search interface and return data frames with little to no effort on your part. If no package exists then hopefully there is an API that allows you to query the website and get results back in JSON or XML. I prefer JSON because it’s “easier” and the packages for parsing JSON return lists which are native data structures to R. So you can easily turn results into data frames. You will ususally use the rvest package in conjunction with XML, and the RSJONIO packages. If the Web site doesn’t have an API then you will need to scrape text. This isn’t hard but it is tedious. You will need to use rvest to parse HMTL elements. If you want to parse mutliple pages then you will need to use rvest to move to the other pages and possibly fill out forms. If there is a lot of Javascript then you might need to use RSelenium to programmatically manage the web page. 1.3 Understanding The Language of The Web The Web has its own languages: HTML, CSS, Javascript &lt;h1&gt;, &lt;h2&gt;, ..., &lt;h6&gt; Heading 1 and so on &lt;p&gt; Paragraph elements &lt;ul&gt; Unordered List &lt;ol&gt; Ordered List &lt;li&gt; List Element &lt;div&gt; Division / Section &lt;table&gt; Tables &lt;form&gt; Web forms So to be productive at scraping requires you to have some familiarity with HMTL XML, and CSS. Here we look at a very basic HTML file. Refer to See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html for a basic introductory session on HTML and webscraping with R &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;h1&gt;My First Heading&lt;/h1&gt; &lt;p&gt;My first paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; And you could apply some styling to this courtest of the CSS language which allows you to inject styles into plain HTML: 1.3.1 Useful tools There are a number of tools that allow us to inspect web pages and see “what is under the hood”. Warning - I just discovered that one of my favorite browser plugins (firebug) to find the xpaths and/or css paths of page elements is no longer supported under Firefox or Chrome. I’ve found a couple of replacements but they don’t work as well. I’ll research it more. The way that Selector Gadget and xPath work is that you install them into your browswer and then activate them whenever you need to identify the selector associated with a part of a web page. Selector Gadget http://selectorgadget.com/ Firebug https://getfirebug.com/ (now integrated into a version of Firefox) xPath https://addons.mozilla.org/en-US/firefox/addon/xpath_finder/ Google Chrome Right click to inspect a page element Google Chrome View Developer - Developer Tools Oxygen Editor Can obtain via the Emory Software Express Site 1.4 Useful Packages You will use the following three primary packages to help you get data from various web pages: rvest, XML, and RJSONIO. Note that you won’t always use them simultaneously but you might use them in pairs or individually depending on the task at hand. 1.5 Quick rvest tutorial Now let’s do a quick rvest tutorial. There are several steps involved in using rvest which are conceptually quite straightforward: Identify a URL to be examined for content Use Selector Gadet, xPath, or Google Insepct to identify the “selector” This will be a paragraph, table, hyper links, images Load rvest Use read_html to “read” the URL Pass the result to html_nodes to get the selectors identified in step number 2 Get the text or table content library(rvest) url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; (paragraphs &lt;- read_html(url) %&gt;% html_nodes(&quot;p&quot;)) ## {xml_nodeset (63)} ## [1] &lt;p class=&quot;mw-empty-elt&quot;&gt;\\n&lt;/p&gt; ## [2] &lt;p class=&quot;mw-empty-elt&quot;&gt;\\n&lt;/p&gt; ## [3] &lt;p&gt;In &lt;a href=&quot;/wiki/Demography&quot; title=&quot;Demography&quot;&gt;demographics&lt;/a ... ## [4] &lt;p&gt;The world population has experienced &lt;a href=&quot;/wiki/Population_g ... ## [5] &lt;p&gt;Total annual births were highest in the late 1980s at about 139 ... ## [6] &lt;p&gt;Six of the &lt;a href=&quot;/wiki/Earth&quot; title=&quot;Earth&quot;&gt;Earth&lt;/a&gt;&#39;s seven ... ## [7] &lt;p&gt;Estimates of world population by their nature are an aspect of &lt; ... ## [8] &lt;p&gt;It is difficult for estimates to be better than rough approximat ... ## [9] &lt;p&gt;Estimates of the population of the world at the time agriculture ... ## [10] &lt;p&gt;The &lt;a href=&quot;/wiki/Plague_of_Justinian&quot; title=&quot;Plague of Justini ... ## [11] &lt;p&gt;Starting in AD 2, the &lt;a href=&quot;/wiki/Han_Dynasty&quot; class=&quot;mw-redi ... ## [12] &lt;p&gt;The &lt;a href=&quot;/wiki/Pre-Columbian_era&quot; title=&quot;Pre-Columbian era&quot;&gt; ... ## [13] &lt;p&gt;During the European &lt;a href=&quot;/wiki/British_Agricultural_Revoluti ... ## [14] &lt;p&gt;Population growth in the West became more rapid after the introd ... ## [15] &lt;p&gt;The first half of the 20th century in &lt;a href=&quot;/wiki/Russian_Emp ... ## [16] &lt;p&gt;Many countries in the &lt;a href=&quot;/wiki/Developing_world&quot; class=&quot;mw ... ## [17] &lt;p&gt;It is estimated that the world population reached one billion fo ... ## [18] &lt;p&gt;According to current projections, the global population will rea ... ## [19] &lt;p&gt;There is no estimation for the exact day or month the world&#39;s po ... ## [20] &lt;p&gt;As of 2012, the global &lt;a href=&quot;/wiki/Human_sex_ratio&quot; title=&quot;Hu ... ## ... Then we might want to actually parse out those paragraphs into text: url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; paragraphs &lt;- read_html(url) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() paragraphs[1:10] ## [1] &quot;\\n&quot; ## [2] &quot;\\n&quot; ## [3] &quot;In demographics, the world population is the total number of humans currently living, and was estimated to have reached 7.7 billion people as of April 2019[update].[2] It took over 200,000 years of human history for the world&#39;s population to reach 1 billion,[3] and only 200 years more to reach 7 billion.[4]&quot; ## [4] &quot;The world population has experienced continuous growth following the Great Famine of 1315–1317 and the end of the Black Death in 1350, when it was near 370 million.[5] \\nThe highest global population growth rates, with increases of over 1.8% per year, occurred between 1955 and 1975—peaking to 2.1% between 1965 and 1970.[6] The growth rate declined to 1.2% between 2010 and 2015 and is projected to decline further in the course of the 21st century.[6] However, the global population is still increasing[7] and is projected to reach about 10 billion in 2050 and more than 11 billion in 2100.[8]&quot; ## [5] &quot;Total annual births were highest in the late 1980s at about 139 million,[9] and as of 2011 were expected to remain essentially constant at a level of 135 million,[10] while deaths numbered 56 million per year and were expected to increase to 80 million per year by 2040.[11] \\nThe median age of the world&#39;s population was estimated to be 30.4 years in 2018.[12] In mid-2019, the United Nations estimated that the world population had reached 7,713,468,000.[13]&quot; ## [6] &quot;Six of the Earth&#39;s seven continents are permanently inhabited on a large scale. Asia is the most populous continent, with its 4.54 billion inhabitants accounting for 60% of the world population. The world&#39;s two most populated countries, China and India, together constitute about 36% of the world&#39;s population. Africa is the second most populated continent, with around 1.28 billion people, or 16% of the world&#39;s population. Europe&#39;s 742 million people make up 10% of the world&#39;s population as of 2018, while the Latin American and Caribbean regions are home to around 651 million (9%). Northern America, primarily consisting of the United States and Canada, has a population of around 363 million (5%), and Oceania, the least populated region, has about 41 million inhabitants (0.5%).[15] Though it is not permanently inhabited by any fixed population, Antarctica has a very small, fluctuating international population of about 1000 people based mainly in polar science stations. This population tends to rise in the summer months and decrease significantly in winter, as visiting researchers return to their home countries.[16]&quot; ## [7] &quot;Estimates of world population by their nature are an aspect of modernity, possible only since the Age of Discovery. Early estimates for the population of the world[18] date to the 17th century: William Petty in 1682 estimated world population at 320 million (modern estimates ranging close to twice this number); by the late 18th century, estimates ranged close to one billion (consistent with modern estimates).[19] More refined estimates, broken down by continents, were published in the first half of the 19th century, at 600 to 1000 million in the early 1800s and at 800 to 1000 million in the 1840s.[20]&quot; ## [8] &quot;It is difficult for estimates to be better than rough approximations, as even modern population estimates are fraught with uncertainties on the order of 3% to 5%.[21]&quot; ## [9] &quot;Estimates of the population of the world at the time agriculture emerged in around 10,000 BC have ranged between 1 million and 15 million.[22][23] Even earlier, genetic evidence suggests humans may have gone through a population bottleneck of between 1,000 and 10,000 people about 70,000 BC, according to the Toba catastrophe theory. By contrast, it is estimated that around 50–60 million people lived in the combined eastern and western Roman Empire in the 4th century AD.[24]&quot; ## [10] &quot;The Plague of Justinian, which first emerged during the reign of the Roman emperor Justinian, caused Europe&#39;s population to drop by around 50% between the 6th and 8th centuries AD.[25] The population of Europe was more than 70 million in 1340.[26] The Black Death pandemic of the 14th century may have reduced the world&#39;s population from an estimated 450 million in 1340 to between 350 and 375 million in 1400;[27] it took 200 years for population figures to recover.[28] The population of China decreased from 123 million in 1200 to 65 million in 1393,[29] presumably from a combination of Mongol invasions, famine, and plague.[30]&quot; Get some other types of HTML obejects. Let’s get all the hyperlinks to other pages read_html(url) %&gt;% html_nodes(&quot;a&quot;) ## {xml_nodeset (1774)} ## [1] &lt;a id=&quot;top&quot;&gt;&lt;/a&gt; ## [2] &lt;a href=&quot;/wiki/Wikipedia:Protection_policy#semi&quot; title=&quot;This articl ... ## [3] &lt;a class=&quot;mw-jump-link&quot; href=&quot;#mw-head&quot;&gt;Jump to navigation&lt;/a&gt; ## [4] &lt;a class=&quot;mw-jump-link&quot; href=&quot;#p-search&quot;&gt;Jump to search&lt;/a&gt; ## [5] &lt;a href=&quot;/wiki/File:Question_book-new.svg&quot; class=&quot;image&quot;&gt;&lt;img alt=&quot; ... ## [6] &lt;a href=&quot;/wiki/Wikipedia:Citing_sources#Inline_citations&quot; title=&quot;Wi ... ## [7] &lt;a class=&quot;external text&quot; href=&quot;https://en.wikipedia.org/w/index.php ... ## [8] &lt;a href=&quot;/wiki/Wikipedia:Verifiability&quot; title=&quot;Wikipedia:Verifiabil ... ## [9] &lt;a href=&quot;/wiki/Template:Citation_needed&quot; title=&quot;Template:Citation n ... ## [10] &lt;a href=&quot;/wiki/Wikipedia:Identifying_reliable_sources&quot; class=&quot;mw-re ... ## [11] &lt;a href=&quot;/wiki/Wikipedia:Verifiability#Burden_of_evidence&quot; title=&quot;W ... ## [12] &lt;a href=&quot;/wiki/Help:Maintenance_template_removal&quot; title=&quot;Help:Maint ... ## [13] &lt;a href=&quot;/wiki/File:World_population_v3.svg&quot; class=&quot;image&quot;&gt;&lt;img alt ... ## [14] &lt;a href=&quot;/wiki/File:World_population_v3.svg&quot; class=&quot;internal&quot; title ... ## [15] &lt;a rel=&quot;nofollow&quot; class=&quot;external text&quot; href=&quot;http://esa.un.org/wpp ... ## [16] &lt;a rel=&quot;nofollow&quot; class=&quot;external text&quot; href=&quot;https://esa.un.org/un ... ## [17] &lt;a href=&quot;/wiki/File:Population_Growth_by_World_Bank_continental_div ... ## [18] &lt;a href=&quot;/wiki/File:Population_Growth_by_World_Bank_continental_div ... ## [19] &lt;a href=&quot;#cite_note-1&quot;&gt;[1]&lt;/a&gt; ## [20] &lt;a href=&quot;/wiki/File:World_population_percentage.png&quot; class=&quot;image&quot;&gt; ... ## ... What about tables ? url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; tables &lt;- read_html(url) %&gt;% html_nodes(&quot;table&quot;) tables ## {xml_nodeset (23)} ## [1] &lt;table class=&quot;box-Verify_sources plainlinks metadata ambox ambox-co ... ## [2] &lt;table class=&quot;infobox&quot; style=&quot;float: right; font-size:90%&quot;&gt;&lt;tbody&gt;\\ ... ## [3] &lt;table class=&quot;wikitable sortable&quot;&gt;\\n&lt;caption&gt;Population by continen ... ## [4] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center; float:right; cle ... ## [5] &lt;table width=&quot;100%&quot;&gt;&lt;tbody&gt;&lt;tr&gt;\\n&lt;td valign=&quot;top&quot;&gt; &lt;div class=&quot;lege ... ## [6] &lt;table class=&quot;wikitable sortable&quot; style=&quot;text-align:right&quot;&gt;&lt;tbody&gt;\\ ... ## [7] &lt;table class=&quot;wikitable sortable&quot; style=&quot;text-align:right&quot;&gt;\\n&lt;capti ... ## [8] &lt;table class=&quot;wikitable sortable&quot; style=&quot;text-align:right&quot;&gt;\\n&lt;capti ... ## [9] &lt;table class=&quot;wikitable sortable&quot; style=&quot;font-size:97%; text-align: ... ## [10] &lt;table class=&quot;wikitable sortable&quot; style=&quot;font-size:97%; text-align: ... ## [11] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:right;&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt; ... ## [12] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center; margin:1em; floa ... ## [13] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:right; font-size:96%;&quot;&gt;\\ ... ## [14] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center;&quot;&gt;\\n&lt;caption&gt;Star ... ## [15] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center;&quot;&gt;\\n&lt;caption&gt;Star ... ## [16] &lt;table role=&quot;presentation&quot; class=&quot;mbox-small plainlinks sistersiteb ... ## [17] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; ... ## [18] &lt;table class=&quot;nowraplinks mw-collapsible mw-collapsed navbox-inner&quot; ... ## [19] &lt;table class=&quot;nowraplinks hlist mw-collapsible autocollapse navbox- ... ## [20] &lt;table class=&quot;nowraplinks hlist mw-collapsible autocollapse navbox- ... ## ... 1.6 Example: Parsing A Table From Wikipedia Look at the Wikipedia Page for world population: https://en.wikipedia.org/wiki/World_population We can get any table we want using rvest We might have to experiment to figure out which one Get the one that lists the ten most populous countries I think this might be the 4th or 5th table on the page How do we get this ? First we will load packages that will help us throughout this session. In this case we’ll need to figure out what number table it is we want. We could fetch all the tables and then experiment to find the precise one. library(rvest) library(tidyr) library(dplyr) library(ggplot2) # Use read_html to fetch the webpage url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_df &lt;- read_html(url) ten_most_populous &lt;- ten_most_df %&gt;% html_nodes(&quot;table&quot;) %&gt;% `[[`(6) %&gt;% html_table() # Let&#39;s get just the first three columns ten_most_populous &lt;- ten_most_populous[,2:4] # Get some content - Change the column names names(ten_most_populous) &lt;- c(&quot;Country_Territory&quot;,&quot;Population&quot;,&quot;Date&quot;) # Do reformatting on the columns to be actual numerics where appropriate ten_most_populous %&gt;% mutate(Population=gsub(&quot;,&quot;,&quot;&quot;,Population)) %&gt;% mutate(Population=round(as.numeric(Population)/1e+06)) %&gt;% ggplot(aes(x=Country_Territory,y=Population)) + geom_point() + labs(y = &quot;Population / 1,000,000&quot;) + coord_flip() + ggtitle(&quot;Top 10 Most Populous Countries&quot;) In the above example we leveraged the fact that we were looking specifically for a table element and it became a project to locate the correct table number. This isn’t always the case with more complicated websites in that the element we are trying to grab or scrape is contained within a nested structure that doesn’t correspond neatly to a paragraph, link, heading, or table. This can be the case if the page is heavily styled with CSS or Javascript. We might have to work harder. But it’s okay to try to use simple elements and then try to refine the search some more. # Could have use the xPath plugin to help url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_df &lt;- read_html(url) ten_most_populous &lt;- ten_most_df %&gt;% html_nodes(xpath=&quot;/html/body/div[3]/div[3]/div[4]/div/table[5]&quot;) %&gt;% html_table() 1.7 Scraping Patient Dialysis Stories Here is an example relating to the experiences of dialysis patients with a specific dialysis provider. It might be more useful to find a support forum that is managed by dialysis patients to get more general opinions but this example is helpful in showing you what is involved. Check out this website: https://www.americanrenal.com/dialysis-centers/patient-stories 1.7.1 Getting More Detail In looking at this page you will see that there are a number of patient stories. Actually, there is a summary line followed by a “Read More” link that provides more detail on the patient experience. Our goal is to get the full content as opposed to only the summary. How would we do this ? 1.7.2 Writing Some Code Let’s use our new found knowledge of rvest to help us get these detailed stories. Maybe we want to do some sentiment analysis on this. If you hover over the Read More link on the website it will provide a specific link for each patient. For example, https://www.americanrenal.com/dialysis-centers/patient-stories/john-baguchinsky What we want to do is first get a list of all these links from the main page after which we can loop over each of the patient specific links and capture that information into a vector. Each element of the vector will be the content of a specific patient’s story. library(rvest) burl &lt;- &quot;https://www.americanrenal.com/dialysis-centers/patient-stories&quot; # Setup an empty vector to which we will add the content of each story workVector &lt;- vector() # Grab the links from the site that relate patient stories links &lt;- read_html(burl) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% grep(&quot;stories&quot;,.,value=TRUE) links ## [1] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories&quot; ## [2] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/randal-beatty&quot; ## [3] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/patricia-garcia&quot; ## [4] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/john-baguchinsky&quot; ## [5] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/sheryll-wyman&quot; ## [6] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/carol-sykes&quot; ## [7] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/sharon-cauthen&quot; ## [8] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/remond-ellis&quot; ## [9] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories&quot; ## [10] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories&quot; Some of these links do not correspond directly to a specific patient name so we need to filter those out. # Get only the ones that seem to have actual names associated with them storiesLinks &lt;- links[-grep(&quot;stories$&quot;,links)] storiesLinks ## [1] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/randal-beatty&quot; ## [2] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/patricia-garcia&quot; ## [3] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/john-baguchinsky&quot; ## [4] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/sheryll-wyman&quot; ## [5] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/carol-sykes&quot; ## [6] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/sharon-cauthen&quot; ## [7] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/remond-ellis&quot; Next we will visit each of these pages and scrape the text information. We’ll step through this in class so you can see this in action but here is the code. We will get each story and place each paragrpah of the story into a vector element. After that we will eliminate blank lines and some junk lines that begin with a new line character. Then we will collapse all of the vector text into a single paragraph and store it into a list element. Let’s step through it for the first link. # This corresponds to the first link # &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/randal-beatty&quot; tmpResult &lt;- read_html(storiesLinks[1]) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() tmpResult ## [1] &quot;Mr. Randal Beatty, University Kidney Center Hikes Lane&quot; ## [2] &quot;In April 2010, Randal Beatty was diagnosed with end stage renal disease (ESRD). The diagnosis came as a surprise, and, Mr. Beatty admits, he kept praying for a miracle.&quot; ## [3] &quot;“I heard all those stories about how people feel during dialysis and I didn’t want to deal with it,” said Mr. Beatty. “I didn’t want to lose my freedom and I certainly didn’t want to feel sick all the time.”&quot; ## [4] &quot;So for three years, he waited, hoping to find his miracle. But by August 2013, he knew he waited too long.&quot; ## [5] &quot;“Before my first dialysis treatment, I could feel myself pulling away from everyone, especially my family. I didn’t want to go anywhere or do anything and I was sick all the time. I realized at that point that how I was feeling was exactly what I wanted to avoid.”&quot; ## [6] &quot;He began his in-center dialysis treatments with American Renal Associates (ARA) at University Kidney Center in Louisville, Kentucky in August 2013 and transferred to another local ARA facility – University Kidney Center Hikes Lane – in May 2014 since it was closer to his home.&quot; ## [7] &quot;“In a short time, dialysis completely changed me,” he said. His health improved, along with his confidence, encouraging him to start driving himself to and from treatments. Not only did this give him a renewed feeling of independence, but a strong sense of accomplishment, as well.&quot; ## [8] &quot;Now 67 years old, Mr. Beatty says he can do everything he did before, including keeping up with his two granddaughters, playing basketball, among other hobbies, and going on family vacations. In fact, with the help of ARA’s Travel Department, Mr. Beatty can travel stress free. Though he admits traveling while on dialysis can be intimidating, he explained, “All I had to do was show up. ARA’s Travel Team took care of everything.”&quot; ## [9] &quot;Receiving a diagnosis of ESRD can be challenging, but Mr. Beatty’s advice is to take a step back and see dialysis as the miracle it is.&quot; ## [10] &quot;“It took me three years to realize that dialysis was the miracle I was waiting for. I was an extremely sick individual and just a few months on dialysis completely changed me. I don’t know why I waited as long as I did. I could have been enjoying life for the last few years rather than staying home sick. And I honestly haven’t been sick since I started my dialysis treatments!”&quot; ## [11] &quot;Read more patient stories&quot; ## [12] &quot;As of September 30, 2019&quot; ## [13] &quot;American Renal Associates operates 244 dialysis clinics in 27 states and Washington D.C., serving more than 17,100 patients with end stage renal disease in partnership with approximately 400 local nephrologists.&quot; ## [14] &quot;\\r\\n&quot; ## [15] &quot;Our core values emphasize taking good care of patients, providing physicians with clinical autonomy and support, hiring the best possible staff and providing best practices management.&quot; ## [16] &quot;&quot; ## [17] &quot;&quot; ## [18] &quot;\\n ©2016 American Renal® Associates. All Rights Reserved.500 Cummings Center, Suite\\n 6550, Beverly, MA, 01915\\n &quot; Okay, that has some junk in it like blank lines and lines that begin with new line characters. # Get rid of elements that are a blank line tmpResult &lt;- tmpResult[tmpResult!=&quot;&quot;] # Get rid of elements that begin with a newline character &quot;\\n&quot; newlines_begin &lt;- sum(grepl(&quot;^\\n&quot;,tmpResult)) if (newlines_begin &gt; 0) { tmpResult &lt;- tmpResult[-grep(&quot;^\\n&quot;,tmpResult)] } tmpResult ## [1] &quot;Mr. Randal Beatty, University Kidney Center Hikes Lane&quot; ## [2] &quot;In April 2010, Randal Beatty was diagnosed with end stage renal disease (ESRD). The diagnosis came as a surprise, and, Mr. Beatty admits, he kept praying for a miracle.&quot; ## [3] &quot;“I heard all those stories about how people feel during dialysis and I didn’t want to deal with it,” said Mr. Beatty. “I didn’t want to lose my freedom and I certainly didn’t want to feel sick all the time.”&quot; ## [4] &quot;So for three years, he waited, hoping to find his miracle. But by August 2013, he knew he waited too long.&quot; ## [5] &quot;“Before my first dialysis treatment, I could feel myself pulling away from everyone, especially my family. I didn’t want to go anywhere or do anything and I was sick all the time. I realized at that point that how I was feeling was exactly what I wanted to avoid.”&quot; ## [6] &quot;He began his in-center dialysis treatments with American Renal Associates (ARA) at University Kidney Center in Louisville, Kentucky in August 2013 and transferred to another local ARA facility – University Kidney Center Hikes Lane – in May 2014 since it was closer to his home.&quot; ## [7] &quot;“In a short time, dialysis completely changed me,” he said. His health improved, along with his confidence, encouraging him to start driving himself to and from treatments. Not only did this give him a renewed feeling of independence, but a strong sense of accomplishment, as well.&quot; ## [8] &quot;Now 67 years old, Mr. Beatty says he can do everything he did before, including keeping up with his two granddaughters, playing basketball, among other hobbies, and going on family vacations. In fact, with the help of ARA’s Travel Department, Mr. Beatty can travel stress free. Though he admits traveling while on dialysis can be intimidating, he explained, “All I had to do was show up. ARA’s Travel Team took care of everything.”&quot; ## [9] &quot;Receiving a diagnosis of ESRD can be challenging, but Mr. Beatty’s advice is to take a step back and see dialysis as the miracle it is.&quot; ## [10] &quot;“It took me three years to realize that dialysis was the miracle I was waiting for. I was an extremely sick individual and just a few months on dialysis completely changed me. I don’t know why I waited as long as I did. I could have been enjoying life for the last few years rather than staying home sick. And I honestly haven’t been sick since I started my dialysis treatments!”&quot; ## [11] &quot;Read more patient stories&quot; ## [12] &quot;As of September 30, 2019&quot; ## [13] &quot;American Renal Associates operates 244 dialysis clinics in 27 states and Washington D.C., serving more than 17,100 patients with end stage renal disease in partnership with approximately 400 local nephrologists.&quot; ## [14] &quot;\\r\\n&quot; ## [15] &quot;Our core values emphasize taking good care of patients, providing physicians with clinical autonomy and support, hiring the best possible staff and providing best practices management.&quot; Next, let’s create a more compact version of the data. We’ll cram it all into a single element. (tmpResult &lt;- paste(tmpResult,collapse=&quot;&quot;)) ## [1] &quot;Mr. Randal Beatty, University Kidney Center Hikes LaneIn April 2010, Randal Beatty was diagnosed with end stage renal disease (ESRD). The diagnosis came as a surprise, and, Mr. Beatty admits, he kept praying for a miracle.“I heard all those stories about how people feel during dialysis and I didn’t want to deal with it,” said Mr. Beatty. “I didn’t want to lose my freedom and I certainly didn’t want to feel sick all the time.”So for three years, he waited, hoping to find his miracle. But by August 2013, he knew he waited too long.“Before my first dialysis treatment, I could feel myself pulling away from everyone, especially my family. I didn’t want to go anywhere or do anything and I was sick all the time. I realized at that point that how I was feeling was exactly what I wanted to avoid.”He began his in-center dialysis treatments with American Renal Associates (ARA) at University Kidney Center in Louisville, Kentucky in August 2013 and transferred to another local ARA facility – University Kidney Center Hikes Lane – in May 2014 since it was closer to his home.“In a short time, dialysis completely changed me,” he said. His health improved, along with his confidence, encouraging him to start driving himself to and from treatments. Not only did this give him a renewed feeling of independence, but a strong sense of accomplishment, as well.Now 67 years old, Mr. Beatty says he can do everything he did before, including keeping up with his two granddaughters, playing basketball, among other hobbies, and going on family vacations. In fact, with the help of ARA’s Travel Department, Mr. Beatty can travel stress free. Though he admits traveling while on dialysis can be intimidating, he explained, “All I had to do was show up. ARA’s Travel Team took care of everything.”Receiving a diagnosis of ESRD can be challenging, but Mr. Beatty’s advice is to take a step back and see dialysis as the miracle it is.“It took me three years to realize that dialysis was the miracle I was waiting for. I was an extremely sick individual and just a few months on dialysis completely changed me. I don’t know why I waited as long as I did. I could have been enjoying life for the last few years rather than staying home sick. And I honestly haven’t been sick since I started my dialysis treatments!”Read more patient storiesAs of September 30, 2019American Renal Associates operates 244 dialysis clinics in 27 states and Washington D.C., serving more than 17,100 patients with end stage renal disease in partnership with approximately 400 local nephrologists.\\r\\nOur core values emphasize taking good care of patients, providing physicians with clinical autonomy and support, hiring the best possible staff and providing best practices management.&quot; So we could put this logic into a loop and process each of the links programmatically. # Now go to these pages and scrape the text necessary to # build a corpus tmpResult &lt;- vector() textList &lt;- list() for (ii in 1:length(storiesLinks)) { tmpResult &lt;- read_html(storiesLinks[ii]) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() # Get rid of elements that are a blank line tmpResult &lt;- tmpResult[tmpResult!=&quot;&quot;] # Get rid of elements that begin with a newline character &quot;\\n&quot; newlines_begin &lt;- sum(grepl(&quot;^\\n&quot;,tmpResult)) if (newlines_begin &gt; 0) { tmpResult &lt;- tmpResult[-grep(&quot;^\\n&quot;,tmpResult)] } # Let&#39;s collpase all the elements into a single element and then store # it into a list element so we can maintain each patient story separately # This is not necessary but until we figure out what we want to do with # the data then this gives us some options tmpResult &lt;- paste(tmpResult,collapse=&quot;&quot;) textList[[ii]] &lt;- tmpResult } If we did our job correctly then each element of the textList will have text in it corresponding to each patient textList[[1]] ## [1] &quot;Mr. Randal Beatty, University Kidney Center Hikes LaneIn April 2010, Randal Beatty was diagnosed with end stage renal disease (ESRD). The diagnosis came as a surprise, and, Mr. Beatty admits, he kept praying for a miracle.“I heard all those stories about how people feel during dialysis and I didn’t want to deal with it,” said Mr. Beatty. “I didn’t want to lose my freedom and I certainly didn’t want to feel sick all the time.”So for three years, he waited, hoping to find his miracle. But by August 2013, he knew he waited too long.“Before my first dialysis treatment, I could feel myself pulling away from everyone, especially my family. I didn’t want to go anywhere or do anything and I was sick all the time. I realized at that point that how I was feeling was exactly what I wanted to avoid.”He began his in-center dialysis treatments with American Renal Associates (ARA) at University Kidney Center in Louisville, Kentucky in August 2013 and transferred to another local ARA facility – University Kidney Center Hikes Lane – in May 2014 since it was closer to his home.“In a short time, dialysis completely changed me,” he said. His health improved, along with his confidence, encouraging him to start driving himself to and from treatments. Not only did this give him a renewed feeling of independence, but a strong sense of accomplishment, as well.Now 67 years old, Mr. Beatty says he can do everything he did before, including keeping up with his two granddaughters, playing basketball, among other hobbies, and going on family vacations. In fact, with the help of ARA’s Travel Department, Mr. Beatty can travel stress free. Though he admits traveling while on dialysis can be intimidating, he explained, “All I had to do was show up. ARA’s Travel Team took care of everything.”Receiving a diagnosis of ESRD can be challenging, but Mr. Beatty’s advice is to take a step back and see dialysis as the miracle it is.“It took me three years to realize that dialysis was the miracle I was waiting for. I was an extremely sick individual and just a few months on dialysis completely changed me. I don’t know why I waited as long as I did. I could have been enjoying life for the last few years rather than staying home sick. And I honestly haven’t been sick since I started my dialysis treatments!”Read more patient storiesAs of September 30, 2019American Renal Associates operates 244 dialysis clinics in 27 states and Washington D.C., serving more than 17,100 patients with end stage renal disease in partnership with approximately 400 local nephrologists.\\r\\nOur core values emphasize taking good care of patients, providing physicians with clinical autonomy and support, hiring the best possible staff and providing best practices management.&quot; 1.8 Summary Need some basic HTML and CSS knowledge to find correct elements How to extract text from common elements How to extract text from specific elements Always have to do some text cleanup of data It usually takes multiple times to get it right See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html "],
["xml.html", "Chapter 2 XML and JSON 2.1 Finding XPaths 2.2 Example: GeoCoding With Google 2.3 Using JSON 2.4 Using the RJSONIO Package", " Chapter 2 XML and JSON This is where things get a little dicey because some web pages will return XML and JSON in response to inquiries and while these formats seem complicated they are actually doing you a really big favor by doing this since these formats can ususally be easily parsed using various packges. XML is a bit hard to get your head around and JSON is the new kid on the block which is easier to use. Since this isn’t a full-on course lecture I’ll keep it short as to how and why you would want to use these but any time you spend trying to better understand JSON (and XML) the better of you will be when parsing web pages. It’s not such a big deal if all you are going to be parsing is raw text since the mthods we use to do that avoid XML and JSON although cleaning up raw text has its own problems. Let’s revisit the Wikipedia example from the previous section. library(rvest) # Use read_html to fetch the webpage url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_df &lt;- read_html(url) ten_most_populous &lt;- ten_most_df %&gt;% html_nodes(&quot;table&quot;) %&gt;% `[[`(6) %&gt;% html_table() Let’s look at an XML file that has some basic content: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;bookstore&gt; &lt;book category=&quot;COOKING&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Everyday Italian&lt;/title&gt; &lt;author&gt;Giada De Laurentiis&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;30.00&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;CHILDREN&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;WEB&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Learning XML&lt;/title&gt; &lt;author&gt;Erik T. Ray&lt;/author&gt; &lt;year&gt;2003&lt;/year&gt; &lt;price&gt;39.95&lt;/price&gt; &lt;/book&gt; &lt;/bookstore&gt; Well we pulled out all tables and then, by experimentation, we isolated table 6 and got the content corresponding to that. But. Is there a more direct way to find the content ? There is. It requires us to install some helper plugins such as the xPath Finder for Firefox and Chrome. In reality there are a number of ways to find the XML Path or CSS Path for an element within a web page but this is a good one to start. Remeber that we want to find the table corresponding to the “10 Most Populous Countries”. So we activate the xPath finder plugin and the highlight the element of interest. This takes some practice to get it right. Once you highlight the desired elment you will see the corresponding XPATH. Here is a screenshot of what I did. We can use the resulting path to directly access the table without first having to first pull out all tables and then trying to find the right one # Use read_html to fetch the webpage url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_populous &lt;- read_html(url) ten_most_df &lt;- ten_most_populous %&gt;% html_nodes(xpath=&#39;/html/body/div[3]/div[3]/div[4]/div/table[5]&#39;) %&gt;% html_table() # We have to get the first element of the list. ten_most_df &lt;- ten_most_df[[1]] ten_most_df ## Rank Country / Territory Population % of worldpopulation ## 1 1 China[note 4] 1,401,231,560 18.0% ## 2 2 India 1,358,326,450 17.5% ## 3 3 United States 329,293,594 4.24% ## 4 4 Indonesia 265,015,300 3.41% ## 5 5 Pakistan 212,742,631 2.74% ## 6 6 Brazil 211,093,998 2.72% ## 7 7 Nigeria 188,500,000 2.43% ## 8 8 Bangladesh 168,062,490 2.16% ## 9 9 Russia[note 5] 146,877,088 1.89% ## 10 10 Mexico 126,577,691 1.63% ## Date Source ## 1 7 Feb 2020 [87] ## 2 7 Feb 2020 [88] ## 3 7 Feb 2020 [89] ## 4 1 July 2018 [90] ## 5 25 May 2017 [91] ## 6 7 Feb 2020 [92] ## 7 31 October 2016 [93] ## 8 7 Feb 2020 [94] ## 9 1 January 2018 [95] ## 10 1 July 2019 [96] 2.1 Finding XPaths In addition to Browser Plugins there are standalone tools such as the Oxygen XML Editor which is availabel through the Emory Software Express Website. This is a comprehensive XML editor that will allow you to parse XML and develop paths to locate specific nodes within an XML document. If you find yourself working with websites with lots of XML then this will be useful. The Oxygen editor is free. Let’s look at an XML file that has some basic content: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;bookstore&gt; &lt;book category=&quot;COOKING&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Everyday Italian&lt;/title&gt; &lt;author&gt;Giada De Laurentiis&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;30.00&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;CHILDREN&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;WEB&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Learning XML&lt;/title&gt; &lt;author&gt;Erik T. Ray&lt;/author&gt; &lt;year&gt;2003&lt;/year&gt; &lt;price&gt;39.95&lt;/price&gt; &lt;/book&gt; &lt;/bookstore&gt; 2.2 Example: GeoCoding With Google Let’s run through an example of using the GeoCoding API with Google. They used to provide free access to this service but no more. You have to sign up for an account and get an API key. If you are currently taking one of my classes I probably have arranged for cloud credits that you can use to do Google Geocoding for free. So one way to do this is to create a URL according to the specification given in the Google Geocoding documentation. We need 1) the base Google URL for the Geocoding service, 2) the format of the desired output (XML or JSON), 3) and address for which we want to find the latitude and longitude, and 4) the API key we create at the Google API service. Here is a fully functional URL you can paste into your browser: https://maps.googleapis.com/maps/api/geocode/xml?address=1510+Clifton+Road,+Atlanta,+GA&amp;key=AIzaSyDPwt1Ya79b7lhsZkh75BjCz-GpMKC9ZYw If you paste this into Chome then you get something back like this: So I could create an R function to take care of this kind of thing so I could maybe pass in arbitrary addressess to be geocided. Let’s run through this example and then look at how I parsed the XML file that is returned by the Google GeoCoding API. We will stick with the 1510 Clifton Rd, Atlanta, GA address which corresponds to the Rollins Research Building. First we will see an example of what Google returns in terms of XML. We can use some tools like Oxygen Editor (available free via Emory Software Express) to develop an appropriate XPATH expression to parse out the latitude and longitude information. # https://maps.googleapis.com/maps/api/geocode/xml?address=1510+Clifton+Road,+Atlanta,+GA&amp;key=AIzaSyDPwt1Ya79b7lhsZkh75BjCz-GpMKC9ZYw # https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&amp;key=AIzaSyDPwt1Ya79b7lhsZkh75BjCz-GpMKC9ZYw myGeo &lt;- function(address=&quot;1510 Clifton Rd Atlanta GA&quot;,form=&quot;xml&quot;) { library(XML) library(RCurl) geourl &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/&quot; # You will need to replace this with your OWN key ! key &lt;- &quot;AIzaSyA3ereIVEjA0gPrxLupTPLO3GH_v98KpMA&quot; address &lt;- gsub(&quot; &quot;,&quot;+&quot;,address) add &lt;- paste0(geourl,form,sep=&quot;?address=&quot;) add &lt;- paste0(add,address,&quot;&amp;key=&quot;) geourl &lt;- paste0(add,key) locale &lt;- getURL(geourl) plocal &lt;- xmlParse(locale,useInternalNodes=TRUE) # Okay let&#39;s extract the lat and lon latlon &lt;- getNodeSet(plocal,&quot;/GeocodeResponse/result/geometry/location/descendant::*&quot;) lat &lt;- as.numeric(xmlSApply(latlon,xmlValue))[1] lon &lt;- as.numeric(xmlSApply(latlon,xmlValue))[2] return(c(lat=lat,lng=lon)) } mylocs &lt;- myGeo() lat lng 33.79667 -84.32319 Now. We could have saved the report to a file on our local computer and open it up with Oxygen editor and figure out what the approproate XPATH would be. This is basically what I did. Here is a screenshot of the session. I picked an XPATH expression of //location/descendant::* We could expand this considerable to process a number of addresses. This is a great example of how once you get a single example working then you can generalize this into a function that will allow you to do the same thing for a much larger numnber of addressess. namevec &lt;- c(&quot;Atlanta GA&quot;, &quot;Birmingham AL&quot;, &quot;Seattle WA&quot;, &quot;Sacramento CA&quot;, &quot;Denver CO&quot;, &quot;LosAngeles CA&quot;, &quot;Rochester NY&quot;) cityList &lt;- lapply(namevec,myGeo,eval=FALSE) # Or to get a data frame cities &lt;- data.frame(city=namevec,do.call(rbind,cityList), stringsAsFactors = FALSE) cities ## city lat lng ## 1 Atlanta GA 33.74900 -84.38798 ## 2 Birmingham AL 33.51859 -86.81036 ## 3 Seattle WA 47.60621 -122.33207 ## 4 Sacramento CA 38.58157 -121.49440 ## 5 Denver CO 39.73924 -104.99025 ## 6 LosAngeles CA 34.05223 -118.24368 ## 7 Rochester NY 43.15658 -77.60885 # Let&#39;s create a Map library(leaflet) m &lt;- leaflet(data=cities) m &lt;- addTiles(m) m &lt;- addMarkers(m,popup=cities$city) ## Assuming &quot;lng&quot; and &quot;lat&quot; are longitude and latitude, respectively # Put up the Map - click on the markers m 2.3 Using JSON JSON is fast becoming the primary interchange format over XML although XML is still well supported. R has a number of packages to ease the parsing of JSON/ documents returned by web pages. Ususally you get back a list which is a native data type in R that can easily be manipulated into a data frame. Most web APIs provide an option for JSON or XML although some only provide JSON. There are rules and regulations about how JSON is formed and we will learn them by example but you can look at the numerous tutotorials on the web to locate definitive references. See http://www.w3schools.com/json/ Here is an XML file that describes some employees. &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;John&lt;/firstName&gt; &lt;lastName&gt;Doe&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Anna&lt;/firstName&gt; &lt;lastName&gt;Smith&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Peter&lt;/firstName&gt; &lt;lastName&gt;Jones&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; And here is the corresposning JSON file: { &quot;employees&quot;:[ {&quot;firstName&quot;:&quot;John&quot;, &quot;lastName&quot;:&quot;Doe&quot;}, {&quot;firstName&quot;:&quot;Anna&quot;, &quot;lastName&quot;:&quot;Smith&quot;}, {&quot;firstName&quot;:&quot;Peter&quot;,&quot;lastName&quot;:&quot;Jones&quot;} ] } It is important to note that the actual information in the document, things like city name, county name, latitude, and longitude are the same as they would be in the comparable XML document. JSON documents are at the heart of the NoSQL“database”called MongoDB JSON can be found within many webpages since it is closely related to JavaScript which is a language strongly related to web pages. JSON is very compact and lighweight which has made it a natural followon to XML so much so that it appears to be replacing XML See http:..www.json.org/ for a full description of the specification An object is an unordered set of name/value pairs. An object begins with (left brace) and ends with (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma). An array is an ordered collection of values. An array begins with [ (left bracket) and ends with ] (right bracket). Values are separated by , (comma). A value can be a string in double quotes, or a number, or true or false or null, or an object or an array. These structures can be nested. A string is a sequence of zero or more Unicode characters, wrapped in double quotes, using backslash escapes. A character is represented as a single character string. Do you remember the Google Geocoding example from before ? We can tell Google to send us back JSON instead of XML just by adjusting the URL accordingly: url &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Rd+Atlanta+GA&amp;key=AIzaSyD0zIyn2ijIqb7OKYTGnAnchXY7zt3VB9Y&quot; 2.4 Using the RJSONIO Package To read/parse this in R we use a package called RJSONIO. There are other packages but this is the one we will be using. Download and install it. There is a function called fromJSON which will parse the JSON file and return a list to contain the data. So we parse lists instead of using XPath. Many people feel this to be easier than trying to construct XPath statments. You will have to decide for yourself. library(RJSONIO) url &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&amp;key=AIzaSyD0zIyn2ijIqb7OKYTGnAnchXY7zt3VB9Y&quot; geo &lt;- fromJSON(url) Since what we get back is a list we can directly access whatever we want. We just index into the list. No need for complicated XPATHS. str(geo,3) ## List of 2 ## $ results:List of 1 ## ..$ :List of 6 ## .. ..$ address_components:List of 7 ## .. ..$ formatted_address : chr &quot;1510 Clifton Rd, Atlanta, GA 30322, USA&quot; ## .. ..$ geometry :List of 3 ## .. ..$ place_id : chr &quot;ChIJ5QjdF_oG9YgRWAJzCm19Vf8&quot; ## .. ..$ plus_code : Named chr [1:2] &quot;QMWG+MP Druid Hills, Georgia, United States&quot; &quot;865QQMWG+MP&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;compound_code&quot; &quot;global_code&quot; ## .. ..$ types : chr &quot;street_address&quot; ## $ status : chr &quot;OK&quot; geo$results[[1]]$geometry$location ## lat lng ## 33.79667 -84.32319 Let’s put this into a function that helps us get the information for a number of addresses myGeo &lt;- function(address=&quot;1510 Clifton Rd Atlanta GA&quot;,form=&quot;json&quot;) { library(RJSONIO) geourl &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/&quot; # You will need to replace this with your OWN key ! key &lt;- &quot;AIzaSyA3ereIVEjA0gPrxLupTWRPOLH_v89DpMA&quot; address &lt;- gsub(&quot; &quot;,&quot;+&quot;,address) add &lt;- paste0(geourl,form,sep=&quot;?address=&quot;) add &lt;- paste0(add,address,&quot;&amp;key=&quot;) geourl &lt;- paste0(add,key) geo &lt;- fromJSON(geourl) lat &lt;- geo$results[[1]]$geometry$location[1] lng &lt;- geo$results[[1]]$geometry$location[2] return(c(lat,lng)) } Consider the following: namevec &lt;- c(&quot;Atlanta GA&quot;, &quot;Birmingham AL&quot;, &quot;Seattle WA&quot;, &quot;Sacramento CA&quot;, &quot;Denver CO&quot;, &quot;LosAngeles CA&quot;, &quot;Rochester NY&quot;) cityList &lt;- lapply(namevec,myGeo) # Or to get a data frame cities &lt;- data.frame(city=namevec,do.call(rbind,cityList), stringsAsFactors = FALSE) Now we can check out the geocoding cities and then make a map cities ## city lat lng ## 1 Atlanta GA 33.74900 -84.38798 ## 2 Birmingham AL 33.51859 -86.81036 ## 3 Seattle WA 47.60621 -122.33207 ## 4 Sacramento CA 38.58157 -121.49440 ## 5 Denver CO 39.73924 -104.99025 ## 6 LosAngeles CA 34.05223 -118.24368 ## 7 Rochester NY 43.15658 -77.60885 # Let&#39;s create a Map library(leaflet) m &lt;- leaflet(data=cities) m &lt;- addTiles(m) m &lt;- addMarkers(m,popup=cities$city) ## Assuming &quot;lng&quot; and &quot;lat&quot; are longitude and latitude, respectively # Put up the Map - click on the markers m "],
["Moreexamples.html", "Chapter 3 More Real Life Examples 3.1 BitCoin Prices 3.2 IMDB 3.3 Faculty Salaries 3.4 Filling Out Forms From a Program 3.5 PubMed", " Chapter 3 More Real Life Examples Okay. This is a tour of some sites that will serve as important examples on how to parse sites. Let’s check the price of bitcoins. You want to be rich don’t you ? 3.1 BitCoin Prices The challenge here is that it’s all one big table and it’s not clear how to adress it. And the owners of the web site will ususally change the format or start using Javascript or HTML5 which will mess things up in the future. One solid approach I frequently use is to simply pull out all the tables and, by experimentation, try to figure out which one has the information I want. This always require some work. library(rvest) url &lt;- &quot;https://coinmarketcap.com/all/views/all/&quot; bc &lt;- read_html(url) bc_table &lt;- bc %&gt;% html_nodes(&#39;table&#39;) %&gt;% html_table() %&gt;% .[[3]] # We get back a one element list that is a data frame str(bc_table,0) ## &#39;data.frame&#39;: 200 obs. of 11 variables: bc_table &lt;- bc_table[,c(2:3,5)] head(bc_table) ## Name Symbol Price ## 1 Bitcoin BTC $9,831.22 ## 2 Ethereum ETH $224.68 ## 3 XRP XRP $0.277099 ## 4 Bitcoin Cash BCH $445.80 ## 5 Bitcoin SV BSV $337.48 ## 6 Litecoin LTC $77.16 Everything is a character at this point so we have to go in an do some surgery on the data frame to turn the Price into an actual numeric. # The data is &quot;dirty&quot; and has characers in it that need cleaning bc_table &lt;- bc_table %&gt;% mutate(Price=gsub(&quot;\\\\$&quot;,&quot;&quot;,Price)) bc_table &lt;- bc_table %&gt;% mutate(Price=gsub(&quot;,&quot;,&quot;&quot;,Price)) bc_table &lt;- bc_table %&gt;% mutate(Price=round(as.numeric(Price),2)) # There are four rows wherein the Price is missing NA bc_table &lt;- bc_table %&gt;% filter(complete.cases(bc_table)) # Let&#39;s get the Crypto currencies with the Top 10 highest prices top_10 &lt;- bc_table %&gt;% arrange(desc(Price)) %&gt;% head(10) top_10 ## Name Symbol Price ## 1 Bitcoin BTC 9831.22 ## 2 Maker MKR 583.28 ## 3 Bitcoin Cash BCH 445.80 ## 4 Bitcoin SV BSV 337.48 ## 5 Ethereum ETH 224.68 ## 6 Dash DASH 128.66 ## 7 Monero XMR 81.20 ## 8 Litecoin LTC 77.16 ## 9 Zcash ZEC 70.75 ## 10 DigixDAO DGD 41.17 Let’s make a barplot of the top 10 crypto currencies. # Next we want to make a barplot of the Top 10 ylim=c(0,max(top_10$Price)+10000) main=&quot;Top 10 Crypto Currencies in Terms of Price&quot; bp &lt;- barplot(top_10$Price,col=&quot;aquamarine&quot;, ylim=ylim,main=main) axis(1, at=bp, labels=top_10$Symbol, cex.axis = 0.7) grid() So that didn’t work out so well since one of the crypto currencies dominates the others in terms of price. So let’s create a log transformed verion of the plot. # Let&#39;s take the log of the price ylim=c(0,max(log(top_10$Price))+5) main=&quot;Top 10 Crypto Currencies in Terms of log(Price)&quot; bp &lt;- barplot(log(top_10$Price),col=&quot;aquamarine&quot;, ylim=ylim,main=main) axis(1, at=bp, labels=top_10$Symbol, cex.axis = 0.7) grid() 3.2 IMDB Look at this example from IMDb (Internet Movie Database). According to Wikipedia: IMDb (Internet Movie Database)[2] is an online database of information related to films, television programs, home videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, fan and critical reviews, and ratings. We can search or refer to specific movies by URL if we wanted. For example, consider the following link to the “Lego Movie”: http://www.imdb.com/title/tt1490017/ In terms of scraping information from this site we could do that using the rvest package. Let’s say that we wanted to capture the rating information which is 7.8 out of 10. We could use the xPath Tool or the Selector gadet tool to zone in on this information. According to selector gadget we have the following xpath expression: url &lt;- &quot;http://www.imdb.com/title/tt1490017/&quot; lego_movie &lt;- read_html(url) # Scrape the website for the movie rating rating &lt;- lego_movie %&gt;% html_nodes(&quot;.ratingValue span&quot;) %&gt;% html_text() So that gives us what we need albeit in character form. Now it’s a simple matter of parsing out the first rating value: (rating &lt;- as.numeric(rating[1])) ## [1] 7.8 That wasn’t so bad. Let’s see what using the xPath plugin gives us: We get a much longer xpath expression which should provide us with direct access to the value. xp &lt;- &quot;/html/body/div[3]/div/div[2]/div[5]/div[1]/div[2]/div/div[1]/div[2]/div/div[1]/div[1]/div[1]/strong/span&quot; # Scrape the website for the movie rating rating &lt;- lego_movie %&gt;% html_nodes(xpath=xp) %&gt;% html_text() %&gt;% as.numeric() rating ## [1] 7.8 Let’s access the summary section of the link. We could use Selector Gadget or the xPath plugin. I’ll use the former. mov_summary &lt;- lego_movie %&gt;% html_nodes(&quot;.summary_text&quot;) %&gt;% html_text() mov_summary ## [1] &quot;\\n An ordinary LEGO construction worker, thought to be the prophesied as \\&quot;special\\&quot;, is recruited to join a quest to stop an evil tyrant from gluing the LEGO universe into eternal stasis.\\n &quot; 3.3 Faculty Salaries In this example we have to parse the main table associated with the results page. Salary Salary url &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey&quot; df &lt;- read_html(url) %&gt;% html_table() %&gt;% `[[`(1) intost &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;) salary &lt;- df %&gt;% separate(InstitutionCategoryState,into=intost,sep=&quot;\\n&quot;) salary ## Institution Category State ## 1 Auburn University Doctoral ALABAMA ## 2 Birmingham Southern College Baccalaureate ALABAMA ## 3 Huntingdon College Baccalaureate ALABAMA ## 4 Jacksonville St U Master’s ALABAMA ## 5 Samford University Master ALABAMA ## 6 Troy University Masters ALABAMA ## 7 The University of Alabama Doctoral ALABAMA ## 8 University of Alabama at Birmingham Doctoral ALABAMA ## 9 University of Alabama Huntsville Doctoral ALABAMA ## 10 University of Montevallo Master ALABAMA ## Avg. SalaryFull Professors Avg. ChangeContinuing Full Professors ## 1 $132,600 4.4% ## 2 $81,000 0.0% ## 3 $76,700 0.0% ## 4 $77,300 N/A ## 5 $105,200 3.2% ## 6 $84,500 1.6% ## 7 $151,600 2.0% ## 8 $139,100 2.8% ## 9 $126,400 2.0% ## 10 $80,700 2.1% ## CountFull Professors Avg. Total CompensationFull Professors ## 1 407 $170,300 ## 2 39 $100,000 ## 3 13 $89,500 ## 4 78 $104,100 ## 5 130 $133,300 ## 6 30 $88,500 ## 7 300 $205,100 ## 8 191 $164,800 ## 9 64 $169,200 ## 10 47 $106,100 ## Salary EquityFull Professors ## 1 90.1 ## 2 92.8 ## 3 109.6 ## 4 94.8 ## 5 86.8 ## 6 105.7 ## 7 85.6 ## 8 88.4 ## 9 95.6 ## 10 94.4 So we could process multiple pages # So now we could process multiple pages url &lt;- &#39;https://www.insidehighered.com/aaup-compensation-survey?institution-name=&amp;professor-category=1591&amp;page=1&#39; str1 &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey?&quot; str2 &lt;- &quot;institution-name=&amp;professor-category=1591&amp;page=&quot; intost &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;) salary &lt;- data.frame() for (ii in 1:2) { nurl &lt;- paste(str1,str2,ii,sep=&quot;&quot;) df &lt;- read_html(nurl) tmp &lt;- df %&gt;% html_table() %&gt;% `[[`(1) tmp &lt;- tmp %&gt;% separate(InstitutionCategoryState,into=intost,sep=&quot;\\n&quot;) salary &lt;- rbind(salary,tmp) } salary Look at the URLs at the bottom of the main page to find beginning and ending page numbers. Visually this is easy. Programmatically we could do something like the following: # https://www.insidehighered.com/aaup-compensation-survey?page=1 # https://www.insidehighered.com/aaup-compensation-survey?page=94 # What is the last page number ? We already know the answer - 94 lastnum &lt;- df %&gt;% html_nodes(xpath=&#39;//a&#39;) %&gt;% html_attr(&quot;href&quot;) %&gt;% &#39;[&#39;(103) %&gt;% strsplit(.,&quot;page=&quot;) %&gt;% &#39;[[&#39;(1) %&gt;% &#39;[&#39;(2) %&gt;% as.numeric(.) # So now we could get all pages of the survey str1 &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey?&quot; str2 &lt;- &quot;institution-name=&amp;professor-category=1591&amp;page=&quot; intost &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;) salary &lt;- data.frame() for (ii in 1:lastnum) { nurl &lt;- paste(str1,str2,ii,sep=&quot;&quot;) df &lt;- read_html(nurl) tmp &lt;- df %&gt;% html_table() %&gt;% `[[`(1) tmp &lt;- tmp %&gt;% separate(InstitutionCategoryState,into=intost,sep=&quot;\\n&quot;) salary &lt;- rbind(salary,tmp) Sys.sleep(1) } names(salary) &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;,&quot;AvgSalFP&quot;,&quot;AvgChgFP&quot;, &quot;CntFP&quot;,&quot;AvgTotCompFP&quot;,&quot;SalEquityFP&quot;) salary &lt;- salary %&gt;% mutate(AvgSalFP=as.numeric(gsub(&quot;\\\\$|,&quot;,&quot;&quot;,salary$AvgSalFP))) %&gt;% mutate(AvgTotCompFP=as.numeric(gsub(&quot;\\\\$|,&quot;,&quot;&quot;,salary$AvgTotCompFP))) salary %&gt;% group_by(State,Category) %&gt;% summarize(avg=mean(AvgSalFP)) %&gt;% arrange(desc(avg)) There are some problems: Data is large and scattered across multiple pages We could use above techniques to move from page to page There is a form we could use to narrow criteria But we have to programmatically submit the form rvest (and other packages) let you do this 3.4 Filling Out Forms From a Program Salary Let’s find salaries between $ 150,000 and the default max ($ 244,000) Find the element name associated with “Average Salary” Establish a connection with the form (usually the url of the page) Get a local copy of the form Fill in the value for the “Average Salary” Submit the lled in form Get the results and parse them like above ` So finding the correct element is more challenging. I use Chrome to do this. Just highlight the area over the form and right click to “Insepct” the element. This opens up the developer tools. You have to dig down to find the corrext form and the element name. Here is a screen shot of my activity: Salary url &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey&quot; # Establish a session mysess &lt;- html_session(url) # Get the form form_unfilled &lt;- mysess %&gt;% html_node(&quot;form&quot;) %&gt;% html_form() form_filled &lt;- form_unfilled %&gt;% set_values(&quot;range-from&quot;=150000) # Submit form results &lt;- submit_form(mysess,form_filled) first_page &lt;- results %&gt;% html_nodes(xpath=expr) %&gt;% html_table() first_page 3.5 PubMed Pubmed provides a rich source of information on published scientific literature. There are tutorials on how to leverage its capabilities but one thing to consider is that MESH terms are a good starting place since the search is index-based. MeSH (Medical Subject Headings) is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed. It’s faster and more accurate so you can first use the MESH browser to generate the appropriate search terms and add that into the Search interface. The MESH browser can be found at https://www.ncbi.nlm.nih.gov/mesh/ What we do here is get the links associated with each publication so we can then process each of those and get the abstract associated with each publication. # &quot;hemodialysis, home&quot; [MeSH Terms] url&lt;-&quot;https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D&quot; # # The results from the search will be of the form: # https://www.ncbi.nlm.nih.gov/pubmed/30380542 results &lt;- read_html(url) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% grep(&quot;/pubmed/[0-9]{1,6}&quot;,.,value=TRUE) %&gt;% unique(.) results ## [1] &quot;/pubmed/31566344&quot; &quot;/pubmed/31556520&quot; &quot;/pubmed/31448033&quot; ## [4] &quot;/pubmed/31320319&quot; &quot;/pubmed/31009189&quot; &quot;/pubmed/30964936&quot; ## [7] &quot;/pubmed/30799978&quot; &quot;/pubmed/30760251&quot; &quot;/pubmed/30663973&quot; ## [10] &quot;/pubmed/30545707&quot; &quot;/pubmed/30480534&quot; &quot;/pubmed/30473061&quot; ## [13] &quot;/pubmed/30463079&quot; &quot;/pubmed/30457224&quot; &quot;/pubmed/30392981&quot; ## [16] &quot;/pubmed/30380542&quot; &quot;/pubmed/30352485&quot; &quot;/pubmed/30314451&quot; ## [19] &quot;/pubmed/30281539&quot; &quot;/pubmed/30235331&quot; So now we could loop through these links and get the abstracts for these results. It looks that there are approximately 20 results per page. As before we would have to dive in to the underlying structure of the page to get the correct HTML pathnames or we could just look for Paragraph elements and pick out the links that way. text.vec &lt;- vector() for (ii in 1:length(results)) { string &lt;- paste0(&quot;https://www.ncbi.nlm.nih.gov&quot;,results[ii]) text.vec[ii] &lt;- read_html(string) %&gt;% html_nodes(&quot;p&quot;) %&gt;% `[[`(11) %&gt;% html_text() } # Eliminate lines with newlines characters final.vec &lt;- text.vec[grep(&quot;^\\n&quot;,text.vec,invert=TRUE)] final.vec ## [1] &quot;Globally, home dialysis prevalence has been declining relative to the increase in end stage renal disease and renal replacement therapy. The goal of this study was to identify international perceptions and practices. A web-based questionnaire was disseminated to nephrology nurses in 30 home dialysis-prevalent nations. Global telehealth use was low (23%), contrasting with 83% respondents agreeing telehealth would improve care. Only 31% of all programs enabled patient training outside of normal working hours (e.g., nights and weekends), and 31% of all program patients had some cost reimbursement, with a significant difference between U.S. and non-U.S. programs (U.S. 11%, non-U.S. 59%, 2 = 93.6, p &lt; 0.0001). Significant differences in the need for monthly clinic visits (U.S. 72%, non-US 44%, 2 = 83.7, p &lt; 0.0001) were also found. Telehealth provision and patient training flexibility is limited, and patient cost reimbursement is low. Increased telehealth, patient cost reimbursement, and flexible training models may promote home dialysis growth.&quot; ## [2] &quot;The authors report the first case of successful peritoneal dialysis (PD) in a developing country performed about a 13-year-old adolescent followed-up for stage V chronic kidney disease (CKD) with anuria. After 3 months of hemodialysis, the parents opted for continuous ambulatory peritoneal dialysis (CAPD) as they wished to return home located 121km from Dakar. After PD catheter insertion, the plan proposed to the patient consisted 3-4 hours stasis of isotonic dialysate during the day and a night stasis of 8 hours of icodextrin for an injection volume of 1L per session. The patient and his mother were trained and assessed on the PD technique. After dialysis adequacy was tested while hospitalised, they were able to return home and continued the sessions following the same plan prescribed and while keeping in touch, by telephone, with the medical team. The technique assessment at the day hospital every 2 weeks revealed dialysis adequacy and satisfactory tolerance of PD at home after 04 months of observation. It was the first case of successful CAPD in the pediatrics unit in this context. Scaling this technique is a challenge for the pediatric nephrologist in developing countries like Senegal.&quot; ## [3] &quot;dialysis; ethnicity; hemodialysis; hemodialysis, home; public policy&quot; ## [4] &quot;This article describes one woman&#39;s journey to home hemodialysis therapy. Her training and trials in adjusting to the therapy led to a passion for promoting improvements in dialysis for herself and others. Ms. Gedney has become a patient advocate who travels the country speaking for home dialyzers to politicians, physicians, and an alphabet soup of renal care committees. Her example should inspire others in the renal care community to speak out to make a difference for positive change.&quot; ## [5] &quot;Improvement in Home Dialysis (HoD) utilizations as a mean to improve the patient reported and health services outcomes, has been a long-held goal of the providers and healthcare system in United States. However, measures to improve HoD rates have yielded limited success so far. Lack of patient awareness of chronic kidney disease (CKD) and its management options, is one of the important barriers against patient adoption of HoD. Despite ample evidence that Comprehensive pre-ESERD Patient Education (CPE) improves patient awareness and informed HoD choice, use of CPE among US advanced CKD patients is low. Need for significant resources, lack of validated data showing unequivocal and reproducible benefits, and the lack of validated CPE protocols proven to have consistent efficacy in improving not only patient awareness but also HoD rates in US population, are major limitations deterring adoption of CPE in routine clinical practice. We recently demonstrated that if a structured, protocol based CPE is integrated within the routine nephrology care for patients with advanced CKD, it substantially improves informed HoD choice and utilizations. However, this requires establishing CPE resources within each nephrology practice. Efficacy of a stand-alone CPE model, independent of clinical care, has not been examined till date. In this report we report the efficacy of our structured CPE protocol, delivered outside the realm of routine nephrology care-as a stand-alone patient education program, in a geographically distant region, and show that: when provided opportunity for informed dialysis choice, a majority of advanced CKD patients in US would prefer HoD. We also show that initiating CPE leads to accelerated growth in HoD utilizations and reduces disparities in HoD utilizations, goals for system improvements. Finally, the reproducibility of our structured CPE protocol with consistent efficacy data suggest that initiating such programs at institutional levels has the potential to improve informed dialysis selection and HoD rates across any similar large healthcare institute within US.&quot; ## [6] &quot;A growing number of dialysis patients is treated with home haemodialysis. Our current pre-analytical protocols require patients to centrifuge the blood sample and transfer the plasma into a new tube at home. This procedure is prone to errors and precludes accurate bicarbonate measurement, required for determining dialysate bicarbonate concentration and maintaining acid-base status. We therefore evaluated whether cooled overnight storage of gel separated plasma is an acceptable alternative.&quot; ## [7] &quot;The survival rate for dialysis patients is poor. Previous studies have shown improved survival with home hemodialysis (HHD), but this could be due to patient selection, since HHD patients tend to be younger and healthier. The aim of the present study is to analyse the long-term effects of HHD on patient survival and on subsequent renal transplantation, compared with institutional hemodialysis (IHD) and peritoneal dialysis (PD), taking age and comorbidity into account.&quot; ## [8] &quot;Intensive hemodialysis is associated with more frequent arteriovenous (AV) access complications. We performed a retrospective cohort study to ascertain potential risk factors associated with access dysfunction in a cohort of nocturnal home hemodialysis (HHD) patients.&quot; ## [9] &quot;Home dialysis therapy, including home hemodialysis and peritoneal dialysis, is underused as a modality for the treatment of chronic kidney failure. The National Kidney Foundation-Kidney Disease Outcomes Quality Initiative sponsored a home dialysis conference in late 2017 that was designed to identify the barriers to starting and maintaining patients on home dialysis therapy. Clinical, operational, policy, and societal barriers were identified that need to be overcome to ensure that dialysis patients have the freedom to choose their treatment modality. Education of patients and patient partners, as well as health care providers, about home dialysis therapy, if offered at all, is often provided in a cursory manner. Lack of exposure to home dialysis therapies perpetuates a lack of familiarity and thus a hesitancy to refer patients to home dialysis therapies. Patient and care partner support, both psychosocial and financial, is also critical to minimize the risk for burnout leading to dropout from a home dialysis modality. Thus, the facilitation of home dialysis therapy will require a systematic change in chronic kidney disease education and the approach to dialysis therapy initiation, the creation of additional incentives for performing home dialysis, and breakthroughs to simplify the performance of home dialysis modalities. The home dialysis work group plans to develop strategies to overcome these barriers to home dialysis therapy, which will be presented at a follow-up home dialysis conference.&quot; ## [10] &quot;Telehealth encompasses a broad variety of technologies and tactics to deliver virtual medical, health, and education services. Telemedicine is the use of electronic communications for the exchange of medical information from one site to another to improve a patient&#39;s clinical health status.Several studies show that, by providing better patient oversight and communication, telehealth in PD enhances patient care, outcomes, quality of care, and satisfaction. Associated benefits include increased patient retention to home dialysis, reduced use of hospital services, and reduced costs of care.The sustainability of telehealth had been limited by reimbursement and regulatory restrictions. The Centers for Medicare and Medicaid Services (CMS) limited services related to end-stage renal disease by providing reimbursement for telehealth only in rural areas or counties outside of a metropolitan statistical area. Moreover, the dialysis facility and the patient&#39;s home were not approved as originating sites. However, effective January 1, 2019, those restrictions will be lifted. Telehealth will require that home dialysis patients be established with 3 initial face-to-face monthly clinical assessments without the use of telehealth; after those initial 3 months, a face-to-face visit at least once every 3 consecutive months will be required. Claims can be submitted using designated Current Procedural Terminology codes and modifiers.The actual extension of telehealth to home dialysis patients will depend on the details of forthcoming CMS regulations.&quot; ## [11] &quot;Since a couple of years, a new paradigm of care has taken place where the patient plays a major role in the choice of his treatment. Patient information in a context where the doctor-patient relationship has drastically changed and where knowledge is continuously growing, is a challenge for the nephrologist. For the patient who decides to undergo home dialysis, numerous questions arise during the decision-making process. How should treatment alternatives be discussed with the patient? What are the current data available on the different methods? As comfort, quality of life and level of autonomy often drive the choice of care management, home dialysis can be promoted more with appropriate assistance from both the nephrologist and the healthcare team. Cet article fait partie du numéro supplément Innovations en Néphrologie réalisé avec le soutien institutionnel de Vifor Fresenius Medical Care Renal Pharma.&quot; ## [12] &quot;Patients on peritoneal dialysis (PD) can be assisted by a nurse or a family member and treated either by automated PD (APD) or continuous ambulatory PD (CAPD). The aim of this study was to evaluate the effect of PD modality and type of assistance on the risk of transfer to haemodialysis (HD) and on the peritonitis risk in assisted PD patients.&quot; ## [13] &quot;Hemodialysis for chronic renal failure was introduced and developed in Seattle, WA, in the 1960s. Using Kiil dialyzers, weekly dialysis time and frequency were established to be about 30 hours on 3 time weekly dialysis. This dialysis time and frequency was associated with 10% yearly mortality in the United States in 1970s. Later in 1970s, newer and more efficient dialyzers were developed and it was felt that dialysis time could be shortened. An additional incentive to shorten dialysis was felt to be lower cost and higher convenience. Additional support for shortening dialysis time was provided by a randomized prospective trial performed by National Cooperative Dialysis Study (NCDS). This study committed a Type II statistical error rejecting the time of dialysis as an important factor in determining the quality of dialysis. This study also provided the basis for the establishment of the Kt/Vurea index as a measure of dialysis adequacy. This index having been established in a sacrosanct randomized controlled trial (RCT), was readily accepted by the HD community, and led to shorter dialysis, and higher mortality in the United States. Kt/Vurea is a poor measure of dialysis quality because it combines three unrelated variables into a single formula. These variables influence the clinical status of the patient independent of each other. It is impossible to compensate short dialysis duration (t) with the increased clearance of urea (K), because the tolerance of ultrafiltration depends on the plasma-refilling rate, which has nothing in common with urea clearance. Later, another RCT (the HEMO study) committed a Type III statistical error by asking the wrong research question, thus not yielding any valuable results. Fortunately, it did not lead to deterioration of dialysis outcomes in the United States. The third RCT in this field (\\&quot;in-center hemodialysis 6 times per week versus 3 times per week\\&quot;) did not bring forth any valuable results, but at least confirmed what was already known. The fourth such trial (\\&quot;The effects of frequent nocturnal home hemodialysis\\&quot;) too did not show any positive results primarily due to significant subject recruitment issues leading to inappropriate selection of patients. Comparison of the value of peritoneal dialysis and HD in RCTs could not be completed because of recruitment problems. Randomized controlled trials have therefore failed to yield any meaningful information in the area of dose and or frequency of hemodialysis.&quot; ## [14] &quot;Increasing uptake of home hemodialysis (HD) has led to interest in characteristics that predict discontinuation of home HD therapy for reasons other than death or transplantation. Recent reports of practice pattern variability led to the hypothesis that there are patient- and center-specific factors that influence these discontinuations.&quot; ## [15] &quot;Assisted PD (assPD) is an option of home dialysis treatment for dependent end-stage renal patients and worldwide applied in different countries since more than 40 years. China and Germany shares similar trends in demographic development with a growing proportion of elderly referred to dialysis treatment. So far number of patients treated by assPD is low in both countries. We analyze experiences in the implementation process, barriers, and benefits of ass PD in the aging population to provide a model for sustainable home dialysis treatment with PD in both countries. Differences and similarities of different factors (industrial, patient and facility based) which affect utilization of assPD are discussed. AssPD should be promoted in China and Germany to realize the benefits of home dialysis for the aging population by providing a structured model of implementation and quality assurance.&quot; ## [16] &quot;The prevalence of end-stage renal disease continues to increase in the United States with commensurate need for renal replacement therapies. Hemodialysis continues to be the predominant modality, though less than 2% of these patients will receive hemodialysis in their own home. While home modalities utilizing peritoneal dialysis have been growing, home hemodialysis (HHD) remains underutilized despite studies showing regression in left ventricular mass, improved quality of life, reduced depressive symptoms, and decreased postdialysis recovery time. To increase penetration of HHD will require a proactive approach from both physicians and dialysis networks to address barriers both in the system and on the level of the patients and families. We are reviewing these issues with a focus on the state of Mississippi.&quot; ## [17] &quot;Utilization of home hemodialysis (HHD) is low in Europe. The Knowledge to Improve Home Dialysis Network in Europe (KIHDNEy) is a multi-center study of HHD patients who have used a transportable hemodialysis machine that employs a low volume of lactate-buffered, ultrapure dialysate per session. In this retrospective cohort analysis, we describe patient factors, HHD prescription factors, and biochemistry and medication use during the first 6 months of HHD and rates of clinical outcomes thereafter.&quot; ## [18] &quot;There is increasing evidence that extended-hours regimens are associated with improved outcomes for patients on maintenance hemodialysis programs. Home hemodialysis programs are a well established way for patients to benefit from extended-hours dialysis overnight; however, there are significant barriers to home hemodialysis, which means that for many this is not an option. In center, nocturnal hemodialysis is an increasingly recognized way of offering extended-hours treatment to patients unable to undertake home-based programs and is an underutilized modality for such patients to gain from the physiological benefits of extended-hours dialysis regimens.&quot; ## [19] &quot;Health-related quality of life (HRQOL) is an important outcome measure in patients with end-stage renal disease. HRQOL is assumed to improve with kidney transplantation and also with nocturnal hemodialysis compared to conventional hemodialysis. However, there is no evidence regarding HRQOL to support the optimal treatment choice for patients on nocturnal hemodialysis who hesitate opting for transplantation. We therefore compared HRQOL between patients who were treated with kidney transplantation or nocturnal hemodialysis for one year.&quot; Well that was tedious. And we processed only the first page of results. How do we “progrmmatically” hit the “Next” Button at the bottom of the page ? This is complicated by the fact that there appears to be some Javascript at work that we would have to somehow interact with to get the URL for the next page. Unlike with the school salary example it isn’t obvious how to do this. If we hove over the “Next” button we don’t get an associated link. "],
["APIs.html", "Chapter 4 APIs 4.1 OMDB 4.2 The omdbapi package 4.3 RSelenium 4.4 EasyPubMed", " Chapter 4 APIs 4.1 OMDB Let’s look at the IMDB page whic catalogues lots of information about movies. Just got to the web site and search although here is an example link. https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2 In this case we would like to get the summary information for the movie. So we would use Selector Gadget or some other method to find the XPATH or CSS associated with this element. This pretty easy and doesn’t present much of a problem although for large scale mining of movie data we would run into trouble because IMDB doesn’t really like you to scrape their pages. They have an API that they would like for you to use. url &lt;- &#39;https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2&#39; summary &lt;- read_html(url) %&gt;% html_nodes(&quot;.summary_text&quot;) %&gt;% html_text() summary But here we go again. We have to parse the desired elements on this page and then what if we wanted to follow other links or set up a general function to search IMDB for other movies of various genres, titles, directors, etc. So as an example on how this works. Paste the URL into any web browser. You must supply your key for this to work. What you get back is a JSON formatted entry corresponding to ”The GodFather”movie. url &lt;- &quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=The+Godfather&quot; library(RJSONIO) url &lt;- &quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=The+Godfather&quot; # Fetch the URL via fromJSON movie &lt;- fromJSON(&quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=The+Godfather&quot;) # We get back a list which is much easier to process than raw JSON or XML str(movie) ## List of 25 ## $ Title : chr &quot;The Godfather&quot; ## $ Year : chr &quot;1972&quot; ## $ Rated : chr &quot;R&quot; ## $ Released : chr &quot;24 Mar 1972&quot; ## $ Runtime : chr &quot;175 min&quot; ## $ Genre : chr &quot;Crime, Drama&quot; ## $ Director : chr &quot;Francis Ford Coppola&quot; ## $ Writer : chr &quot;Mario Puzo (screenplay by), Francis Ford Coppola (screenplay by), Mario Puzo (based on the novel by)&quot; ## $ Actors : chr &quot;Marlon Brando, Al Pacino, James Caan, Richard S. Castellano&quot; ## $ Plot : chr &quot;The aging patriarch of an organized crime dynasty transfers control of his clandestine empire to his reluctant son.&quot; ## $ Language : chr &quot;English, Italian, Latin&quot; ## $ Country : chr &quot;USA&quot; ## $ Awards : chr &quot;Won 3 Oscars. Another 26 wins &amp; 30 nominations.&quot; ## $ Poster : chr &quot;https://m.media-amazon.com/images/M/MV5BM2MyNjYxNmUtYTAwNi00MTYxLWJmNWYtYzZlODY3ZTk3OTFlXkEyXkFqcGdeQXVyNzkwMjQ&quot;| __truncated__ ## $ Ratings :List of 3 ## ..$ : Named chr [1:2] &quot;Internet Movie Database&quot; &quot;9.2/10&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Source&quot; &quot;Value&quot; ## ..$ : Named chr [1:2] &quot;Rotten Tomatoes&quot; &quot;98%&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Source&quot; &quot;Value&quot; ## ..$ : Named chr [1:2] &quot;Metacritic&quot; &quot;100/100&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Source&quot; &quot;Value&quot; ## $ Metascore : chr &quot;100&quot; ## $ imdbRating: chr &quot;9.2&quot; ## $ imdbVotes : chr &quot;1,502,403&quot; ## $ imdbID : chr &quot;tt0068646&quot; ## $ Type : chr &quot;movie&quot; ## $ DVD : chr &quot;09 Oct 2001&quot; ## $ BoxOffice : chr &quot;N/A&quot; ## $ Production: chr &quot;Paramount Pictures&quot; ## $ Website : chr &quot;N/A&quot; ## $ Response : chr &quot;True&quot; movie$Plot ## [1] &quot;The aging patriarch of an organized crime dynasty transfers control of his clandestine empire to his reluctant son.&quot; sapply(movie$Ratings,unlist) ## [,1] [,2] [,3] ## Source &quot;Internet Movie Database&quot; &quot;Rotten Tomatoes&quot; &quot;Metacritic&quot; ## Value &quot;9.2/10&quot; &quot;98%&quot; &quot;100/100&quot; Let’s Get all the Episodes for Season 1 of Game of Thrones url &lt;- &quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=Game%20of%20Thrones&amp;Season=1&quot; movie &lt;- fromJSON(url) str(movie,1) ## List of 5 ## $ Title : chr &quot;Game of Thrones&quot; ## $ Season : chr &quot;1&quot; ## $ totalSeasons: chr &quot;8&quot; ## $ Episodes :List of 10 ## $ Response : chr &quot;True&quot; episodes &lt;- data.frame(do.call(rbind,movie$Episodes),stringsAsFactors = FALSE) episodes ## Title Released Episode imdbRating ## 1 Winter Is Coming 2011-04-17 1 9.0 ## 2 The Kingsroad 2011-04-24 2 8.8 ## 3 Lord Snow 2011-05-01 3 8.7 ## 4 Cripples, Bastards, and Broken Things 2011-05-08 4 8.8 ## 5 The Wolf and the Lion 2011-05-15 5 9.1 ## 6 A Golden Crown 2011-05-22 6 9.2 ## 7 You Win or You Die 2011-05-29 7 9.2 ## 8 The Pointy End 2011-06-05 8 9.0 ## 9 Baelor 2011-06-12 9 9.6 ## 10 Fire and Blood 2011-06-19 10 9.5 ## imdbID ## 1 tt1480055 ## 2 tt1668746 ## 3 tt1829962 ## 4 tt1829963 ## 5 tt1829964 ## 6 tt1837862 ## 7 tt1837863 ## 8 tt1837864 ## 9 tt1851398 ## 10 tt1851397 4.2 The omdbapi package Wait a minute. Looks like someone created an R package that wraps all this for us. It is called omdbapi # Use devtools to install devtools::install_github(&quot;hrbrmstr/omdbapi&quot;) library(omdbapi) # The first time you use this you will be prompted to enter your # API key movie_df &lt;- search_by_title(&quot;Star Wars&quot;, page = 2) (movie_df &lt;- movie_df[,-5]) ## Title Year imdbID ## 1 Star Wars: Episode IX - The Rise of Skywalker 2019 tt2527338 ## 2 Star Wars: The Clone Wars 2008 tt1185834 ## 3 Star Wars: The Clone Wars 2008–2020 tt0458290 ## 4 Star Wars Rebels 2014–2018 tt2930604 ## 5 Star Wars: Clone Wars 2003–2005 tt0361243 ## 6 The Star Wars Holiday Special 1978 tt0193524 ## 7 Robot Chicken: Star Wars 2007 tt1020990 ## 8 Star Wars: Knights of the Old Republic 2003 tt0356070 ## 9 Robot Chicken: Star Wars Episode II 2008 tt1334272 ## 10 Star Wars: The Force Unleashed 2008 tt1024923 ## Type ## 1 movie ## 2 movie ## 3 series ## 4 series ## 5 series ## 6 movie ## 7 movie ## 8 game ## 9 movie ## 10 game # Get lots of info on The GodFather (gf &lt;- find_by_title(&quot;The GodFather&quot;)) ## # A tibble: 3 x 25 ## Title Year Rated Released Runtime Genre Director Writer Actors Plot ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The … 1972 R 1972-03-24 175 min Crim… Francis… Mario… Marlo… The … ## 2 The … 1972 R 1972-03-24 175 min Crim… Francis… Mario… Marlo… The … ## 3 The … 1972 R 1972-03-24 175 min Crim… Francis… Mario… Marlo… The … ## # … with 15 more variables: Language &lt;chr&gt;, Country &lt;chr&gt;, Awards &lt;chr&gt;, ## # Poster &lt;chr&gt;, Ratings &lt;list&gt;, Metascore &lt;chr&gt;, imdbRating &lt;dbl&gt;, ## # imdbVotes &lt;dbl&gt;, imdbID &lt;chr&gt;, Type &lt;chr&gt;, DVD &lt;date&gt;, ## # BoxOffice &lt;chr&gt;, Production &lt;chr&gt;, Website &lt;chr&gt;, Response &lt;chr&gt; # Get the actors from the GodFather get_actors((gf)) ## [1] &quot;Marlon Brando&quot; &quot;Al Pacino&quot; &quot;James Caan&quot; ## [4] &quot;Richard S. Castellano&quot; 4.3 RSelenium Sometimes we interact with websites that use Javascript to load more text or comments in a user forum. Here is an example of that. Look at https://www.dailystrength.org/group/dialysis which is a website associated with people wanting to share information about dialysis. If you check the bottom of the pag you will see a button. # https://www.dailystrength.org/group/dialysis library(RSelenium) library(rvest) library(tm) library(SentimentAnalysis) library(wordcloud) url &lt;- &quot;https://www.dailystrength.org/group/dialysis&quot; # The website has a &quot;show more&quot; button that hides most of the patient posts # If we don&#39;t find a way to programmatically &quot;click&quot; this button then we can # only get a few of the posts and their responses. To do this we need to # use the RSelenium package which does a lot of behind the scenes work # See https://cran.r-project.org/web/packages/RSelenium/RSelenium.pdf # http://brazenly.blogspot.com/2016/05/r-advanced-web-scraping-dynamic.html # Open up a connection # rD &lt;- rsDriver() # So, you might have to specify the version of chrome you are using # For someone reason this seems now to be necessary (11/4/19) rD &lt;- rsDriver(browser=c(&quot;chrome&quot;),chromever=&quot;78.0.3904.70&quot;) remDr &lt;- rD[[&quot;client&quot;]] remDr$navigate(url) loadmorebutton &lt;- remDr$findElement(using = &#39;css selector&#39;, &quot;#load-more-discussions&quot;) # Do this a number of times to get more links loadmorebutton$clickElement() # Now get the page with more comments and questions page_source &lt;- remDr$getPageSource() # So let&#39;s parse the contents comments &lt;- read_html(page_source[[1]]) cumulative_comments &lt;- vector() links &lt;- comments %&gt;% html_nodes(css=&quot;.newsfeed__description&quot;) %&gt;% html_node(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) full_links &lt;- paste0(&quot;https://www.dailystrength.org&quot;,links) if (length(grep(&quot;NA&quot;,full_links)) &gt; 0) { full_links &lt;- full_links[-grep(&quot;NA&quot;,full_links)] } ugly_xpath &lt;- &#39;//*[contains(concat( &quot; &quot;, @class, &quot; &quot; ), concat( &quot; &quot;, &quot;comments__comment-text&quot;, &quot; &quot; ))] | //p&#39; for (ii in 1:length(full_links)) { text &lt;- read_html(full_links[ii]) %&gt;% html_nodes(xpath=ugly_xpath) %&gt;% html_text() length(text) &lt;- length(text) - 1 text &lt;- text[-1] text cumulative_comments &lt;- c(cumulative_comments,text) } remDr$close() # stop the selenium server rD[[&quot;server&quot;]]$stop() 4.4 EasyPubMed So there is an R package called EasyPubMed that helps ease the access of data on the Internet. The idea behind this package is to be able to query NCBI Entrez and retrieve PubMed records in XML or TXT format. The PubMed records can be downloaded and saved as XML or text files if desired. According to the package authours, “Data integrity is enforced during data download, allowing to retrieve and save very large number of records effortlessly”. The bottom line is that you can do what you want after that. Let’s look at an example involving home hemodialysis library(easyPubMed) Let’s do some searching my_query &lt;- &#39;hemodialysis, home&quot; [MeSH Terms]&#39; my_entrez_id &lt;- get_pubmed_ids(my_query) my_abstracts &lt;- fetch_pubmed_data(my_entrez_id) my_abstracts &lt;- custom_grep(my_abstracts,&quot;AbstractText&quot;,&quot;char&quot;) my_abstracts[1:3] [1] &quot;Assisted PD (assPD) is an option of home dialysis treatment for dependent end-stage renal patients and worldwide applied in different countries since more than 40 years. China and Germany shares similar trends in demographic development with a growing proportion of elderly referred to dialysis treatment. So far number of patients treated by assPD is low in both countries. We analyze experiences in the implementation process, barriers, and benefits of ass PD in the aging population to provide a model for sustainable home dialysis treatment with PD in both countries. Differences and similarities of different factors (industrial, patient and facility based) which affect utilization of assPD are discussed. AssPD should be promoted in China and Germany to realize the benefits of home dialysis for the aging population by providing a structured model of implementation and quality assurance.&quot; [2] &quot;End-stage renal disease (ESRD) is the final stage of chronic kidney disease in which the kidney is not sufficient to meet the needs of daily life. It is necessary to understand the role of genes expression involved in ESRD patient responses to nocturnal hemodialysis (NHD) and to improve the immunity responsiveness. The aim of this study was to investigate novel immune-associated genes that may play important roles in patients with ESRD.The microarray expression profiles of peripheral blood in patients with ESRD before and after NHD were analyzed by network-based approaches, and then using Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes pathway analysis to explore the biological process and molecular functions of differentially expressed genes. Subsequently, a transcriptional regulatory network of the core genes and the connected transcriptional regulators was constructed. We found that NHD had a significant effect on neutrophil activation and immune response in patients with ESRD.In addition, Our findings suggest that MAPKAPK3, RHOA, ARRB2, FLOT1, MYH9, PRKCD, RHOG, PTPN6, MAPK3, CNPY3, PI3KCG, and PYGL genes maybe potential targets regulated by core transcriptional factors, including ARNT, C/EBPalpha, CEBPA, CREB1, PSG1, DAND5, SP1, GATA1, MYC, EGR2, and EGR3.&quot; [3] &quot;Only a minority of patients with chronic kidney disease treated by hemodialysis are currently treated at home. Until relatively recently, the only type of hemodialysis machine available for these patients was a slightly smaller version of the standard machines used for in-center dialysis treatments. Areas covered: There are now an alternative generation of dialysis machines specifically designed for home hemodialysis. The home dialysis patient wants a smaller machine, which is intuitive to use, easy to trouble shoot, robust and reliable, quick to setup and put away, requiring minimal waste disposal. The machines designed for home dialysis have some similarities in terms of touch-screen patient interfaces, and using pre-prepared cartridges to speed up setting up the machine. On the other hand, they differ in terms of whether they use slower or standard dialysate flows, prepare batches of dialysis fluid, require separate water purification equipment, or whether this is integrated, or use pre-prepared sterile bags of dialysis fluid. Expert commentary: Dialysis machine complexity is one of the hurdles reducing the number of patients opting for home hemodialysis and the introduction of the newer generation of dialysis machines designed for ease of use will hopefully increase the number of patients opting for home hemodialysis.&quot; "],
["bagofwords.html", "Chapter 5 Bag of Words Sentiment Analysis 5.1 Workflow 5.2 Simple Example 5.3 tidytext 5.4 Back To The PubMed Example 5.5 BiGrams", " Chapter 5 Bag of Words Sentiment Analysis One we have a collection of text it’s interesting to figure out what it might mean or infer - if anything at all. In text analysis and NLP (Natural Language Processing) we talk about “Bag of Words” to describe a collection or “corpus” of unstructured text. What do we do with a “bag of words” ? Extract meaning from collections of text (without reading !) Detect and analyze patterns in unstructured textual collections Use Natural Language Processing techniques to reach conclusions Discover what ideas occur in text and how they might be linked Determine if the discovered patterns be used to predict behavior ? Identify interesting ideas that might otherwise be ignored 5.1 Workflow Identify and Obtain text (e.g. websites, Twitter, Databases, PDFs, surveys) Create a text ”Corpus”- a structure that contains the raw text Apply transformations: Normalize case (convert to lower case) Remove puncutation and stopwords Remove domain specific stopwords Perform Analysis and Visualizations (word frequency, tagging, wordclouds) Do Sentiment Analysis R has Packages to Help. These are just some of them: QDAP - Quantitative Discourse Package tm - text mining applications within R tidytext - Text Mining using ddplyr and ggplot and tidyverse tools SentimentAnalysis - For Sentiment Analysis However, consider that: Some of these are easier to use than others Some can be kind of a problem to install (e.g. qdap) They all offer similar capabilities We’ll look at tidytext 5.2 Simple Example Find the URL for Lincoln’s March 4, 1865 Speech: url &lt;- &quot;https://millercenter.org/the-presidency/presidential-speeches/march-4-1865-second-inaugural-address&quot; library(rvest) lincoln_doc &lt;- read_html(url) %&gt;% html_nodes(&quot;.view-transcript&quot;) %&gt;% html_text() lincoln_doc ## [1] &quot;TranscriptFellow-Countrymen: At this second appearing to take the oath of the Presidential office there is less occasion for an extended address than there was at the first. Then a statement somewhat in detail of a course to be pursued seemed fitting and proper. Now, at the expiration of four years, during which public declarations have been constantly called forth on every point and phase of the great contest which still absorbs the attention and engrosses the energies of the nation, little that is new could be presented. The progress of our arms, upon which all else chiefly depends, is as well known to the public as to myself, and it is, I trust, reasonably satisfactory and encouraging to all. With high hope for the future, no prediction in regard to it is ventured.On the occasion corresponding to this four years ago all thoughts were anxiously directed to an impending civil war. All dreaded it, all sought to avert it. While the inaugural address was being delivered from this place, devoted altogether to saving the Union without war, insurgent agents were in the city seeking to destroy it without war-seeking to dissolve the Union and divide effects by negotiation. Both parties deprecated war, but one of them would make war rather than let the nation survive, and the other would accept war rather than let it perish, and the war came.One-eighth of the whole population were colored slaves, not distributed generally over the Union. but localized in the southern part of it. These slaves constituted a peculiar and powerful interest. All knew that this interest was somehow the cause of the war. To strengthen, perpetuate, and extend this interest was the object for which the insurgents would rend the Union even by war, while the Government claimed no right to do more than to restrict the territorial enlargement of it. Neither party expected for the war the magnitude or the duration which it has already attained. Neither anticipated that the cause of the conflict might cease with or even before the conflict itself should cease. Each looked for an easier triumph, and a result less fundamental and astounding. Both read the same Bible and pray to the same God, and each invokes His aid against the other. It may seem strange that any men should dare to ask a just God&#39;s assistance in wringing their bread from the sweat of other men&#39;s faces, but let us judge not, that we be not judged. The prayers of both could not be answered. That of neither has been answered fully. The Almighty has His own purposes. \\&quot;Woe unto the world because of offenses; for it must needs be that offenses come, but woe to that man by whom the offense cometh.\\&quot; If we shall suppose that American slavery is one of those offenses which, in the providence of God, must needs come, but which, having continued through His appointed time, He now wills to remove, and that He gives to both North and South this terrible war as the woe due to those by whom the offense came, shall we discern therein any departure from those divine attributes which the believers in a living God always ascribe to Him? Fondly do we hope, fervently do we pray, that this mighty scourge of war may speedily pass away. Yet, if God wills that it continue until all the wealth piled by the bondsman&#39;s two hundred and fifty years of unrequited toil shall be sunk, and until every drop of blood drawn with the lash shall be paid by another drawn with the sword, as was said three thousand years ago, so still it must be said \\&quot;the judgments of the Lord are true and righteous altogether.\\&quot;With malice toward none, with charity for all, with firmness in the fight as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nation&#39;s wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.&quot; There are probably lots of words that don’t really “matter” or contribute to the “real” meaning of the speech. word_vec &lt;- unlist(strsplit(lincoln_doc,&quot; &quot;)) word_vec[1:20] ## [1] &quot;TranscriptFellow-Countrymen:&quot; &quot;&quot; ## [3] &quot;At&quot; &quot;this&quot; ## [5] &quot;second&quot; &quot;appearing&quot; ## [7] &quot;to&quot; &quot;take&quot; ## [9] &quot;the&quot; &quot;oath&quot; ## [11] &quot;of&quot; &quot;the&quot; ## [13] &quot;Presidential&quot; &quot;office&quot; ## [15] &quot;there&quot; &quot;is&quot; ## [17] &quot;less&quot; &quot;occasion&quot; ## [19] &quot;for&quot; &quot;an&quot; sort(table(word_vec),decreasing = TRUE)[1:10] ## word_vec ## the to and of that for be in it a ## 54 26 24 22 11 9 8 8 8 7 How do we remove all the uninteresting words ? We could do it manaully # Remove all punctuation marks word_vec &lt;- gsub(&quot;[[:punct:]]&quot;,&quot;&quot;,word_vec) stop_words &lt;- c(&quot;the&quot;,&quot;to&quot;,&quot;and&quot;,&quot;of&quot;,&quot;the&quot;,&quot;for&quot;,&quot;in&quot;,&quot;it&quot;, &quot;a&quot;,&quot;this&quot;,&quot;which&quot;,&quot;by&quot;,&quot;is&quot;,&quot;an&quot;,&quot;hqs&quot;,&quot;from&quot;, &quot;that&quot;,&quot;with&quot;,&quot;as&quot;) for (ii in 1:length(stop_words)) { for (jj in 1:length(word_vec)) { if (stop_words[ii] == word_vec[jj]) { word_vec[jj] &lt;- &quot;&quot; } } } word_vec &lt;- word_vec[word_vec != &quot;&quot;] sort(table(word_vec),decreasing = TRUE)[1:10] ## word_vec ## war all be we but God shall was do let ## 11 8 8 6 5 5 5 5 4 4 word_vec[1:30] ## [1] &quot;TranscriptFellowCountrymen&quot; &quot;At&quot; ## [3] &quot;second&quot; &quot;appearing&quot; ## [5] &quot;take&quot; &quot;oath&quot; ## [7] &quot;Presidential&quot; &quot;office&quot; ## [9] &quot;there&quot; &quot;less&quot; ## [11] &quot;occasion&quot; &quot;extended&quot; ## [13] &quot;address&quot; &quot;than&quot; ## [15] &quot;there&quot; &quot;was&quot; ## [17] &quot;at&quot; &quot;first&quot; ## [19] &quot;Then&quot; &quot;statement&quot; ## [21] &quot;somewhat&quot; &quot;detail&quot; ## [23] &quot;course&quot; &quot;be&quot; ## [25] &quot;pursued&quot; &quot;seemed&quot; ## [27] &quot;fitting&quot; &quot;proper&quot; ## [29] &quot;Now&quot; &quot;at&quot; 5.3 tidytext So the tidytext package provides some accomodations to convert your body of text into individual tokens which then simplfies the removal of less meaningful words and the creation of word frequency counts. The first thing you do is to create a data frame where the there is one line for each body of text. In this case we have only one long string of text this will be a one line data frame. library(tidytext) library(tidyr) text_df &lt;- data_frame(line = 1:length(lincoln_doc), text = lincoln_doc) text_df ## # A tibble: 1 x 2 ## line text ## &lt;int&gt; &lt;chr&gt; ## 1 1 &quot;TranscriptFellow-Countrymen: At this second appearing to take th… The next step is to breakup each of text lines (we have only 1) into invdividual rows, each with it’s own line. We also want to count the number of times that each word appears. This is known as tokenizing the data frame. token_text &lt;- text_df %&gt;% unnest_tokens(word, text) # Let&#39;s now count them token_text %&gt;% count(word,sort=TRUE) ## # A tibble: 339 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 58 ## 2 to 27 ## 3 and 24 ## 4 of 22 ## 5 it 13 ## 6 that 12 ## 7 war 12 ## 8 all 10 ## 9 for 9 ## 10 in 9 ## # … with 329 more rows But we need to get rid of the “stop words”. It’s a good thing that the tidytext package has a way to filter out the common words that do not significantly contribute to the meaning of the overall text. The stop_words data frame is built into tidytext. Take a look to see some of the words contained therein: data(stop_words) # Sample 40 random stop words stop_words %&gt;% sample_n(40) ## # A tibble: 40 x 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 may SMART ## 2 aren&#39;t snowball ## 3 hardly SMART ## 4 began onix ## 5 unlikely SMART ## 6 for SMART ## 7 any onix ## 8 youngest onix ## 9 upon SMART ## 10 clear onix ## # … with 30 more rows # Now remove stop words from the document tidy_text &lt;- token_text %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; # This could also be done by the following. I point this out only because some people react # negatively to &quot;joins&quot; although fully understanding what joins are can only help you since # much of what the dplyr package does is based on SQL type joins. tidy_text &lt;- token_text %&gt;% filter(!word %in% stop_words$word) tidy_text %&gt;% count(word,sort=TRUE) ## # A tibble: 193 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 war 12 ## 2 god 5 ## 3 union 4 ## 4 offenses 3 ## 5 woe 3 ## 6 address 2 ## 7 ago 2 ## 8 altogether 2 ## 9 answered 2 ## 10 cease 2 ## # … with 183 more rows tidy_text %&gt;% count(word,sort=TRUE) ## # A tibble: 193 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 war 12 ## 2 god 5 ## 3 union 4 ## 4 offenses 3 ## 5 woe 3 ## 6 address 2 ## 7 ago 2 ## 8 altogether 2 ## 9 answered 2 ## 10 cease 2 ## # … with 183 more rows tidy_text %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 2) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() 5.4 Back To The PubMed Example We have around 935 abstracts that we mess with based on our work using the easyPubMed package # Create a data frame out of the cleaned up abstracts library(tidytext) library(dplyr) text_df &lt;- data_frame(line = 1:length(my_abstracts), text = my_abstracts) token_text &lt;- text_df %&gt;% unnest_tokens(word, text) # Many of these words aren&#39;t helpful token_text %&gt;% count(total=word,sort=TRUE) ## # A tibble: 6,936 x 2 ## total n ## &lt;chr&gt; &lt;int&gt; ## 1 the 3062 ## 2 of 2896 ## 3 and 2871 ## 4 in 1915 ## 5 to 1884 ## 6 a 1373 ## 7 dialysis 1365 ## 8 patients 1335 ## 9 home 1281 ## 10 with 1035 ## # … with 6,926 more rows # Now remove stop words data(stop_words) tidy_text &lt;- token_text %&gt;% anti_join(stop_words) # This could also be done by the following. I point this out only because some people react # negatively to &quot;joins&quot; although fully understanding what joins are can only help you since # much of what the dplyr package does is based on SQL type joins. tidy_text &lt;- token_text %&gt;% filter(!word %in% stop_words$word) # Arrange the text by descending word frequency tidy_text %&gt;% count(word, sort = TRUE) ## # A tibble: 6,460 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 dialysis 1365 ## 2 patients 1335 ## 3 home 1281 ## 4 hemodialysis 674 ## 5 hd 463 ## 6 hhd 440 ## 7 patient 395 ## 8 pd 303 ## 9 renal 279 ## 10 study 268 ## # … with 6,450 more rows Some of the most frequently occurring words are in fact “dialysis”, “patients” so maybe we should consider them to be stop words also since we already know quite well that the overall theme is, well, dialysis and kidneys. There are also synonymns and abbreviations that are somewhat redundant such as “pdd”,“pd”,“hhd” so let’s eliminate them also. tidy_text &lt;- token_text %&gt;% filter(!word %in% c(stop_words$word,&quot;dialysis&quot;,&quot;patients&quot;,&quot;home&quot;,&quot;kidney&quot;, &quot;hemodialysis&quot;,&quot;haemodialysis&quot;,&quot;patient&quot;,&quot;hhd&quot;, &quot;pd&quot;,&quot;peritoneal&quot;,&quot;hd&quot;,&quot;renal&quot;,&quot;study&quot;,&quot;care&quot;, &quot;ci&quot;,&quot;chd&quot;,&quot;nhd&quot;,&quot;disease&quot;,&quot;treatment&quot;)) tidy_text %&gt;% count(word, sort = TRUE) ## # A tibble: 6,441 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 therapy 193 ## 2 conventional 191 ## 3 survival 191 ## 4 center 186 ## 5 compared 180 ## 6 clinical 175 ## 7 nocturnal 171 ## 8 outcomes 171 ## 9 quality 171 ## 10 data 161 ## # … with 6,431 more rows Let’s do some plotting of these words library(ggplot2) tidy_text %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 120) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() Okay, it looks like there are numbers in there which might be useful. I suspect that the “95” is probably associated with the idea of a confidence interval. But there are other references to numbers. grep(&quot;^[0-9]{1,3}$&quot;,tidy_text$word)[1:20] ## [1] 9 273 275 284 288 293 296 305 308 387 388 554 614 671 679 680 682 ## [18] 744 758 762 tidy_text_nonum &lt;- tidy_text[grep(&quot;^[0-9]{1,3}$&quot;,tidy_text$word,invert=TRUE),] Okay well I think maybe we have some reasonable data to examine. As you might have realized by now, manipulating data to get it “clean” can be tedious and frustrating though it is an inevitable part of the process. tidy_text_nonum %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 120) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() 5.4.1 How Do You Feel ? The next step is to explore what some of these words might mean. The tidytext package has four dictionaries that help you figure out what sentiment is being expressed by your data frame. # NRC Emotion Lexicon from Saif Mohammad and Peter Turney get_sentiments(&quot;nrc&quot;) %&gt;% sample_n(20) ## # A tibble: 20 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 pandemic sadness ## 2 melancholic negative ## 3 pointedly positive ## 4 amusing positive ## 5 embarrassment surprise ## 6 protector trust ## 7 raging anger ## 8 angelic joy ## 9 ornate positive ## 10 kindred anticipation ## 11 abacus trust ## 12 delegate positive ## 13 comfort anticipation ## 14 aspiring trust ## 15 auditor trust ## 16 custody trust ## 17 mad anger ## 18 pornographic negative ## 19 mucous disgust ## 20 healthful positive # the sentiment lexicon from Bing Liu and collaborators get_sentiments(&quot;bing&quot;) %&gt;% sample_n(20) ## # A tibble: 20 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 amply positive ## 2 illuminating positive ## 3 foreboding negative ## 4 best-performing positive ## 5 treacherously negative ## 6 dirt-cheap positive ## 7 downside negative ## 8 providence positive ## 9 elimination negative ## 10 leaky negative ## 11 eyecatch positive ## 12 bewildering negative ## 13 forgetfulness negative ## 14 savages negative ## 15 stumbled negative ## 16 loopholes negative ## 17 shrill negative ## 18 hindrance negative ## 19 deadbeat negative ## 20 gnawing negative # Tim Loughran and Bill McDonald get_sentiments(&quot;loughran&quot;) %&gt;% sample_n(20) ## # A tibble: 20 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 benefited positive ## 2 confession negative ## 3 alleging negative ## 4 escalate negative ## 5 invalidate negative ## 6 stricter constraining ## 7 understates negative ## 8 dissenters negative ## 9 weakest negative ## 10 negative negative ## 11 contending negative ## 12 droughts negative ## 13 forthwith litigious ## 14 hostile negative ## 15 delights positive ## 16 testimony litigious ## 17 frivolous negative ## 18 usurious negative ## 19 curtailments negative ## 20 scandalous negative # Pull out words that correspond to joy nrc_joy &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;joy&quot;) nrc_joy ## # A tibble: 689 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 absolution joy ## 2 abundance joy ## 3 abundant joy ## 4 accolade joy ## 5 accompaniment joy ## 6 accomplish joy ## 7 accomplished joy ## 8 achieve joy ## 9 achievement joy ## 10 acrobat joy ## # … with 679 more rows So we will use the nrc sentiment dictionary to see the “sentiment” expressed in our abstracts. bing_word_counts &lt;- tidy_text_nonum %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) %&gt;% count(word,sentiment,sort=TRUE) ## Joining, by = &quot;word&quot; t the positive vs negative words bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() ## Selecting by n Let’s create a word cloud library(wordcloud) # tidy_text_nonum %&gt;% count(word) %&gt;% with(wordcloud(word,n,max.words=90,scale=c(4,.5),colors=brewer.pal(8,&quot;Dark2&quot;))) 5.5 BiGrams Let’s look at bigrams. We need to go back to the cleaned abstracts and pair words to get phrase that might be suggestive of some sentiment text_df &lt;- data_frame(line = 1:length(my_abstracts), text = my_abstracts) dialysis_bigrams &lt;- text_df %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) dialysis_bigrams %&gt;% count(bigram, sort = TRUE) ## # A tibble: 41,738 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 in the 382 ## 2 of the 310 ## 3 home dialysis 300 ## 4 home hemodialysis 279 ## 5 of home 195 ## 6 peritoneal dialysis 193 ## 7 associated with 174 ## 8 home hd 153 ## 9 home haemodialysis 144 ## 10 in center 144 ## # … with 41,728 more rows But we have to filter out stop words library(tidyr) bigrams_sep &lt;- dialysis_bigrams %&gt;% separate(bigram,c(&quot;word1&quot;,&quot;word2&quot;),sep=&quot; &quot;) stop_list &lt;- c(stop_words$word,&quot;dialysis&quot;,&quot;patients&quot;,&quot;home&quot;,&quot;kidney&quot;, &quot;hemodialysis&quot;,&quot;haemodialysis&quot;,&quot;treatment&quot;,&quot;patient&quot;,&quot;hhd&quot;, &quot;pd&quot;,&quot;peritoneal&quot;,&quot;hd&quot;,&quot;renal&quot;,&quot;study&quot;,&quot;care&quot;, &quot;ci&quot;,&quot;chd&quot;,&quot;nhd&quot;,&quot;esrd&quot;,&quot;lt&quot;,&quot;95&quot;,&quot;0.001&quot;) bigrams_filtered &lt;- bigrams_sep %&gt;% filter(!word1 %in% stop_list) %&gt;% filter(!word2 %in% stop_list) bigram_counts &lt;- bigrams_filtered %&gt;% count(word1, word2, sort = TRUE) bigrams_united &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) bigrams_united %&gt;% count(bigram, sort = TRUE) %&gt;% print(n=25) ## # A tibble: 11,842 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 replacement therapy 71 ## 2 vascular access 65 ## 3 technique failure 54 ## 4 confidence interval 41 ## 5 left ventricular 39 ## 6 blood pressure 36 ## 7 short daily 35 ## 8 clinical outcomes 33 ## 9 thrice weekly 30 ## 10 technique survival 29 ## 11 hazard ratio 26 ## 12 quality improvement 26 ## 13 adverse events 22 ## 14 6 months 21 ## 15 access related 21 ## 16 arteriovenous fistula 21 ## 17 12 months 19 ## 18 ventricular mass 18 ## 19 3 times 15 ## 20 buttonhole cannulation 15 ## 21 cost effective 15 ## 22 observational studies 15 ## 23 retrospective cohort 15 ## 24 cost effectiveness 14 ## 25 daily life 14 ## # … with 1.182e+04 more rows library(tidyquant) bigram_counts %&gt;% filter(n &gt; 30) %&gt;% ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) + geom_tile(alpha = 0.8, color = &quot;white&quot;) + scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) + coord_flip() + theme_tq() + theme(legend.position = &quot;right&quot;) + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + labs(x = &quot;first word in pair&quot;, y = &quot;second word in pair&quot;) "],
["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
