[
["index.html", "Web Scraping with R Chapter 1 Motivations 1.1 Lots of Data For The Taking ? 1.2 Web Scraping Can Be Ugly 1.3 Understanding the language of the Web 1.4 Useful Packages 1.5 Quick rvest tutorial 1.6 Example: Parsing A Table From Wikipedia 1.7 Summary", " Web Scraping with R Steve Pittard 2018-11-11 Chapter 1 Motivations 1.1 Lots of Data For The Taking ? The web hosts lots of interesting data that you can ”scrape”. Some of it is stashed in data bases, behind APIs, or in free form text. Lots of people want to grab information of of Twitter or from user forums to see what people are thinking. There is a lot of valuable information out there for the taking although some web sites have “caught on” and either block programmatic access or they setup “pay walls” that require you to subscribe to an API for access. The New York Times does this. But there are lots of opportunities to get data. tables Fetch tables like from Wikipedia forms You can submit forms and fetch the results css You can access parts of a web site using style or css selectors Tweets Process tweets including emojis Web Sites User forums have lots of content Instagram Yes you can “scrape” photos also 1.2 Web Scraping Can Be Ugly Depending on what web sites you want to scrape the process can be involved and quite tedious. Many websites are very much aware that people are scraping so they offer Application Programming Interfaces (APIs) to make requests for information easier for the user and easier for the server administrators to control access. Most times the user must apply for a “key” to gain access. For premium sites, the key costs money. Some sites like Google and Wunderground (a popular weather site) allow some number of free accesses before they start charging you. Even so the results are typically returned in XML or JSON which then requires you to parse the result to get the information you want. In the best situation there is an R package that will wrap in the parsing and will return lists or data frames. Here is a summary: First. Always try to find an R package that will access a site (e.g. New York Times, Wunderground, PubMed). These packages (e.g. omdbapi, easyPubMed, RBitCoin, rtimes) provide a programmatic search interface and return data frames with little to no effort on your part. If no package exists then hopefully there is an API that allows you to query the website and get results back in JSON or XML. I prefer JSON because it’s “easier” and the packages for parsing JSON return lists which are native data structures to R. So you can easily turn results into data frames. You will ususally use the rvest package in conjunction with XML, and the RSJONIO packages. If the Web site doesn’t have an API then you will need to scrape text. This isn’t hard but it is tedious. You will need to use rvest to parse HMTL elements. If you want to parse mutliple pages then you will need to use rvest to move to the other pages and possibly fill out forms. If there is a lot of Javascript then you might need to use RSelenium to programmatically manage the web page. 1.3 Understanding the language of the Web The Web has its own languages: HTML, CSS, Javascript &lt;h1&gt;, &lt;h2&gt;, ..., &lt;h6&gt; Heading 1 and so on &lt;p&gt; Paragraph elements &lt;ul&gt; Unordered List &lt;ol&gt; Ordered List &lt;li&gt; List Element &lt;div&gt; Division / Section &lt;table&gt; Tables &lt;form&gt; Web forms So to be productive at scraping requires you to have some familiairty with HMTL XML, and CSS. Here we look at a very basic HTML file &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;h1&gt;My First Heading&lt;/h1&gt; &lt;p&gt;My first paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; And you could apply some styling to this courtest of the CSS language which allows you to inject styles into plain HTML: There are a number of tools that allow us to inspect web pages and see “what is under the hood”. Warning - I just discovered that one of my favorite browser plugins to find the xpaths and/or css paths of page elements is no longer supported under Firefox or Chrome. I’ve found a couple of replacements but they don’t work as well. I’ll research it more. Selector Gadget http://selectorgadget.com/ Firebug https://getfirebug.com/ (now integrated into a version of Firefox) Google Chrome Right click to inspect a page element Google Chrome View Developer - Developer Tools Oxygen Editor Can obtain via the Emory Software Express Site 1.4 Useful Packages You will use the following three primary packages to help you get data from various web pages: rvest, XML, and RJSONIO. Note that you won’t always use them simultaneously but you might use them in pairs or individually depending on the task at hand. 1.5 Quick rvest tutorial Now let’s do a quick rvest tutorial: url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; (paragraphs &lt;- read_html(url) %&gt;% html_nodes(&quot;p&quot;)) ## {xml_nodeset (61)} ## [1] &lt;p class=&quot;mw-empty-elt&quot;&gt; \\n&lt;/p&gt; ## [2] &lt;p class=&quot;mw-empty-elt&quot;&gt;\\n&lt;/p&gt; ## [3] &lt;p&gt;In &lt;a href=&quot;/wiki/Demographics&quot; class=&quot;mw-redirect&quot; title=&quot;Demog ... ## [4] &lt;p&gt;World population has experienced &lt;a href=&quot;/wiki/Population_growt ... ## [5] &lt;p&gt;Total annual births were highest in the late 1980s at about 139 ... ## [6] &lt;p&gt;Six of the &lt;a href=&quot;/wiki/Earth&quot; title=&quot;Earth&quot;&gt;Earth&lt;/a&gt;&#39;s seven ... ## [7] &lt;p&gt;Estimates of world population by their nature are an aspect of &lt; ... ## [8] &lt;p&gt;It is difficult for estimates to be better than rough approximat ... ## [9] &lt;p&gt;Estimates of the population of the world at the time agriculture ... ## [10] &lt;p&gt;The &lt;a href=&quot;/wiki/Plague_of_Justinian&quot; title=&quot;Plague of Justini ... ## [11] &lt;p&gt;Starting in AD 2, the &lt;a href=&quot;/wiki/Han_Dynasty&quot; class=&quot;mw-redi ... ## [12] &lt;p&gt;The &lt;a href=&quot;/wiki/Pre-Columbian_era&quot; title=&quot;Pre-Columbian era&quot;&gt; ... ## [13] &lt;p&gt;During the European &lt;a href=&quot;/wiki/British_Agricultural_Revoluti ... ## [14] &lt;p&gt;Population growth in the West became more rapid after the introd ... ## [15] &lt;p&gt;The first half of the 20th century in &lt;a href=&quot;/wiki/Russian_Emp ... ## [16] &lt;p&gt;Many countries in the &lt;a href=&quot;/wiki/Developing_world&quot; class=&quot;mw ... ## [17] &lt;p&gt;It is estimated that the world population reached one billion fo ... ## [18] &lt;p&gt;According to current projections, the global population will rea ... ## [19] &lt;p&gt;There is no estimation for the exact day or month the world&#39;s po ... ## [20] &lt;p&gt;As of 2012, the global &lt;a href=&quot;/wiki/Human_sex_ratio&quot; title=&quot;Hu ... ## ... Then we might want to actually parse out those paragraphs into text: paragraphs &lt;- read_html(url) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() paragraphs[1:10] ## [1] &quot; \\n&quot; ## [2] &quot;\\n&quot; ## [3] &quot;In demographics, the world population is the total number of humans currently living, and was estimated to have reached 7.6 billion people as of May 2018.[1] It took over 200,000 years of human history for the world&#39;s population to reach 1 billion;[2] and only 200 years more to reach 7 billion.[3]&quot; ## [4] &quot;World population has experienced continuous growth since the end of the Great Famine of 1315–17 and the Black Death in 1350, when it was near 370 million.[4] \\nThe highest population growth rates – global population increases above 1.8% per year – occurred between 1955 and 1975, peaking to 2.06% between 1965 and 1970.[5] The growth rate has declined to 1.18% between 2010 and 2015 and is projected to decline further in the course of the 21st century.[5]&quot; ## [5] &quot;Total annual births were highest in the late 1980s at about 139 million,[6] and as of 2011 were expected to remain essentially constant at a level of 135 million,[7] while deaths numbered 56 million per year and were expected to increase to 80 million per year by 2040.[8] \\nThe median age of the world&#39;s population was estimated to be 30.4 years in 2018.[9]&quot; ## [6] &quot;Six of the Earth&#39;s seven continents are permanently inhabited on a large scale. Asia is the most populous continent, with its 4.54 billion inhabitants accounting for 60% of the world population. The world&#39;s two most populated countries, China and India, together constitute about 36% of the world&#39;s population. Africa is the second most populated continent, with around 1.28 billion people, or 16% of the world&#39;s population. Europe&#39;s 742 million people make up 10% of the world&#39;s population as of 2018, while the Latin American and Caribbean regions are home to around 651 million (9%). Northern America, primarily consisting of the United States and Canada, has a population of around 363 million (5%), and Oceania, the least populated region, has about 41 million inhabitants (0.5%).[11] Though it is not permanently inhabited by any fixed population, Antarctica has a small, fluctuating international population based mainly in polar science stations. This population tends to rise in the summer months and decrease significantly in winter, as visiting researchers return to their home countries.[12]&quot; ## [7] &quot;Estimates of world population by their nature are an aspect of modernity, possible only since the Age of Discovery. Early estimates for the population of the world[14] date to the 17th century: William Petty in 1682 estimated world population at 320 million (modern estimates ranging close to twice this number); by the late 18th century, estimates ranged close to one billion (consistent with modern estimates).[15] More refined estimates, broken down by continents, were published in the first half of the 19th century, at 600 to 1000 million in the early 1800s and at 800 to 1000 million in the 1840s.[16]&quot; ## [8] &quot;It is difficult for estimates to be better than rough approximations, as even modern population estimates are fraught with uncertainties on the order of 3% to 5%.[17]&quot; ## [9] &quot;Estimates of the population of the world at the time agriculture emerged in around 10,000 BC have ranged between 1 million and 15 million.[18][19] Even earlier, genetic evidence suggests humans may have gone through a population bottleneck of between 1,000 and 10,000 people about 70,000 BC, according to the Toba catastrophe theory. By contrast, it is estimated that around 50–60 million people lived in the combined eastern and western Roman Empire in the 4th century AD.[20]&quot; ## [10] &quot;The Plague of Justinian, which first emerged during the reign of the Roman emperor Justinian, caused Europe&#39;s population to drop by around 50% between the 6th and 8th centuries AD.[21] The population of Europe was more than 70 million in 1340.[22] The Black Death pandemic of the 14th century may have reduced the world&#39;s population from an estimated 450 million in 1340 to between 350 and 375 million in 1400;[23] it took 200 years for population figures to recover.[24] The population of China decreased from 123 million in 1200 to 65 million in 1393,[25] presumably due to a combination of Mongol invasions, famine, and plague.[26]&quot; Get some other types of HTML obejects. Let’s get all the “h2” HTML elements: read_html(url) %&gt;% html_nodes(&quot;h2&quot;) %&gt;% html_text() ## [1] &quot;Contents&quot; &quot;Population by region&quot; ## [3] &quot;History&quot; &quot;Global demographics&quot; ## [5] &quot;Largest populations by country&quot; &quot;Fluctuation&quot; ## [7] &quot;Mathematical approximations&quot; &quot;Overpopulation&quot; ## [9] &quot;See also&quot; &quot;Notes&quot; ## [11] &quot;References&quot; &quot;External links&quot; ## [13] &quot;Navigation menu&quot; 1.6 Example: Parsing A Table From Wikipedia Look at the Wikipedia Page for world population: https://en.wikipedia.org/wiki/World_population We can get any table we want using rvest We might have to experiment to figure out which one Get the one that lists the ten most populous countries I think this might be the 4th or 5th table on the page How do we get this ? First we will load packages that will help us throughout this session. In this case we’ll need to figure out what number table it is we want. We could fetch all the tables and then experiment to find the precise one. library(rvest) # Use read_html to fetch the webpage url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_df &lt;- read_html(url) ten_most_populous &lt;- ten_most_df %&gt;% html_nodes(&quot;table&quot;) %&gt;% `[[`(6) %&gt;% html_table() # Let&#39;s get just the first three columns ten_most_populous &lt;- ten_most_populous[,2:4] # Get some content - Change the column names names(ten_most_populous) &lt;- c(&quot;Country_Territory&quot;,&quot;Population&quot;,&quot;Date&quot;) # Do reformatting on the columns to be actual numerics where appropriate ten_most_populous %&gt;% mutate(Population=gsub(&quot;,&quot;,&quot;&quot;,Population)) %&gt;% mutate(Population=round(as.numeric(Population)/1e+06)) %&gt;% ggplot(aes(x=Country_Territory,y=Population)) + geom_point() + labs(y = &quot;Population / 1,000,000&quot;) + coord_flip() + ggtitle(&quot;Top 10 Most Populous Countries&quot;) In the above example we leveraged the fact that we were looking specifically for a table element and it became a project to locate the correct table number. This isn’t always the case with more complicated websites in that the element we are trying to grab or scrape is contained within a nested structure that doesn’t correspond neatly to a paragraph, link, heading, or table. This can be the case if the page is heavily styled with CSS or Javascript. We might have to work harder. But it’s okay to try to use simple elements and then try to refine the search some more. 1.7 Summary Need some basic HTML and CSS knowledge to find correct elements How to extract text from common elements How to extract text from specific elements Always have to do some text cleanup of data It usually takes multiple times to get it right See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html "],
["xml.html", "Chapter 2 XML and JSON 2.1 Finding XPaths 2.2 Example: GeoCoding With Google 2.3 Using JSON 2.4 Using the RJSONIO Package", " Chapter 2 XML and JSON This is where things get a little dicey because some web pages will return XML and JSON in response to inquiries and while these formats seem complicated they are actually doing you a really big favor by doing this since these formats can ususally be easily parsed using various packges. XML is a bit hard to get your head around and JSON is the new kid on the block which is easier to use. Since this isn’t a full-on course lecture I’ll keep it short as to how and why you would want to use these but any time you spend trying to better understand JSON (and XML) the better of you will be when parsing web pages. It’s not such a big deal if all you are going to be parsing is raw text since the mthods we use to do that avoid XML and JSON although cleaning up raw text has its own problems. Let’s revisit the Wikipedia example from the previous section. library(rvest) # Use read_html to fetch the webpage url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_df &lt;- read_html(url) ten_most_populous &lt;- ten_most_df %&gt;% html_nodes(&quot;table&quot;) %&gt;% `[[`(6) %&gt;% html_table() Well we pulled out all tables and then, by experimentation, we isolated table 6 and got the content corresponding to that. But. Is there a more direct way to find the content ? There is. It requires us to install some helper plugins such as the xPath Finder for Firefox and Chrome. In reality there are a number of ways to find the XML Path or CSS Path for an element within a web page but this is a good one to start. Remeber that we want to find the table corresponding to the “10 Most Populous Countries”. So we activate the xPath finder plugin and the highlight the element of interest. This takes some practice to get it right. Once you highlight the desired elment you will see the corresponding XPATH. Here is a screenshot of what I did. We can use the resulting path to directly access the table without first having to first pull out all tables and then trying to find the right one # Use read_html to fetch the webpage url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_populous &lt;- read_html(url) ten_most_df &lt;- ten_most_populous %&gt;% html_nodes(xpath=&#39;/html/body/div[3]/div[3]/div[4]/div/table[5]&#39;) %&gt;% html_table() # We have to get the first element of the list. ten_most_df &lt;- ten_most_df[[1]] ten_most_df ## Rank Country / Territory Population Date ## 1 1 China[note 4] 1,395,100,000 November 10, 2018 ## 2 2 India 1,339,370,000 November 10, 2018 ## 3 3 United States 328,150,000 November 10, 2018 ## 4 4 Indonesia 265,015,300 July 1, 2018 ## 5 5 Pakistan 212,742,631 May 25, 2017 ## 6 6 Brazil 209,829,000 November 10, 2018 ## 7 7 Nigeria 188,500,000 October 31, 2016 ## 8 8 Bangladesh 165,508,000 November 10, 2018 ## 9 9 Russia[note 5] 146,877,088 January 1, 2018 ## 10 10 Japan 126,440,000 October 1, 2018 ## % of worldpopulation Source ## 1 18.2% [82] ## 2 17.5% [83] ## 3 4.28% [84] ## 4 3.46% [85] ## 5 2.78% [86] ## 6 2.74% [87] ## 7 2.46% [88] ## 8 2.16% [89] ## 9 1.92% [90] ## 10 1.65% [91] 2.1 Finding XPaths In addition to Browser Plugins there are standalone tools such as the Oxygen XML Editor which is availabel through the Emory Software Express Website. This is a comprehensive XML editor that will allow you to parse XML and develop paths to locate specific nodes within an XML document. If you find yourself working with websites with lots of XML then this will be useful. The Oxygen editor is free. Let’s look at an XML file that has some basic content: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;bookstore&gt; &lt;book category=&quot;COOKING&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Everyday Italian&lt;/title&gt; &lt;author&gt;Giada De Laurentiis&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;30.00&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;CHILDREN&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;WEB&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Learning XML&lt;/title&gt; &lt;author&gt;Erik T. Ray&lt;/author&gt; &lt;year&gt;2003&lt;/year&gt; &lt;price&gt;39.95&lt;/price&gt; &lt;/book&gt; &lt;/bookstore&gt; 2.2 Example: GeoCoding With Google Let’s run through an example of using the GeoCoding API with Google. They used to provide free access to this service and they still do although you are limited to a certain number. You have to sign up for an account and get an API key. Anyway, let’s run through this example and then look at how I parsed the XML file that is returned by the Google GeoCoding API. Let’s say that we wanted to get the latitude and longitude associated with the the address 1510 Clidton Rd, Atlanta, GA which corresponds to the Rollins Research Building. # https://maps.googleapis.com/maps/api/geocode/xml?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA&amp;key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM # https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA&amp;key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM myGeo &lt;- function(address=&quot;1510 Clifton Rd Atlanta GA&quot;,form=&quot;xml&quot;) { library(XML) library(RCurl) geourl &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/&quot; key &lt;- &quot;AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM&quot; address &lt;- gsub(&quot; &quot;,&quot;+&quot;,address) add &lt;- paste0(geourl,form,sep=&quot;?address=&quot;) add &lt;- paste0(add,address,&quot;&amp;key=&quot;) geourl &lt;- paste0(add,key) locale &lt;- getURL(geourl) plocal &lt;- xmlParse(locale,useInternalNodes=TRUE) # Okay let&#39;s extract the lat and lon latlon &lt;- getNodeSet(plocal,&quot;/GeocodeResponse/result/geometry/location/descendant::*&quot;) lat &lt;- as.numeric(xmlSApply(latlon,xmlValue))[1] lon &lt;- as.numeric(xmlSApply(latlon,xmlValue))[2] return(c(lat=lat,lng=lon)) } myGeo() ## lat lng ## 33.79667 -84.32319 Now. We could have saved the report to a file on our local computer and open it up with Oxygen editor and figure out what the approproate XPATH would be. This is basically what I did. Here is a screenshot of the session: We could expand this considerable to process a number of addresses: namevec &lt;- c(&quot;Atlanta GA&quot;, &quot;Birmingham AL&quot;, &quot;Seattle WA&quot;, &quot;Sacramento CA&quot;, &quot;Denver CO&quot;, &quot;LosAngeles CA&quot;, &quot;Rochester NY&quot;) cityList &lt;- lapply(namevec,myGeo) # Or to get a data frame cities &lt;- data.frame(city=namevec,do.call(rbind,cityList), stringsAsFactors = FALSE) cities ## city lat lng ## 1 Atlanta GA 33.74900 -84.38798 ## 2 Birmingham AL 33.51859 -86.81036 ## 3 Seattle WA 47.60621 -122.33207 ## 4 Sacramento CA 38.58157 -121.49440 ## 5 Denver CO 39.73924 -104.99025 ## 6 LosAngeles CA 34.05223 -118.24368 ## 7 Rochester NY 43.15658 -77.60885 # Let&#39;s create a Map library(leaflet) m &lt;- leaflet(data=cities) m &lt;- addTiles(m) m &lt;- addMarkers(m,popup=cities$city) # Put up the Map - click on the markers m 2.3 Using JSON JSON is fast becoming the primary interchange format over XML although XML is still well supported. R has a number of packages to ease the parsing of JSON/ documents returned by web pages. Ususally you get back a list which is a native data type in R that can easily be manipulated into a data frame. Most web APIs provide an option for JSON or XML although some only provide JSON. There are rules and regulations about how JSON is formed and we will learn them by example but you can look at the numerous tutotorials on the web to locate definitive references. See http://www.w3schools.com/json/ Here is an XML file that describes some employees. &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;John&lt;/firstName&gt; &lt;lastName&gt;Doe&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Anna&lt;/firstName&gt; &lt;lastName&gt;Smith&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Peter&lt;/firstName&gt; &lt;lastName&gt;Jones&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; And here is the corresposning JSON file: { &quot;employees&quot;:[ {&quot;firstName&quot;:&quot;John&quot;, &quot;lastName&quot;:&quot;Doe&quot;}, {&quot;firstName&quot;:&quot;Anna&quot;, &quot;lastName&quot;:&quot;Smith&quot;}, {&quot;firstName&quot;:&quot;Peter&quot;,&quot;lastName&quot;:&quot;Jones&quot;} ] } It is important to note that the actual information in the document, things like city name, county name, latitude, and longitude are the same as they would be in the comparable XML document. JSON documents are at the heart of the NoSQL“database”called MongoDB JSON can be found within many webpages since it is closely related to JavaScript which is a language strongly related to web pages. JSON is very compact and lighweight which has made it a natural followon to XML so much so that it appears to be replacing XML See http:..www.json.org/ for a full description of the specification An object is an unordered set of name/value pairs. An object begins with (left brace) and ends with (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma). An array is an ordered collection of values. An array begins with [ (left bracket) and ends with ] (right bracket). Values are separated by , (comma). A value can be a string in double quotes, or a number, or true or false or null, or an object or an array. These structures can be nested. A string is a sequence of zero or more Unicode characters, wrapped in double quotes, using backslash escapes. A character is represented as a single character string. Do you remember the Google Geocding example from before ? We can tell Google to send us back JSON instead of XML just by adjusting the URL accordingly: url &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Rd+Atlanta+GA&amp;key=AIzaSyAbhVdPH0wDVWUayUaj5N4r379Fbkq5NBM&quot; 2.4 Using the RJSONIO Package To read/parse this in R we use a package called RJSONIO. There are other packages but this is the one we will be using. Download and install it. There is a function called fromJSON which will parse the JSON file and return a list to contain the data. So we parse lists instead of using XPath. Many people feel this to be easier than trying to construct XPath statments. You will have to decide for yourself. library(RJSONIO) geo &lt;- fromJSON(url) str(geo,3) ## List of 2 ## $ results:List of 1 ## ..$ :List of 6 ## .. ..$ address_components:List of 7 ## .. ..$ formatted_address : chr &quot;1510 Clifton Rd, Atlanta, GA 30322, USA&quot; ## .. ..$ geometry :List of 3 ## .. ..$ place_id : chr &quot;ChIJ5QjdF_oG9YgRWAJzCm19Vf8&quot; ## .. ..$ plus_code : Named chr [1:2] &quot;QMWG+MP Druid Hills, Georgia, United States&quot; &quot;865QQMWG+MP&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;compound_code&quot; &quot;global_code&quot; ## .. ..$ types : chr &quot;street_address&quot; ## $ status : chr &quot;OK&quot; Since what we get back is a list we can directly access whatever we want. We just index into the list. No need for complicated XPATHS. geo$results[[1]]$geometry$location ## lat lng ## 33.79667 -84.32319 "],
["Moreexamples.html", "Chapter 3 More Real Life Examples 3.1 BitCoin Prices 3.2 Faculty Salaries 3.3 Filling Out Forms From a Program 3.4 PubMed", " Chapter 3 More Real Life Examples Okay. This is a tour of some sites that will serve as important examples on how to parse sites. Let’s check the price of bitcoins. You want to be rich don’t you ? 3.1 BitCoin Prices library(rvest) url &lt;- &quot;https://coinmarketcap.com/all/views/all/&quot; bc &lt;- read_html(url) path &lt;- &#39;//*[@id=&quot;currencies-all&quot;]&#39; bc_table &lt;- bc %&gt;% html_nodes(xpath=path) %&gt;% html_table() # We get back a one element list that is a data frame str(bc_table,0) ## List of 1 bc_table &lt;- bc_table[[1]] head(bc_table[,3:5]) ## Symbol Market Cap Price ## 1 BTC $111,500,535,213 $6419.13 ## 2 ETH $21,884,325,108 $212.21 ## 3 XRP $20,322,397,945 $0.505463 ## 4 BCH $9,557,556,660 $547.66 ## 5 XLM $5,075,125,487 $0.267997 ## 6 EOS $4,889,401,090 $5.40 # The data is &quot;dirty&quot; and has characers in it that need cleaning bc_table &lt;- bc_table %&gt;% select(Name,Symbol,Price) bc_table &lt;- bc_table %&gt;% mutate(Name=gsub(&quot;\\n&quot;,&quot; &quot;,Name)) bc_table &lt;- bc_table %&gt;% mutate(Name=gsub(&quot;\\\\.+&quot;,&quot;&quot;,Name)) bc_table &lt;- bc_table %&gt;% mutate(Price=gsub(&quot;\\\\$&quot;,&quot;&quot;,Price)) bc_table &lt;- bc_table %&gt;% mutate(Price=round(as.numeric(Price),2)) ## Warning in evalq(round(as.numeric(Price), 2), &lt;environment&gt;): NAs ## introduced by coercion # There are four rows wherein the Price is missing NA bc_table &lt;- bc_table %&gt;% filter(complete.cases(bc_table)) # Let&#39;s get the Crypto currencies with the Top 10 highest prices top_10 &lt;- bc_table %&gt;% arrange(desc(Price)) %&gt;% head(10) # Next we want to make a barplot of the Top 10 ylim=c(0,max(top_10$Price)+10000) main=&quot;Top 10 Crypto Currencies in Terms of Price&quot; bp &lt;- barplot(top_10$Price,col=&quot;aquamarine&quot;, ylim=ylim,main=main) axis(1, at=bp, labels=top_10$Symbol, cex.axis = 0.7) grid() # Let&#39;s take the log of the price ylim=c(0,max(log(top_10$Price))+5) main=&quot;Top 10 Crypto Currencies in Terms of log(Price)&quot; bp &lt;- barplot(log(top_10$Price),col=&quot;aquamarine&quot;, ylim=ylim,main=main) axis(1, at=bp, labels=top_10$Symbol, cex.axis = 0.7) grid() 3.2 Faculty Salaries In this example we have to parse the main table associated with the results page. Salary Salary url &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey&quot; df &lt;- read_html(url) %&gt;% html_table() %&gt;% `[[`(1) intost &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;) salary &lt;- df %&gt;% separate(InstitutionCategoryState,into=intost,sep=&quot;\\n&quot;) salary ## Institution Category State ## 1 Auburn University Doctoral ALABAMA ## 2 Birmingham Southern College Baccalaureate ALABAMA ## 3 Huntingdon College Baccalaureate ALABAMA ## 4 Samford University Master ALABAMA ## 5 Troy University Master ALABAMA ## 6 The University of Alabama Doctoral ALABAMA ## 7 University of Alabama at Birmingham Doctoral ALABAMA ## 8 University of Alabama in Huntsville Doctoral ALABAMA ## 9 University of Montevallo Master ALABAMA ## 10 University of South Alabama Master ALABAMA ## Avg. SalaryFull Professors Avg. ChangeContinuing Full Professors ## 1 $126,900 3.7% ## 2 $80,400 0.3% ## 3 $76,700 0.0% ## 4 $104,500 1.4% ## 5 $84,500 1.6% ## 6 $148,100 2.0% ## 7 $135,100 1.9% ## 8 $131,000 2.4% ## 9 $79,800 0.0% ## 10 $89,100 N/A ## CountFull Professors Avg. Total CompensationFull Professors ## 1 411 $181,600 ## 2 38 $102,800 ## 3 13 $89,500 ## 4 117 $141,300 ## 5 30 $88,500 ## 6 300 $202,400 ## 7 191 $178,800 ## 8 62 $177,700 ## 9 44 - ## 10 111 $97,100 ## Salary EquityFull Professors ## 1 88.8 ## 2 95.5 ## 3 109.6 ## 4 84.3 ## 5 105.7 ## 6 86.7 ## 7 89.4 ## 8 92.0 ## 9 93.4 ## 10 97.6 So we could process multiple pages # So now we could process multiple pages url &lt;- &#39;https://www.insidehighered.com/aaup-compensation-survey?institution-name=&amp;professor-category=1591&amp;page=1&#39; str1 &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey?&quot; str2 &lt;- &quot;institution-name=&amp;professor-category=1591&amp;page=&quot; intost &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;) salary &lt;- data.frame() for (ii in 1:2) { nurl &lt;- paste(str1,str2,ii,sep=&quot;&quot;) df &lt;- read_html(nurl) tmp &lt;- df %&gt;% html_table() %&gt;% `[[`(1) tmp &lt;- tmp %&gt;% separate(InstitutionCategoryState,into=intost,sep=&quot;\\n&quot;) salary &lt;- rbind(salary,tmp) } salary Look at the URLs at the bottom of the main page to find beginning and ending page numbers. Visually this is easy. Programmatically we could do something like the following: # https://www.insidehighered.com/aaup-compensation-survey?page=1 # https://www.insidehighered.com/aaup-compensation-survey?page=94 # What is the last page number ? We already know the answer - 94 lastnum &lt;- df %&gt;% html_nodes(xpath=&#39;//a&#39;) %&gt;% html_attr(&quot;href&quot;) %&gt;% &#39;[&#39;(103) %&gt;% strsplit(.,&quot;page=&quot;) %&gt;% &#39;[[&#39;(1) %&gt;% &#39;[&#39;(2) %&gt;% as.numeric(.) # So now we could get all pages of the survey str1 &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey?&quot; str2 &lt;- &quot;institution-name=&amp;professor-category=1591&amp;page=&quot; intost &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;) salary &lt;- data.frame() for (ii in 1:lastnum) { nurl &lt;- paste(str1,str2,ii,sep=&quot;&quot;) df &lt;- read_html(nurl) tmp &lt;- df %&gt;% html_table() %&gt;% `[[`(1) tmp &lt;- tmp %&gt;% separate(InstitutionCategoryState,into=intost,sep=&quot;\\n&quot;) salary &lt;- rbind(salary,tmp) Sys.sleep(1) } names(salary) &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;,&quot;AvgSalFP&quot;,&quot;AvgChgFP&quot;, &quot;CntFP&quot;,&quot;AvgTotCompFP&quot;,&quot;SalEquityFP&quot;) salary &lt;- salary %&gt;% mutate(AvgSalFP=as.numeric(gsub(&quot;\\\\$|,&quot;,&quot;&quot;,salary$AvgSalFP))) %&gt;% mutate(AvgTotCompFP=as.numeric(gsub(&quot;\\\\$|,&quot;,&quot;&quot;,salary$AvgTotCompFP))) salary %&gt;% group_by(State,Category) %&gt;% summarize(avg=mean(AvgSalFP)) %&gt;% arrange(desc(avg)) There are some problems: Data is large and scattered across multiple pages We could use above techniques to move from page to page There is a form we could use to narrow criteria But we have to programmatically submit the form rvest (and other packages) let you do this 3.3 Filling Out Forms From a Program Salary Let’s find salaries between $ 150,000 and the default max ($ 244,000) Find the element name associated with “Average Salary” Establish a connection with the form (usually the url of the page) Get a local copy of the form Fill in the value for the “Average Salary” Submit the lled in form Get the results and parse them like above ` So finding the correct element is more challenging. I use Chrome to do this. Just highlight the area over the form and right click to “Insepct” the element. This opens up the developer tools. You have to dig down to find the corrext form and the element name. Here is a screen shot of my activity: Salary url &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey&quot; # Establish a session mysess &lt;- html_session(url) # Get the form form_unfilled &lt;- mysess %&gt;% html_node(&quot;form&quot;) %&gt;% html_form() form_filled &lt;- form_unfilled %&gt;% set_values(&quot;range-from&quot;=150000) # Submit form results &lt;- submit_form(mysess,form_filled) ## Submitting with &#39;op&#39; first_page &lt;- results %&gt;% html_nodes(xpath=expr) %&gt;% html_table() first_page ## [[1]] ## InstitutionCategoryState ## 1 Ohio State University-Main Campus\\nDoctoral\\nOHIO ## 2 University of Illinois at Urbana-Champaign\\nDoctoral\\nILLINOIS ## 3 University of Colorado Denver/Anschutz Medical Campus\\nDoctoral\\nCOLORADO ## 4 University of Houston\\nDoctoral\\nTEXAS ## 5 Amherst College\\nBaccalaureate\\nMASSACHUSETTS ## 6 University of California-Merced\\nBaccalaureate\\nCALIFORNIA ## 7 University of Illinois at Chicago\\nDoctoral\\nILLINOIS ## 8 Wake Forest University\\nMaster\\nNORTH CAROLINA ## 9 Rensselaer Polytechnic Institute\\nDoctoral\\nNEW YORK ## 10 Brandeis University\\nDoctoral\\nMASSACHUSETTS ## Avg. SalaryFull Professors Avg. ChangeContinuing Full Professors ## 1 $150,000 2.7% ## 2 $150,500 4.1% ## 3 $150,800 2.2% ## 4 $150,900 0.9% ## 5 $151,000 2.8% ## 6 $151,100 N/A ## 7 $151,100 4.1% ## 8 $151,700 3.2% ## 9 $151,800 3.3% ## 10 $151,900 3.5% ## CountFull Professors Avg. Total CompensationFull Professors ## 1 994 $195,700 ## 2 833 $190,600 ## 3 179 $207,200 ## 4 427 $183,300 ## 5 85 $196,700 ## 6 55 $204,500 ## 7 346 $194,000 ## 8 194 $191,000 ## 9 199 $204,000 ## 10 150 $194,300 ## Salary EquityFull Professors ## 1 89.6 ## 2 91.4 ## 3 93.1 ## 4 85.8 ## 5 92.3 ## 6 88.9 ## 7 94.4 ## 8 85.4 ## 9 98.6 ## 10 96.6 3.4 PubMed Pubmed provides a rich source of information on published scientific literature. There are tutorials on how to leverage its capabilities but one thing to consider is that MESH terms are a good starting place since the search is index-based. MeSH (Medical Subject Headings) is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed. It’s faster and more accurate so you can first use the MESH browser to generate the appropriate search terms and add that into the Search interface. The MESH browser can be found at https://www.ncbi.nlm.nih.gov/mesh/ What we do here is get the links associated with each publication so we can then process each of those and get the abstract associated with each publication. url &lt;- &quot;https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D&quot; # results &lt;- read_html(url) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% grep(&quot;/pubmed/[0-9]{1,6}&quot;,.,value=TRUE) %&gt;% unique(.) results ## [1] &quot;/pubmed/30212930&quot; &quot;/pubmed/29656667&quot; &quot;/pubmed/29272506&quot; ## [4] &quot;/pubmed/29148012&quot; &quot;/pubmed/29122999&quot; &quot;/pubmed/29122220&quot; ## [7] &quot;/pubmed/28978613&quot; &quot;/pubmed/28963833&quot; &quot;/pubmed/28835369&quot; ## [10] &quot;/pubmed/28710094&quot; &quot;/pubmed/28679363&quot; &quot;/pubmed/28651528&quot; ## [13] &quot;/pubmed/28614637&quot; &quot;/pubmed/28597060&quot; &quot;/pubmed/28546368&quot; ## [16] &quot;/pubmed/28535526&quot; &quot;/pubmed/28495360&quot; &quot;/pubmed/28490435&quot; ## [19] &quot;/pubmed/28485119&quot; &quot;/pubmed/28448966&quot; So now we could loop through these links and get the abstracts for these results. It looks that there are approximately 10 results per page. As before we would have to dive in to the underlying structure of the page to get the correct XML pathnames or we could look for Paragraph elements and pick out the links that way. text.vec &lt;- vector() for (ii in 1:length(results)) { string &lt;- paste0(&quot;https://www.ncbi.nlm.nih.gov&quot;,results[ii]) text.vec[ii] &lt;- read_html(string) %&gt;% html_nodes(&quot;p&quot;) %&gt;% `[[`(10) %&gt;% html_text() } # Eliminate lines with newlines characters final.vec &lt;- text.vec[grep(&quot;^\\n&quot;,text.vec,invert=TRUE)] final.vec ## [1] &quot;End-stage renal disease (ESRD) is the final stage of chronic kidney disease in which the kidney is not sufficient to meet the needs of daily life. It is necessary to understand the role of genes expression involved in ESRD patient responses to nocturnal hemodialysis (NHD) and to improve the immunity responsiveness. The aim of this study was to investigate novel immune-associated genes that may play important roles in patients with ESRD.The microarray expression profiles of peripheral blood in patients with ESRD before and after NHD were analyzed by network-based approaches, and then using Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes pathway analysis to explore the biological process and molecular functions of differentially expressed genes. Subsequently, a transcriptional regulatory network of the core genes and the connected transcriptional regulators was constructed. We found that NHD had a significant effect on neutrophil activation and immune response in patients with ESRD.In addition, Our findings suggest that MAPKAPK3, RHOA, ARRB2, FLOT1, MYH9, PRKCD, RHOG, PTPN6, MAPK3, CNPY3, PI3KCG, and PYGL genes maybe potential targets regulated by core transcriptional factors, including ARNT, C/EBPalpha, CEBPA, CREB1, PSG1, DAND5, SP1, GATA1, MYC, EGR2, and EGR3.&quot; ## [2] &quot;Only a minority of patients with chronic kidney disease treated by hemodialysis are currently treated at home. Until relatively recently, the only type of hemodialysis machine available for these patients was a slightly smaller version of the standard machines used for in-center dialysis treatments. Areas covered: There are now an alternative generation of dialysis machines specifically designed for home hemodialysis. The home dialysis patient wants a smaller machine, which is intuitive to use, easy to trouble shoot, robust and reliable, quick to setup and put away, requiring minimal waste disposal. The machines designed for home dialysis have some similarities in terms of touch-screen patient interfaces, and using pre-prepared cartridges to speed up setting up the machine. On the other hand, they differ in terms of whether they use slower or standard dialysate flows, prepare batches of dialysis fluid, require separate water purification equipment, or whether this is integrated, or use pre-prepared sterile bags of dialysis fluid. Expert commentary: Dialysis machine complexity is one of the hurdles reducing the number of patients opting for home hemodialysis and the introduction of the newer generation of dialysis machines designed for ease of use will hopefully increase the number of patients opting for home hemodialysis.&quot; ## [3] &quot;Home haemodialysis (HHD) has the potential to impact positively on patient outcomes and health resource management. There has been rejuvenated international interest in HHD in recent years.&quot; ## [4] &quot;Assess the feasibility of using a bovine carotid artery graft for buttonhole technique hemodialysis in patients who are not candidates for a native fistula.&quot; ## [5] &quot;renal system; urology; vascular surgery&quot; ## [6] &quot;NxStage System One is a new dialytic technology based on easy setup, simplicity of use and reduced dimensions, which is increasingly in use worldwide for home hemodialysis treatments. The system utilizes a low amount of dialysate, usually 15-30 liters according to anthropometric patients&#39; values. The dialysate is supplied at very low flux, generally about 1/3 of blood flow, in order to obtain an elevated saturation of dialysate for solutes. In these conditions the clearance of urea will be almost equal to dialysate flow rate. In order to achieve an obptimal weekly clearance evaluated by Std Kt/V the dialysis sessions are repeated six times a week. In this way a good control of blood voleme can be reached. In this paper we report our experience of treatment with NxStage System One in 12 patients from May 2011 to Dicember 2016.&quot; ## [7] &quot;Data on racial disparities in home dialysis utilization and outcomes are lacking in Canada, where health care is universally available.&quot; ## [8] &quot;Telehealth and remote monitoring of a patient&#39;s health status has become more commonplace in the last decade and has been applied to conditions such as heart failure, diabetes mellitus, hypertension, and chronic obstructive pulmonary disease. Conversely, uptake of these technologies to help engender and support home RRTs has lagged. Although studies have looked at the role of telehealth in RRT, they are small and single-centered, and both outcome and cost-effectiveness data are needed to inform future decision making. Furthermore, alignment of payer and government (federal and state) regulations with telehealth procedures is needed along with a better understanding of the viewpoints of the various stakeholders in this process (patients, caregivers, clinicians, payers, dialysis organizations, and government regulators). Despite these barriers, telehealth has great potential to increase the acceptance of home dialysis, and improve outcomes and patient satisfaction while potentially decreasing costs. The Kidney Health Initiative convened a multidisciplinary workgroup to examine the current state of telehealth use in home RRTs as well as outline potential benefits and drawbacks, impediments to implementation, and key unanswered questions.&quot; ## [9] &quot;Patients starting dialysis often have substantial residual kidney function. Incremental hemodialysis provides a hemodialysis prescription that supplements patients&#39; residual kidney function while maintaining total (residual + dialysis) urea clearance (standard Kt/Vurea) targets. We describe our experience with incremental hemodialysis in patients using NxStage System One for home hemodialysis.&quot; ## [10] &quot;Internationally, the use of patient-reported outcomes (PROs) is increasing. Electronic PROs (ePROs) offer immediate access of such reports to healthcare providers. The objectives of this study were to assess nurses&#39; perspectives on the usefulness and impact of ePRO administration in home dialysis clinics and assess patient perceptions of satisfaction with nursing care following use of ePROs.&quot; ## [11] &quot;The Lancashire Teaching Hospitals NHS Trust in the UK has been providing renal care through video-as-a-service (VAAS) to patients since 2013, with support from the North West NHS Shared Infrastructure Service, a collaborative team that supports information and communication technology use in the UK National Health Service.&quot; ## [12] &quot;Neonatal autosomal recessive polycystic kidney disease (ARPKD) is associated with giant kidneys, lung hypoplasia, pulmonal hypertension, and end-stage renal failure. Depending on the study, mortality is reported to range between 20 and 80%.&quot; ## [13] &quot;Improvement in the rates of home dialysis has been a desirable but difficult-to-achieve target for United States nephrology. Provision of comprehensive predialysis education (CPE) in institutes with established home dialysis programs has been shown to facilitate a higher home dialysis choice amongst chronic kidney disease (CKD) patients. Unfortunately, limited data have shown the efficacy of such programs in the United States or in institutes with small home dialysis (HoD) programs.&quot; ## [14] &quot;Home hemodialysis (HD) is undergoing a resurgence. A major driver of this is economics, however, providers are also encouraged by a combination of excellent patient outcomes and patient experiences as well as the development of newer technologies that offer ease of use. Home HD offers significant advantages in flexible scheduling and the practical implementation of extended hours dialysis. This paper explores the reasons why home HD is making a comeback and strives to offer approaches to improve the uptake of this dialysis modality.&quot; ## [15] &quot;Bone deformities and fractures are common consequences of renal osteodystrophy in the dialysis population. Persistent hypophosphatemia may be observed with more frequent home hemodialysis regimens, but the specific effects on the skeleton are unknown. We present a patient with end-stage renal disease treated with frequent home hemodialysis who developed severe bone pain and multiple fractures, including a hip fracture and a tibia-fibula fracture complicated by nonunion, rendering her nonambulatory and wheelchair bound for more than a year. A bone biopsy revealed severe osteomalacia, likely secondary to chronic hypophosphatemia and hypocalcemia. Treatment changes included the addition of phosphate to the dialysate, a higher dialysate calcium concentration, and increased calcitriol dose. Several months later, the patient no longer required a wheelchair and was able to ambulate without pain. Repeat bone biopsy revealed marked improvements in bone mineralization and turnover parameters. Also, with increased dialysate phosphate and calcium concentrations, as well as increased calcitriol, circulating fibroblast growth factor 23 levels increased.&quot; ## [16] &quot;The ESRD Prospective Payment System introduced two incentives to increase home dialysis use: bundling injectable medications into a single payment for treatment and paying for home dialysis training. We evaluated the effects of the ESRD Prospective Payment System on home dialysis use by patients starting dialysis in the United States from January 1, 2006 to August 31, 2013. We analyzed data on dialysis modality, insurance type, and comorbidities from the United States Renal Data System. We estimated the effect of the policy on home dialysis use with multivariable logistic regression and compared the effect on Medicare Parts A/B beneficiaries with the effect on patients with other types of insurance. The ESRD Prospective Payment System associated with a 5.0% (95% confidence interval [95% CI], 4.0% to 6.0%) increase in home dialysis use by the end of the study period. Home dialysis use increased by 5.8% (95% CI, 4.3% to 6.9%) among Medicare beneficiaries and 4.1% (95% CI, 2.3% to 5.4%) among patients covered by other forms of health insurance. The difference between these groups was not statistically significant (1.8%; 95% CI, -0.2% to 3.8%). Conversely, in both populations, the training add-on did not associate with increases in home dialysis use beyond the effect of the policy. The ESRD Prospective Payment System bundling, but not the training add-on, associated with substantial increases in home dialysis, which were identical for both Medicare and non-Medicare patients. These spill-over effects suggest that major payment changes in Medicare can affect all patients with ESRD.&quot; ## [17] &quot;We present a case of a patient on home hemodialysis who developed Mycobacterium mucogenicum bacteremia. While infections with this particular organism are rare, disseminated infections have been reported and have been associated with significant morbidity and mortality. Diagnosis required appropriate cultures, understanding of natural habitat of organism and complete environmental analysis including blood, dialysis sample port, reverse osmosis and incoming water supply cultures. The patient was treated successfully with systemic antibiotics, removal of central venous catheter, patient education and complete exchange of the hemodialysis circuit.&quot; Well that was a lot of work and it’s very tedious. And we procssed only the first page of results. How do we “progrmmatically” hit the “Next” Button at the bottom of the page ? This is complicated by the fact that there appears to be some Javascript at work that we would have to somehow interact with to get the URL for the next page. Unlike with the school salary example it isn’t obvious how to do this. If we hove over the “Next” button we don’t get an associated link. "],
["APIs.html", "Chapter 4 APIs 4.1 OMDB 4.2 The omdbapi package 4.3 RSelenium 4.4 EasyPubMed", " Chapter 4 APIs 4.1 OMDB Let’s look at the IMDB page whic catalogues lots of information about movies. Just got to the web site and search although here is an example link. https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2 In this case we would like to get the summary information for the movie. So we would use Selector Gadget or some other method to find the XPATH or CSS associated with this element. This pretty easy and doesn’t present much of a problem although for large scale mining of movie data we would run into trouble because IMDB doesn’t really like you to scrape their pages. They have an API that they would like for you to use. url &lt;- &#39;https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2&#39; summary &lt;- read_html(url) %&gt;% html_nodes(&quot;.summary_text&quot;) %&gt;% html_text() But here we go again. We have to parse the desired elements on this page and then what if we wanted to follow other links or set up a general function to search IMDB for other movies of various genres, titles, directors, etc. So as an example on how this works. Paste the URL into any web browser. You must supply your key for this to work. What you get back is a JSON formatted entry corresponding to ”The GodFather”movie. url &lt;- &quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=The+Godfather&quot; library(RJSONIO) url &lt;- &quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=The+Godfather&quot; # Fetch the URL via fromJSON movie &lt;- fromJSON(&quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=The+Godfather&quot;) # We get back a list which is much easier to process than raw JSON or XML str(movie) ## List of 25 ## $ Title : chr &quot;The Godfather&quot; ## $ Year : chr &quot;1972&quot; ## $ Rated : chr &quot;R&quot; ## $ Released : chr &quot;24 Mar 1972&quot; ## $ Runtime : chr &quot;175 min&quot; ## $ Genre : chr &quot;Crime, Drama&quot; ## $ Director : chr &quot;Francis Ford Coppola&quot; ## $ Writer : chr &quot;Mario Puzo (screenplay by), Francis Ford Coppola (screenplay by), Mario Puzo (based on the novel by)&quot; ## $ Actors : chr &quot;Marlon Brando, Al Pacino, James Caan, Richard S. Castellano&quot; ## $ Plot : chr &quot;The aging patriarch of an organized crime dynasty transfers control of his clandestine empire to his reluctant son.&quot; ## $ Language : chr &quot;English, Italian, Latin&quot; ## $ Country : chr &quot;USA&quot; ## $ Awards : chr &quot;Won 3 Oscars. Another 24 wins &amp; 28 nominations.&quot; ## $ Poster : chr &quot;https://m.media-amazon.com/images/M/MV5BM2MyNjYxNmUtYTAwNi00MTYxLWJmNWYtYzZlODY3ZTk3OTFlXkEyXkFqcGdeQXVyNzkwMjQ&quot;| __truncated__ ## $ Ratings :List of 3 ## ..$ : Named chr [1:2] &quot;Internet Movie Database&quot; &quot;9.2/10&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Source&quot; &quot;Value&quot; ## ..$ : Named chr [1:2] &quot;Rotten Tomatoes&quot; &quot;98%&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Source&quot; &quot;Value&quot; ## ..$ : Named chr [1:2] &quot;Metacritic&quot; &quot;100/100&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Source&quot; &quot;Value&quot; ## $ Metascore : chr &quot;100&quot; ## $ imdbRating: chr &quot;9.2&quot; ## $ imdbVotes : chr &quot;1,378,375&quot; ## $ imdbID : chr &quot;tt0068646&quot; ## $ Type : chr &quot;movie&quot; ## $ DVD : chr &quot;09 Oct 2001&quot; ## $ BoxOffice : chr &quot;N/A&quot; ## $ Production: chr &quot;Paramount Pictures&quot; ## $ Website : chr &quot;http://www.thegodfather.com&quot; ## $ Response : chr &quot;True&quot; movie$Plot ## [1] &quot;The aging patriarch of an organized crime dynasty transfers control of his clandestine empire to his reluctant son.&quot; sapply(movie$Ratings,unlist) ## [,1] [,2] [,3] ## Source &quot;Internet Movie Database&quot; &quot;Rotten Tomatoes&quot; &quot;Metacritic&quot; ## Value &quot;9.2/10&quot; &quot;98%&quot; &quot;100/100&quot; Let’s Get all the Episodes for Season 1 of Game of Thrones url &lt;- &quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=Game%20of%20Thrones&amp;Season=1&quot; movie &lt;- fromJSON(url) str(movie,1) ## List of 5 ## $ Title : chr &quot;Game of Thrones&quot; ## $ Season : chr &quot;1&quot; ## $ totalSeasons: chr &quot;8&quot; ## $ Episodes :List of 10 ## $ Response : chr &quot;True&quot; episodes &lt;- data.frame(do.call(rbind,movie$Episodes),stringsAsFactors = FALSE) episodes ## Title Released Episode imdbRating ## 1 Winter Is Coming 2011-04-17 1 9.0 ## 2 The Kingsroad 2011-04-24 2 8.8 ## 3 Lord Snow 2011-05-01 3 8.7 ## 4 Cripples, Bastards, and Broken Things 2011-05-08 4 8.8 ## 5 The Wolf and the Lion 2011-05-15 5 9.1 ## 6 A Golden Crown 2011-05-22 6 9.2 ## 7 You Win or You Die 2011-05-29 7 9.3 ## 8 The Pointy End 2011-06-05 8 9.1 ## 9 Baelor 2011-06-12 9 9.6 ## 10 Fire and Blood 2011-06-19 10 9.5 ## imdbID ## 1 tt1480055 ## 2 tt1668746 ## 3 tt1829962 ## 4 tt1829963 ## 5 tt1829964 ## 6 tt1837862 ## 7 tt1837863 ## 8 tt1837864 ## 9 tt1851398 ## 10 tt1851397 4.2 The omdbapi package Wait a minute. Looks like someone created an R package that wraps all this for us. It is called omdbapi # Use devtools to install devtools::install_github(&quot;hrbrmstr/omdbapi&quot;) library(omdbapi) # The first time you use this you will be prompted to enter your # API key movie_df &lt;- search_by_title(&quot;Star Wars&quot;, page = 2) (movie_df &lt;- movie_df[,-5]) ## Title Year imdbID Type ## 1 Star Wars: The Clone Wars 2008 tt1185834 movie ## 2 Star Wars: The Clone Wars 2008–2019 tt0458290 series ## 3 Star Wars Rebels 2014–2018 tt2930604 series ## 4 Star Wars: Clone Wars 2003–2005 tt0361243 series ## 5 The Star Wars Holiday Special 1978 tt0193524 movie ## 6 Robot Chicken: Star Wars 2007 tt1020990 movie ## 7 Star Wars: Knights of the Old Republic 2003 tt0356070 game ## 8 Star Wars: The Force Unleashed 2008 tt1024923 game ## 9 Star Wars: Battlefront II 2005 tt0483168 game ## 10 Robot Chicken: Star Wars Episode II 2008 tt1334272 movie # Get lots of info on The GodFather (gf &lt;- find_by_title(&quot;The GodFather&quot;)) ## # A tibble: 3 x 25 ## Title Year Rated Released Runtime Genre Director Writer Actors Plot ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The … 1972 R 1972-03-24 175 min Crim… Francis… Mario… Marlo… The … ## 2 The … 1972 R 1972-03-24 175 min Crim… Francis… Mario… Marlo… The … ## 3 The … 1972 R 1972-03-24 175 min Crim… Francis… Mario… Marlo… The … ## # ... with 15 more variables: Language &lt;chr&gt;, Country &lt;chr&gt;, Awards &lt;chr&gt;, ## # Poster &lt;chr&gt;, Ratings &lt;list&gt;, Metascore &lt;chr&gt;, imdbRating &lt;dbl&gt;, ## # imdbVotes &lt;dbl&gt;, imdbID &lt;chr&gt;, Type &lt;chr&gt;, DVD &lt;date&gt;, ## # BoxOffice &lt;chr&gt;, Production &lt;chr&gt;, Website &lt;chr&gt;, Response &lt;chr&gt; # Get the actors from the GodFather get_actors((gf)) ## [1] &quot;Marlon Brando&quot; &quot;Al Pacino&quot; &quot;James Caan&quot; ## [4] &quot;Richard S. Castellano&quot; 4.3 RSelenium Sometimes we interact with websites that use Javascript to load more text or comments in a user forum. Here is an example of that. Look at https://www.dailystrength.org/group/dialysis which is a website associated with people wanting to share information about dialysis. If you check the bottom of the pag you will see a button. # https://www.dailystrength.org/group/dialysis library(RSelenium) library(rvest) library(tm) library(SentimentAnalysis) library(wordcloud) url &lt;- &quot;https://www.dailystrength.org/group/dialysis&quot; # The website has a &quot;show more&quot; button that hides most of the patient posts # If we don&#39;t find a way to programmatically &quot;click&quot; this button then we can # only get a few of the posts and their responses. To do this we need to # use the RSelenium package which does a lot of behind the scenes work # See https://cran.r-project.org/web/packages/RSelenium/RSelenium.pdf # http://brazenly.blogspot.com/2016/05/r-advanced-web-scraping-dynamic.html # Open up a connection rD &lt;- rsDriver() remDr &lt;- rD[[&quot;client&quot;]] remDr$navigate(url) loadmorebutton &lt;- remDr$findElement(using = &#39;css selector&#39;, &quot;#load-more-discussions&quot;) # Do this a number of times to get more links loadmorebutton$clickElement() # Now get the page with more comments and questions page_source &lt;- remDr$getPageSource() # So let&#39;s parse the contents comments &lt;- read_html(page_source[[1]]) cumulative_comments &lt;- vector() links &lt;- comments %&gt;% html_nodes(css=&quot;.newsfeed__description&quot;) %&gt;% html_node(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) full_links &lt;- paste0(&quot;https://www.dailystrength.org&quot;,links) if (length(grep(&quot;NA&quot;,full_links)) &gt; 0) { full_links &lt;- full_links[-grep(&quot;NA&quot;,full_links)] } ugly_xpath &lt;- &#39;//*[contains(concat( &quot; &quot;, @class, &quot; &quot; ), concat( &quot; &quot;, &quot;comments__comment-text&quot;, &quot; &quot; ))] | //p&#39; for (ii in 1:length(full_links)) { text &lt;- read_html(full_links[ii]) %&gt;% html_nodes(xpath=ugly_xpath) %&gt;% html_text() length(text) &lt;- length(text) - 1 text &lt;- text[-1] text cumulative_comments &lt;- c(cumulative_comments,text) } remDr$close() # stop the selenium server rD[[&quot;server&quot;]]$stop() 4.4 EasyPubMed So there is an R package called EasyPubMed that helps ease the access of data on the Internet. The idea behind this package is to be able to query NCBI Entrez and retrieve PubMed records in XML or TXT format. The PubMed records can be downloaded and saved as XML or text files if desired. According to the package authours, “Data integrity is enforced during data download, allowing to retrieve and save very large number of records effortlessly”. The bottom line is that you can do what you want after that. Let’s look at an example involving home hemodialysis library(easyPubMed) Let’s do some searching my_query &lt;- &#39;hemodialysis, home&quot; [MeSH Terms]&#39; my_entrez_id &lt;- get_pubmed_ids(my_query) my_abstracts_txt &lt;- fetch_pubmed_data(my_entrez_id, format = &quot;abstract&quot;) my_abstracts_txt[1:10] ## [1] &quot;&quot; ## [2] &quot;1. Medicine (Baltimore). 2018 Sep;97(37):e12018. doi: 10.1097/MD.0000000000012018.&quot; ## [3] &quot;&quot; ## [4] &quot;Gene co-expression network analysis identifies the hub genes associated with&quot; ## [5] &quot;immune functions for nocturnal hemodialysis in patients with end-stage renal&quot; ## [6] &quot;disease.&quot; ## [7] &quot;&quot; ## [8] &quot;Dai H, Zhou J, Zhu B.&quot; ## [9] &quot;&quot; ## [10] &quot;End-stage renal disease (ESRD) is the final stage of chronic kidney disease in&quot; my_abstracts_xml &lt;- fetch_pubmed_data(my_entrez_id) Well the results aren’t bad but we still have to parse out some junk but nothing like before. It’s a minor problem. In this case the API gives us some XML back that we then have to parse. my_abstracts &lt;- unlist(xpathApply(my_abstracts_xml, &quot;//AbstractText&quot;, saveXML)) # Still have to clear out some junk in each abstract my_abstracts &lt;- gsub(&#39;&lt;(.*)&quot;&gt;&#39;,&quot;&quot;,my_abstracts) my_abstracts &lt;- gsub(&#39;&lt;AbstractText&gt;|&lt;/AbstractText&gt;&#39;,&quot;&quot;,my_abstracts) length(my_abstracts) ## [1] 935 But we have around 935 abstracts that we can now mess with. The benefit of this approach is that we dont’ have to do all the text conversions and manipulations that we would have with the manual approach. Plus, we are able to get a lot more results more conveniently. my_abstracts[1:3] ## [1] &quot;End-stage renal disease (ESRD) is the final stage of chronic kidney disease in which the kidney is not sufficient to meet the needs of daily life. It is necessary to understand the role of genes expression involved in ESRD patient responses to nocturnal hemodialysis (NHD) and to improve the immunity responsiveness. The aim of this study was to investigate novel immune-associated genes that may play important roles in patients with ESRD.The microarray expression profiles of peripheral blood in patients with ESRD before and after NHD were analyzed by network-based approaches, and then using Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes pathway analysis to explore the biological process and molecular functions of differentially expressed genes. Subsequently, a transcriptional regulatory network of the core genes and the connected transcriptional regulators was constructed. We found that NHD had a significant effect on neutrophil activation and immune response in patients with ESRD.In addition, Our findings suggest that MAPKAPK3, RHOA, ARRB2, FLOT1, MYH9, PRKCD, RHOG, PTPN6, MAPK3, CNPY3, PI3KCG, and PYGL genes maybe potential targets regulated by core transcriptional factors, including ARNT, C/EBPalpha, CEBPA, CREB1, PSG1, DAND5, SP1, GATA1, MYC, EGR2, and EGR3.&quot; ## [2] &quot;Only a minority of patients with chronic kidney disease treated by hemodialysis are currently treated at home. Until relatively recently, the only type of hemodialysis machine available for these patients was a slightly smaller version of the standard machines used for in-center dialysis treatments. Areas covered: There are now an alternative generation of dialysis machines specifically designed for home hemodialysis. The home dialysis patient wants a smaller machine, which is intuitive to use, easy to trouble shoot, robust and reliable, quick to setup and put away, requiring minimal waste disposal. The machines designed for home dialysis have some similarities in terms of touch-screen patient interfaces, and using pre-prepared cartridges to speed up setting up the machine. On the other hand, they differ in terms of whether they use slower or standard dialysate flows, prepare batches of dialysis fluid, require separate water purification equipment, or whether this is integrated, or use pre-prepared sterile bags of dialysis fluid. Expert commentary: Dialysis machine complexity is one of the hurdles reducing the number of patients opting for home hemodialysis and the introduction of the newer generation of dialysis machines designed for ease of use will hopefully increase the number of patients opting for home hemodialysis.&quot; ## [3] &quot;Home haemodialysis (HHD) has the potential to impact positively on patient outcomes and health resource management. There has been rejuvenated international interest in HHD in recent years.&quot; "],
["bagofwords.html", "Chapter 5 Bag of Words Sentiment Analysis 5.1 Workflow 5.2 Simple Example 5.3 tidytext 5.4 Back To The PubMed Example 5.5 BiGrams", " Chapter 5 Bag of Words Sentiment Analysis One we have a collection of text it’s interesting to figure out what it might mean or infer - if anything at all. In text analysis and NLP (Natural Language Processing) we talk about “Bag of Words” to describe a collection or “corpus” of unstructured text. What do we do with a “bag of words” ? Extract meaning from collections of text (without reading !) Detect and analyze patterns in unstructured textual collections Use Natural Language Processing techniques to reach conclusions Discover what ideas occur in text and how they might be linked Determine if the discovered patterns be used to predict behavior ? Identify interesting ideas that might otherwise be ignored 5.1 Workflow Identify and Obtain text (e.g. websites, Twitter, Databases, PDFs, surveys) Create a text ”Corpus”- a structure that contains the raw text Apply transformations: Normalize case (convert to lower case) Remove puncutation and stopwords Remove domain specific stopwords Perform Analysis and Visualizations (word frequency, tagging, wordclouds) Do Sentiment Analysis R has Packages to Help. These are just some of them: QDAP - Quantitative Discourse Package tm - text mining applications within R tidytext - Text Mining using ddplyr and ggplot and tidyverse tools SentimentAnalysis - For Sentiment Analysis However, consider that: Some of these are easier to use than others Some can be kind of a problem to install (e.g. qdap) They all offer similar capabilities We’ll look at tidytext 5.2 Simple Example Find the URL for Lincoln’s March 4, 1865 Speech: url &lt;- &quot;https://millercenter.org/the-presidency/presidential-speeches/march-4-1865-second-inaugural-address&quot; library(rvest) lincoln_doc &lt;- read_html(url) %&gt;% html_nodes(&quot;.view-transcript&quot;) %&gt;% html_text() lincoln_doc ## [1] &quot;TranscriptFellow-Countrymen: At this second appearing to take the oath of the Presidential office there is less occasion for an extended address than there was at the first. Then a statement somewhat in detail of a course to be pursued seemed fitting and proper. Now, at the expiration of four years, during which public declarations have been constantly called forth on every point and phase of the great contest which still absorbs the attention and engrosses the energies of the nation, little that is new could be presented. The progress of our arms, upon which all else chiefly depends, is as well known to the public as to myself, and it is, I trust, reasonably satisfactory and encouraging to all. With high hope for the future, no prediction in regard to it is ventured.On the occasion corresponding to this four years ago all thoughts were anxiously directed to an impending civil war. All dreaded it, all sought to avert it. While the inaugural address was being delivered from this place, devoted altogether to saving the Union without war, insurgent agents were in the city seeking to destroy it without war-seeking to dissolve the Union and divide effects by negotiation. Both parties deprecated war, but one of them would make war rather than let the nation survive, and the other would accept war rather than let it perish, and the war came.One-eighth of the whole population were colored slaves, not distributed generally over the Union. but localized in the southern part of it. These slaves constituted a peculiar and powerful interest. All knew that this interest was somehow the cause of the war. To strengthen, perpetuate, and extend this interest was the object for which the insurgents would rend the Union even by war, while the Government claimed no right to do more than to restrict the territorial enlargement of it. Neither party expected for the war the magnitude or the duration which it has already attained. Neither anticipated that the cause of the conflict might cease with or even before the conflict itself should cease. Each looked for an easier triumph, and a result less fundamental and astounding. Both read the same Bible and pray to the same God, and each invokes His aid against the other. It may seem strange that any men should dare to ask a just God&#39;s assistance in wringing their bread from the sweat of other men&#39;s faces, but let us judge not, that we be not judged. The prayers of both could not be answered. That of neither has been answered fully. The Almighty has His own purposes. \\&quot;Woe unto the world because of offenses; for it must needs be that offenses come, but woe to that man by whom the offense cometh.\\&quot; If we shall suppose that American slavery is one of those offenses which, in the providence of God, must needs come, but which, having continued through His appointed time, He now wills to remove, and that He gives to both North and South this terrible war as the woe due to those by whom the offense came, shall we discern therein any departure from those divine attributes which the believers in a living God always ascribe to Him? Fondly do we hope, fervently do we pray, that this mighty scourge of war may speedily pass away. Yet, if God wills that it continue until all the wealth piled by the bondsman&#39;s two hundred and fifty years of unrequited toil shall be sunk, and until every drop of blood drawn with the lash shall be paid by another drawn with the sword, as was said three thousand years ago, so still it must be said \\&quot;the judgments of the Lord are true and righteous altogether.\\&quot;With malice toward none, with charity for all, with firmness in the fight as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nation&#39;s wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.&quot; There are probably lots of words that don’t really “matter” or contribute to the “real” meaning of the speech. word_vec &lt;- unlist(strsplit(lincoln_doc,&quot; &quot;)) word_vec[1:20] ## [1] &quot;TranscriptFellow-Countrymen:&quot; &quot;&quot; ## [3] &quot;At&quot; &quot;this&quot; ## [5] &quot;second&quot; &quot;appearing&quot; ## [7] &quot;to&quot; &quot;take&quot; ## [9] &quot;the&quot; &quot;oath&quot; ## [11] &quot;of&quot; &quot;the&quot; ## [13] &quot;Presidential&quot; &quot;office&quot; ## [15] &quot;there&quot; &quot;is&quot; ## [17] &quot;less&quot; &quot;occasion&quot; ## [19] &quot;for&quot; &quot;an&quot; sort(table(word_vec),decreasing = TRUE)[1:10] ## word_vec ## the to and of that for be in it a ## 54 26 24 22 11 9 8 8 8 7 How do we remove all the uninteresting words ? We could do it manaully # Remove all punctuation marks word_vec &lt;- gsub(&quot;[[:punct:]]&quot;,&quot;&quot;,word_vec) stop_words &lt;- c(&quot;the&quot;,&quot;to&quot;,&quot;and&quot;,&quot;of&quot;,&quot;the&quot;,&quot;for&quot;,&quot;in&quot;,&quot;it&quot;, &quot;a&quot;,&quot;this&quot;,&quot;which&quot;,&quot;by&quot;,&quot;is&quot;,&quot;an&quot;,&quot;hqs&quot;,&quot;from&quot;, &quot;that&quot;,&quot;with&quot;,&quot;as&quot;) for (ii in 1:length(stop_words)) { for (jj in 1:length(word_vec)) { if (stop_words[ii] == word_vec[jj]) { word_vec[jj] &lt;- &quot;&quot; } } } word_vec &lt;- word_vec[word_vec != &quot;&quot;] sort(table(word_vec),decreasing = TRUE)[1:10] ## word_vec ## war all be we but God shall was do let ## 11 8 8 6 5 5 5 5 4 4 word_vec[1:30] ## [1] &quot;TranscriptFellowCountrymen&quot; &quot;At&quot; ## [3] &quot;second&quot; &quot;appearing&quot; ## [5] &quot;take&quot; &quot;oath&quot; ## [7] &quot;Presidential&quot; &quot;office&quot; ## [9] &quot;there&quot; &quot;less&quot; ## [11] &quot;occasion&quot; &quot;extended&quot; ## [13] &quot;address&quot; &quot;than&quot; ## [15] &quot;there&quot; &quot;was&quot; ## [17] &quot;at&quot; &quot;first&quot; ## [19] &quot;Then&quot; &quot;statement&quot; ## [21] &quot;somewhat&quot; &quot;detail&quot; ## [23] &quot;course&quot; &quot;be&quot; ## [25] &quot;pursued&quot; &quot;seemed&quot; ## [27] &quot;fitting&quot; &quot;proper&quot; ## [29] &quot;Now&quot; &quot;at&quot; 5.3 tidytext A better way would be to use the tidytext package. First we need to create a data frame out of the text. Then we “tokenize” the text which means we have one line per word. library(tidytext) text_df &lt;- data_frame(line = 1:length(lincoln_doc), text = lincoln_doc) token_text &lt;- text_df %&gt;% unnest_tokens(word, text) token_text %&gt;% count(word,sort=TRUE) ## # A tibble: 339 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 58 ## 2 to 27 ## 3 and 24 ## 4 of 22 ## 5 it 13 ## 6 that 12 ## 7 war 12 ## 8 all 10 ## 9 for 9 ## 10 in 9 ## # ... with 329 more rows But we need to get rid of the “stop words” # Now remove stop words data(stop_words) tidy_text &lt;- token_text %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; # This could also be done by the following. I point this out only because some people react # negatively to &quot;joins&quot; although fully understanding what joins are can only help you since # much of what the dplyr package does is based on SQL type joins. tidy_text &lt;- token_text %&gt;% filter(!word %in% stop_words$word) tidy_text %&gt;% count(word,sort=TRUE) ## # A tibble: 193 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 war 12 ## 2 god 5 ## 3 union 4 ## 4 offenses 3 ## 5 woe 3 ## 6 address 2 ## 7 ago 2 ## 8 altogether 2 ## 9 answered 2 ## 10 cease 2 ## # ... with 183 more rows tidy_text %&gt;% count(word,sort=TRUE) ## # A tibble: 193 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 war 12 ## 2 god 5 ## 3 union 4 ## 4 offenses 3 ## 5 woe 3 ## 6 address 2 ## 7 ago 2 ## 8 altogether 2 ## 9 answered 2 ## 10 cease 2 ## # ... with 183 more rows tidy_text %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 2) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() 5.4 Back To The PubMed Example We have around 935 abstracts that we can now mess with # Create a data frame out of the cleaned up abstracts text_df &lt;- data_frame(line = 1:length(my_abstracts), text = my_abstracts) token_text &lt;- text_df %&gt;% unnest_tokens(word, text) # Now remove stop words data(stop_words) tidy_text &lt;- token_text %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; # This could also be done by the following. I point this out only because some people react # negatively to &quot;joins&quot; although fully understanding what joins are can only help you since # much of what the dplyr package does is based on SQL type joins. tidy_text &lt;- token_text %&gt;% filter(!word %in% stop_words$word) # Arrange the text by descending word frequency tidy_text %&gt;% count(word, sort = TRUE) ## # A tibble: 6,465 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 dialysis 1335 ## 2 patients 1324 ## 3 home 1236 ## 4 hemodialysis 665 ## 5 hd 472 ## 6 hhd 406 ## 7 patient 380 ## 8 pd 307 ## 9 renal 277 ## 10 study 270 ## # ... with 6,455 more rows Some of the most frequently occurring words are in fact “dialysis”, “patients” so maybe we should consider them to be stop words also since we already know quite well that the overall theme is, well, dialysis and kidneys. There are also synonymns and abbreviations that are somewhat redundant such as “pdd”,“pd”,“hhd” so let’s eliminate them also. tidy_text &lt;- token_text %&gt;% filter(!word %in% c(stop_words$word,&quot;dialysis&quot;,&quot;patients&quot;,&quot;home&quot;, &quot;hemodialysis&quot;,&quot;haemodialysis&quot;,&quot;patient&quot;,&quot;hhd&quot;, &quot;pd&quot;,&quot;peritoneal&quot;,&quot;hd&quot;,&quot;renal&quot;,&quot;study&quot;,&quot;care&quot;, &quot;ci&quot;,&quot;chd&quot;,&quot;nhd&quot;,&quot;disease&quot;)) tidy_text %&gt;% count(word, sort = TRUE) ## # A tibble: 6,448 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 treatment 228 ## 2 therapy 191 ## 3 survival 188 ## 4 clinical 180 ## 5 center 178 ## 6 conventional 177 ## 7 nocturnal 177 ## 8 quality 176 ## 9 compared 172 ## 10 outcomes 165 ## # ... with 6,438 more rows Let’s do some plotting of these words tidy_text %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 120) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() Okay, it looks like there are numbers in there which might be useful. I suspect that the “95” is probably associated with the idea of a confidence interval. But there are other references to numbers. grep(&quot;^[0-9]{1,3}$&quot;,tidy_text$word)[1:20] ## [1] 206 263 271 272 274 336 350 354 359 403 404 412 413 449 516 526 533 ## [18] 548 555 572 tidy_text_nonum &lt;- tidy_text[grep(&quot;^[0-9]{1,3}$&quot;,tidy_text$word,invert=TRUE),] Okay well I think maybe we have some reasonable data to examine. As you might have realized by now, manipulating data to get it “clean” can be tedious and frustrating though it is an inevitable part of the process. tidy_text_nonum %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 120) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() nrc_joy &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;joy&quot;) bing_word_counts &lt;- tidy_text_nonum %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word,sentiment,sort=TRUE) ## Joining, by = &quot;word&quot; t the positive vs negative words bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() ## Selecting by n Let’s create a word cloud library(wordcloud) # tidy_text_nonum %&gt;% count(word) %&gt;% with(wordcloud(word,n,max.words=100,colors=brewer.pal(8,&quot;Dark2&quot;))) 5.5 BiGrams Let’s look at bigrams. We need to go back to the cleaned abstracts and pair words to get phrase that might be suggestive of some sentiment text_df &lt;- data_frame(line = 1:length(my_abstracts), text = my_abstracts) dialysis_bigrams &lt;- text_df %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) dialysis_bigrams %&gt;% count(bigram, sort = TRUE) ## # A tibble: 41,630 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 in the 382 ## 2 of the 316 ## 3 home dialysis 280 ## 4 home hemodialysis 267 ## 5 peritoneal dialysis 198 ## 6 of home 189 ## 7 associated with 172 ## 8 home hd 153 ## 9 home haemodialysis 138 ## 10 in center 138 ## # ... with 41,620 more rows But we have to filter out stop words bigrams_sep &lt;- dialysis_bigrams %&gt;% separate(bigram,c(&quot;word1&quot;,&quot;word2&quot;),sep=&quot; &quot;) stop_list &lt;- c(stop_words$word,&quot;dialysis&quot;,&quot;patients&quot;,&quot;home&quot;,&quot;kidney&quot;, &quot;hemodialysis&quot;,&quot;haemodialysis&quot;,&quot;patient&quot;,&quot;hhd&quot;, &quot;pd&quot;,&quot;peritoneal&quot;,&quot;hd&quot;,&quot;renal&quot;,&quot;study&quot;,&quot;care&quot;, &quot;ci&quot;,&quot;chd&quot;,&quot;nhd&quot;,&quot;esrd&quot;,&quot;lt&quot;,&quot;95&quot;,&quot;0.001&quot;) bigrams_filtered &lt;- bigrams_sep %&gt;% filter(!word1 %in% stop_list) %&gt;% filter(!word2 %in% stop_list) bigram_counts &lt;- bigrams_filtered %&gt;% count(word1, word2, sort = TRUE) bigrams_united &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) bigrams_united %&gt;% count(bigram, sort = TRUE) %&gt;% print(n=25) ## # A tibble: 11,825 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 replacement therapy 69 ## 2 vascular access 62 ## 3 technique failure 54 ## 4 left ventricular 42 ## 5 confidence interval 40 ## 6 blood pressure 39 ## 7 clinical outcomes 36 ## 8 thrice weekly 32 ## 9 short daily 31 ## 10 technique survival 29 ## 11 quality improvement 26 ## 12 hazard ratio 25 ## 13 adverse events 22 ## 14 6 months 20 ## 15 access related 20 ## 16 ventricular mass 19 ## 17 12 months 18 ## 18 arteriovenous fistula 17 ## 19 3 times 15 ## 20 buttonhole cannulation 15 ## 21 daily life 15 ## 22 health related 15 ## 23 observational studies 15 ## 24 cost effective 14 ## 25 cost effectiveness 14 ## # ... with 1.18e+04 more rows bigram_counts %&gt;% filter(n &gt; 30) %&gt;% ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) + geom_tile(alpha = 0.8, color = &quot;white&quot;) + scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) + coord_flip() + theme_tq() + theme(legend.position = &quot;right&quot;) + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + labs(x = &quot;first word in pair&quot;, y = &quot;second word in pair&quot;) "],
["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
