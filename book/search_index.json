[["index.html", "Web Scraping with R Chapter 1 Motivations 1.1 Lots of Data For The Taking ? 1.2 Web Scraping Can Be Ugly 1.3 Understanding The Language of The Web 1.4 Useful Packages 1.5 Quick rvest tutorial 1.6 Example: Parsing A Table From Wikipedia 1.7 Scraping Patient Dialysis Stories 1.8 Summary", " Web Scraping with R Steve Pittard 2022-02-08 Chapter 1 Motivations 1.1 Lots of Data For The Taking ? The web hosts lots of interesting data that you can ”scrape”. Some of it is stashed in data bases, behind APIs, or in free form text. Lots of people want to grab information of of Twitter or from user forums to see what people are thinking. There is a lot of valuable information out there for the taking although some web sites have “caught on” and either block programmatic access or they setup “pay walls” that require you to subscribe to an API for access. The New York Times does this. But there are lots of opportunities to get data. tables Fetch tables like from Wikipedia forms You can submit forms and fetch the results css You can access parts of a web site using style or css selectors Tweets Process tweets including emojis Web Sites User forums have lots of content Instagram Yes you can “scrape” photos also 1.2 Web Scraping Can Be Ugly Depending on what web sites you want to scrape the process can be involved and quite tedious. Many websites are very much aware that people are scraping so they offer Application Programming Interfaces (APIs) to make requests for information easier for the user and easier for the server administrators to control access. Most times the user must apply for a “key” to gain access. For premium sites, the key costs money. Some sites like Google and Wunderground (a popular weather site) allow some number of free accesses before they start charging you. Even so the results are typically returned in XML or JSON which then requires you to parse the result to get the information you want. In the best situation there is an R package that will wrap in the parsing and will return lists or data frames. Here is a summary: First. Always try to find an R package that will access a site (e.g. New York Times, Wunderground, PubMed). These packages (e.g. omdbapi, easyPubMed, RBitCoin, rtimes) provide a programmatic search interface and return data frames with little to no effort on your part. If no package exists then hopefully there is an API that allows you to query the website and get results back in JSON or XML. I prefer JSON because it’s “easier” and the packages for parsing JSON return lists which are native data structures to R. So you can easily turn results into data frames. You will ususally use the rvest package in conjunction with XML, and the RSJONIO packages. If the Web site doesn’t have an API then you will need to scrape text. This isn’t hard but it is tedious. You will need to use rvest to parse HMTL elements. If you want to parse mutliple pages then you will need to use rvest to move to the other pages and possibly fill out forms. If there is a lot of Javascript then you might need to use RSelenium to programmatically manage the web page. 1.3 Understanding The Language of The Web The Web has its own languages: HTML, CSS, Javascript &lt;h1&gt;, &lt;h2&gt;, ..., &lt;h6&gt; Heading 1 and so on &lt;p&gt; Paragraph elements &lt;ul&gt; Unordered List &lt;ol&gt; Ordered List &lt;li&gt; List Element &lt;div&gt; Division / Section &lt;table&gt; Tables &lt;form&gt; Web forms So to be productive at scraping requires you to have some familiarity with HMTL XML, and CSS. Here we look at a very basic HTML file. Refer to See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html for a basic introductory session on HTML and webscraping with R &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;h1&gt;My First Heading&lt;/h1&gt; &lt;p&gt;My first paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;   And you could apply some styling to this courtest of the CSS language which allows you to inject styles into plain HTML:   1.3.1 Useful tools There are a number of tools that allow us to inspect web pages and see “what is under the hood.” Warning - I just discovered that one of my favorite browser plugins (firebug) to find the xpaths and/or css paths of page elements is no longer supported under Firefox or Chrome. I’ve found a couple of replacements but they don’t work as well. I’ll research it more. The way that Selector Gadget and xPath work is that you install them into your browswer and then activate them whenever you need to identify the selector associated with a part of a web page.   Selector Gadget http://selectorgadget.com/ Firebug https://getfirebug.com/ (now integrated into a version of Firefox) xPath https://addons.mozilla.org/en-US/firefox/addon/xpath_finder/ Google Chrome Right click to inspect a page element Google Chrome View Developer - Developer Tools Oxygen Editor Can obtain via the Emory Software Express Site 1.4 Useful Packages You will use the following three primary packages to help you get data from various web pages: rvest, XML, and RJSONIO. Note that you won’t always use them simultaneously but you might use them in pairs or individually depending on the task at hand. 1.5 Quick rvest tutorial Now let’s do a quick rvest tutorial. There are several steps involved in using rvest which are conceptually quite straightforward: Identify a URL to be examined for content Use Selector Gadet, xPath, or Google Insepct to identify the “selector” This will be a paragraph, table, hyper links, images Load rvest Use read_html to “read” the URL Pass the result to html_nodes to get the selectors identified in step number 2 Get the text or table content library(rvest) url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; (paragraphs &lt;- read_html(url) %&gt;% html_nodes(&quot;p&quot;)) ## {xml_nodeset (51)} ## [1] &lt;p class=&quot;mw-empty-elt&quot;&gt;\\n\\n&lt;/p&gt; ## [2] &lt;p&gt;In &lt;a href=&quot;/wiki/Demography&quot; title=&quot;Demography&quot;&gt;demographics&lt;/a&gt;, the &lt;b&gt;world popula ... ## [3] &lt;p&gt;The world population has experienced &lt;a href=&quot;/wiki/Population_growth&quot; title=&quot;Populati ... ## [4] &lt;p&gt;&lt;a href=&quot;/wiki/Birth_rate&quot; title=&quot;Birth rate&quot;&gt;Birth rates&lt;/a&gt; were highest in the late ... ## [5] &lt;p&gt;Six of the Earth&#39;s seven &lt;a href=&quot;/wiki/Continent&quot; title=&quot;Continent&quot;&gt;continents&lt;/a&gt; ar ... ## [6] &lt;p&gt;Estimates of world population by their nature are an aspect of &lt;a href=&quot;/wiki/Modernit ... ## [7] &lt;p&gt;It is difficult for estimates to be better than rough approximations, as even modern p ... ## [8] &lt;p&gt;Estimates of the population of the world at the time agriculture emerged in around 10, ... ## [9] &lt;p&gt;The &lt;a href=&quot;/wiki/Plague_of_Justinian&quot; title=&quot;Plague of Justinian&quot;&gt;Plague of Justinia ... ## [10] &lt;p&gt;Starting in AD 2, the &lt;a href=&quot;/wiki/Han_Dynasty&quot; class=&quot;mw-redirect&quot; title=&quot;Han Dynas ... ## [11] &lt;p&gt;The &lt;a href=&quot;/wiki/Pre-Columbian_era&quot; title=&quot;Pre-Columbian era&quot;&gt;pre-Columbian&lt;/a&gt; popu ... ## [12] &lt;p&gt;During the European &lt;a href=&quot;/wiki/British_Agricultural_Revolution&quot; title=&quot;British Agr ... ## [13] &lt;p&gt;Population growth in the West became more rapid after the introduction of &lt;a href=&quot;/wi ... ## [14] &lt;p&gt;The first half of the 20th century in &lt;a href=&quot;/wiki/Russian_Empire&quot; title=&quot;Russian Em ... ## [15] &lt;p&gt;Many countries in the &lt;a href=&quot;/wiki/Developing_world&quot; class=&quot;mw-redirect&quot; title=&quot;Deve ... ## [16] &lt;p&gt;It is estimated that the world population reached one billion for the first time in 18 ... ## [17] &lt;p&gt;According to current projections, the global population will reach eight billion by 20 ... ## [18] &lt;p&gt;There is no estimation for the exact day or month the world&#39;s population surpassed one ... ## [19] &lt;p&gt;As of 2012, the global &lt;a href=&quot;/wiki/Human_sex_ratio&quot; title=&quot;Human sex ratio&quot;&gt;sex rat ... ## [20] &lt;p&gt;According to the &lt;a href=&quot;/wiki/World_Health_Organization&quot; title=&quot;World Health Organiz ... ## ... Then we might want to actually parse out those paragraphs into text: url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; paragraphs &lt;- read_html(url) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() paragraphs[1:10] ## [1] &quot;\\n\\n&quot; ## [2] &quot;In demographics, the world population is the total number of humans currently living, and was estimated to have exceeded 7.9 billion people as of November 2021[update].[2] It took over 2 million years of human prehistory and history for the world&#39;s population to reach 1 billion[3] and only 200 years more to grow to 7 billion.[4]&quot; ## [3] &quot;The world population has experienced continuous growth following the Great Famine of 1315–1317 and the end of the Black Death in 1350, when it was near 370 million.[5]\\nThe highest global population growth rates, with increases of over 1.8% per year, occurred between 1955 and 1975 – peaking at 2.1% between 1965 and 1970.[6] The growth rate declined to 1.2% between 2010 and 2015 and is projected to decline further in the course of the 21st century.[6] The global population is still increasing, but there is significant uncertainty about its long-term trajectory due to changing rates of fertility and mortality.[7] The UN Department of Economics and Social Affairs projects between 9–10 billion people by 2050, and gives an 80% confidence interval of 10–12 billion by the end of the 21st century.[8] Other demographers predict that world population will begin to decline in the second half of the 21st century.[9]&quot; ## [4] &quot;Birth rates were highest in the late 1980s at about 139 million,[11] and as of 2011 were expected to remain essentially constant at a level of 135 million,[12] while the mortality rate numbered 56 million per year and were expected to increase to 80 million per year by 2040.[13]\\nThe median age of human beings as of 2020 is 31 years.[14]&quot; ## [5] &quot;Six of the Earth&#39;s seven continents are permanently inhabited on a large scale. Asia is the most populous continent, with its 4.64 billion inhabitants accounting for 60% of the world population. The world&#39;s two most populated countries, China and India, together constitute about 36% of the world&#39;s population. Africa is the second most populated continent, with around 1.34 billion people, or 17% of the world&#39;s population. Europe&#39;s 747 million people make up 10% of the world&#39;s population as of 2020, while the Latin American and Caribbean regions are home to around 653 million (8%). Northern America, primarily consisting of the United States and Canada, has a population of around 368 million (5%), and Oceania, the least populated region, has about 42 million inhabitants (0.5%).[16]Antarctica only has a very small, fluctuating population of about 1200 people based mainly in polar science stations.[17]&quot; ## [6] &quot;Estimates of world population by their nature are an aspect of modernity, possible only since the Age of Discovery. Early estimates for the population of the world[18] date to the 17th century: William Petty in 1682 estimated world population at 320 million (modern estimates ranging close to twice this number); by the late 18th century, estimates ranged close to one billion (consistent with modern estimates).[19] More refined estimates, broken down by continents, were published in the first half of the 19th century, at 600 million to 1 billion in the early 1800s and at 800 million to 1 billion in the 1840s.[20]&quot; ## [7] &quot;It is difficult for estimates to be better than rough approximations, as even modern population estimates are fraught with uncertainties on the order of 3% to 5%.[21]&quot; ## [8] &quot;Estimates of the population of the world at the time agriculture emerged in around 10,000 BC have ranged between 1 million and 15 million.[22][23] Even earlier, genetic evidence suggests humans may have gone through a population bottleneck of between 1,000 and 10,000 people about 70,000 BC, according to the Toba catastrophe theory. By contrast, it is estimated that around 50–60 million people lived in the combined eastern and western Roman Empire in the 4th century AD.[24]&quot; ## [9] &quot;The Plague of Justinian, which first emerged during the reign of the Roman emperor Justinian, caused Europe&#39;s population to drop by around 50% between the 6th and 8th centuries AD.[25] The population of Europe was more than 70 million in 1340.[26] The Black Death pandemic of the 14th century may have reduced the world&#39;s population from an estimated 450 million in 1340 to between 350 and 375 million in 1400;[27] it took 200 years for population figures to recover.[28] The population of China decreased from 123 million in 1200 to 65 million in 1393,[29] presumably from a combination of Mongol invasions, famine, and plague.[30]&quot; ## [10] &quot;Starting in AD 2, the Han Dynasty of ancient China kept consistent family registers in order to properly assess the poll taxes and labor service duties of each household.[31] In that year, the population of Western Han was recorded as 57,671,400 individuals in 12,366,470 households, decreasing to 47,566,772 individuals in 9,348,227 households by AD 146, towards the End of the Han Dynasty.[31] At the founding of the Ming Dynasty in 1368, China&#39;s population was reported to be close to 60 million; toward the end of the dynasty in 1644, it may have approached 150 million.[32] England&#39;s population reached an estimated 5.6 million in 1650, up from an estimated 2.6 million in 1500.[33] New crops that were brought to Asia and Europe from the Americas by Portuguese and Spanish colonists in the 16th century are believed to have contributed to population growth.[34][35][36] Since their introduction to Africa by Portuguese traders in the 16th century,[37]maize and cassava have similarly replaced traditional African crops as the most important staple food crops grown on the continent.[38]&quot; Get some other types of HTML obejects. Let’s get all the hyperlinks to other pages read_html(url) %&gt;% html_nodes(&quot;a&quot;) ## {xml_nodeset (1647)} ## [1] &lt;a id=&quot;top&quot;&gt;&lt;/a&gt; ## [2] &lt;a href=&quot;/wiki/Wikipedia:Protection_policy#semi&quot; title=&quot;This article is semi-protected.&quot;&gt; ... ## [3] &lt;a class=&quot;mw-jump-link&quot; href=&quot;#mw-head&quot;&gt;Jump to navigation&lt;/a&gt; ## [4] &lt;a class=&quot;mw-jump-link&quot; href=&quot;#searchInput&quot;&gt;Jump to search&lt;/a&gt; ## [5] &lt;a href=&quot;/wiki/Demographics_of_the_world&quot; title=&quot;Demographics of the world&quot;&gt;Demographics ... ## [6] &lt;a href=&quot;/wiki/File:World_Population_Prospects_2019.png&quot; class=&quot;image&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/ ... ## [7] &lt;a href=&quot;/wiki/File:World_Population_Prospects_2019.png&quot; class=&quot;internal&quot; title=&quot;Enlarge&quot; ... ## [8] &lt;a href=&quot;#cite_note-1&quot;&gt;[1]&lt;/a&gt; ## [9] &lt;a href=&quot;/wiki/Demography&quot; title=&quot;Demography&quot;&gt;demographics&lt;/a&gt; ## [10] &lt;a href=&quot;/wiki/Human&quot; title=&quot;Human&quot;&gt;humans&lt;/a&gt; ## [11] &lt;a class=&quot;external text&quot; href=&quot;https://en.wikipedia.org/w/index.php?title=World_populatio ... ## [12] &lt;a href=&quot;#cite_note-2&quot;&gt;[2]&lt;/a&gt; ## [13] &lt;a href=&quot;/wiki/Prehistory&quot; title=&quot;Prehistory&quot;&gt;human prehistory&lt;/a&gt; ## [14] &lt;a href=&quot;/wiki/Human_history&quot; title=&quot;Human history&quot;&gt;history&lt;/a&gt; ## [15] &lt;a href=&quot;/wiki/Billion&quot; title=&quot;Billion&quot;&gt;billion&lt;/a&gt; ## [16] &lt;a href=&quot;#cite_note-3&quot;&gt;[3]&lt;/a&gt; ## [17] &lt;a href=&quot;#cite_note-4&quot;&gt;[4]&lt;/a&gt; ## [18] &lt;a href=&quot;/wiki/Population_growth&quot; title=&quot;Population growth&quot;&gt;continuous growth&lt;/a&gt; ## [19] &lt;a href=&quot;/wiki/Great_Famine_of_1315%E2%80%931317&quot; title=&quot;Great Famine of 1315–1317&quot;&gt;Great ... ## [20] &lt;a href=&quot;/wiki/Black_Death&quot; title=&quot;Black Death&quot;&gt;Black Death&lt;/a&gt; ## ... What about tables ? url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; tables &lt;- read_html(url) %&gt;% html_nodes(&quot;table&quot;) tables ## {xml_nodeset (26)} ## [1] &lt;table class=&quot;infobox&quot; style=&quot;float: right; font-size:90%&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;&lt;th colspan=&quot;5&quot; s ... ## [2] &lt;table class=&quot;wikitable sortable&quot;&gt;\\n&lt;caption&gt;Population by region (2020 estimates)\\n&lt;/cap ... ## [3] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center; float:right; clear:right; margin-left: ... ## [4] &lt;table width=&quot;100%&quot;&gt;&lt;tbody&gt;&lt;tr&gt;\\n&lt;td valign=&quot;top&quot;&gt; &lt;style data-mw-deduplicate=&quot;TemplateSt ... ## [5] &lt;table class=&quot;wikitable sortable plainrowheaders&quot; style=&quot;text-align:right&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\ ... ## [6] &lt;table class=&quot;wikitable sortable&quot; style=&quot;text-align:right&quot;&gt;\\n&lt;caption&gt;10 most densely pop ... ## [7] &lt;table class=&quot;wikitable sortable&quot; style=&quot;text-align:right&quot;&gt;\\n&lt;caption&gt;Countries ranking h ... ## [8] &lt;table class=&quot;wikitable sortable&quot;&gt;\\n&lt;caption&gt;Global annual population growth&lt;sup id=&quot;cite ... ## [9] &lt;table class=&quot;wikitable sortable&quot; style=&quot;font-size:97%; text-align:right;&quot;&gt;\\n&lt;caption&gt;Wor ... ## [10] &lt;table class=&quot;wikitable sortable&quot; style=&quot;font-size:97%; text-align:right;&quot;&gt;\\n&lt;caption&gt;Wor ... ## [11] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:right;&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Year\\n&lt;/th&gt;\\n&lt;th st ... ## [12] &lt;table class=&quot;box-More_citations_needed_section plainlinks metadata ambox ambox-content a ... ## [13] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center; margin-top:0.5em; margin-right:1em; fl ... ## [14] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:right; margin-top:2.6em; font-size:96%;&quot;&gt;\\n&lt;ca ... ## [15] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center&quot;&gt;\\n&lt;caption&gt;Starting at 500 million\\n&lt;/ ... ## [16] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center&quot;&gt;\\n&lt;caption&gt;Starting at 375 million\\n&lt;/ ... ## [17] &lt;table role=&quot;presentation&quot; class=&quot;mbox-small plainlinks sistersitebox&quot; style=&quot;background- ... ## [18] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; style=&quot;border-spacing ... ## [19] &lt;table class=&quot;nowraplinks mw-collapsible mw-collapsed navbox-inner&quot; style=&quot;border-spacing ... ## [20] &lt;table class=&quot;nowraplinks hlist mw-collapsible autocollapse navbox-inner&quot; style=&quot;border-s ... ## ... 1.6 Example: Parsing A Table From Wikipedia Look at the Wikipedia Page for world population: https://en.wikipedia.org/wiki/World_population We can get any table we want using rvest We might have to experiment to figure out which one Get the one that lists the ten most populous countries I think this might be the 4th or 5th table on the page How do we get this ? First we will load packages that will help us throughout this session. In this case we’ll need to figure out what number table it is we want. We could fetch all the tables and then experiment to find the precise one. library(rvest) library(tidyr) library(dplyr) library(ggplot2) # Use read_html to fetch the webpage url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_df &lt;- read_html(url) ten_most_populous &lt;- ten_most_df %&gt;% html_nodes(&quot;table&quot;) %&gt;% `[[`(6) %&gt;% html_table() # Let&#39;s get just the first three columns ten_most_populous &lt;- ten_most_populous[,2:4] # Get some content - Change the column names names(ten_most_populous) &lt;- c(&quot;Country_Territory&quot;,&quot;Population&quot;,&quot;Date&quot;) # Do reformatting on the columns to be actual numerics where appropriate ten_most_populous %&gt;% mutate(Population=gsub(&quot;,&quot;,&quot;&quot;,Population)) %&gt;% mutate(Population=round(as.numeric(Population)/1e+06)) %&gt;% ggplot(aes(x=Country_Territory,y=Population)) + geom_point() + labs(y = &quot;Population / 1,000,000&quot;) + coord_flip() + ggtitle(&quot;Top 10 Most Populous Countries&quot;) In the above example we leveraged the fact that we were looking specifically for a table element and it became a project to locate the correct table number. This isn’t always the case with more complicated websites in that the element we are trying to grab or scrape is contained within a nested structure that doesn’t correspond neatly to a paragraph, link, heading, or table. This can be the case if the page is heavily styled with CSS or Javascript. We might have to work harder. But it’s okay to try to use simple elements and then try to refine the search some more. # Could have use the xPath plugin to help url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_df &lt;- read_html(url) ten_most_populous &lt;- ten_most_df %&gt;% html_nodes(xpath=&quot;/html/body/div[3]/div[3]/div[4]/div/table[5]&quot;) %&gt;% html_table() 1.7 Scraping Patient Dialysis Stories Here is an example relating to the experiences of dialysis patients with a specific dialysis provider. It might be more useful to find a support forum that is managed by dialysis patients to get more general opinions but this example is helpful in showing you what is involved. Check out this website: https://www.americanrenal.com/dialysis-centers/patient-stories 1.7.1 Getting More Detail In looking at this page you will see that there are a number of patient stories. Actually, there is a summary line followed by a “Read More” link that provides more detail on the patient experience. Our goal is to get the full content as opposed to only the summary. How would we do this ? 1.7.2 Writing Some Code Let’s use our new found knowledge of rvest to help us get these detailed stories. Maybe we want to do some sentiment analysis on this. If you hover over the Read More link on the website it will provide a specific link for each patient. For example, https://www.americanrenal.com/dialysis-centers/patient-stories/john-baguchinsky What we want to do is first get a list of all these links from the main page after which we can loop over each of the patient specific links and capture that information into a vector. Each element of the vector will be the content of a specific patient’s story. library(rvest) burl &lt;- &quot;https://www.americanrenal.com/dialysis-centers/patient-stories&quot; # Setup an empty vector to which we will add the content of each story workVector &lt;- vector() # Grab the links from the site that relate patient stories links &lt;- read_html(burl) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% grep(&quot;stories&quot;,.,value=TRUE) links ## [1] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories&quot; ## [2] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/randal-beatty&quot; ## [3] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/patricia-garcia&quot; ## [4] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/john-baguchinsky&quot; ## [5] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/sheryll-wyman&quot; ## [6] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/carol-sykes&quot; ## [7] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/sharon-cauthen&quot; ## [8] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/remond-ellis&quot; ## [9] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories&quot; ## [10] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories&quot; Some of these links do not correspond directly to a specific patient name so we need to filter those out. # Get only the ones that seem to have actual names associated with them storiesLinks &lt;- links[-grep(&quot;stories$&quot;,links)] storiesLinks ## [1] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/randal-beatty&quot; ## [2] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/patricia-garcia&quot; ## [3] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/john-baguchinsky&quot; ## [4] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/sheryll-wyman&quot; ## [5] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/carol-sykes&quot; ## [6] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/sharon-cauthen&quot; ## [7] &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/remond-ellis&quot; Next we will visit each of these pages and scrape the text information. We’ll step through this in class so you can see this in action but here is the code. We will get each story and place each paragrpah of the story into a vector element. After that we will eliminate blank lines and some junk lines that begin with a new line character. Then we will collapse all of the vector text into a single paragraph and store it into a list element. Let’s step through it for the first link. # This corresponds to the first link # &quot;http://www.americanrenal.com/dialysis-centers/patient-stories/randal-beatty&quot; tmpResult &lt;- read_html(storiesLinks[1]) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() tmpResult ## [1] &quot;Mr. Randal Beatty, University Kidney Center Hikes Lane&quot; ## [2] &quot;In April 2010, Randal Beatty was diagnosed with end stage renal disease (ESRD). The diagnosis came as a surprise, and, Mr. Beatty admits, he kept praying for a miracle.&quot; ## [3] &quot;“I heard all those stories about how people feel during dialysis and I didn’t want to deal with it,” said Mr. Beatty. “I didn’t want to lose my freedom and I certainly didn’t want to feel sick all the time.”&quot; ## [4] &quot;So for three years, he waited, hoping to find his miracle. But by August 2013, he knew he waited too long.&quot; ## [5] &quot;“Before my first dialysis treatment, I could feel myself pulling away from everyone, especially my family. I didn’t want to go anywhere or do anything and I was sick all the time. I realized at that point that how I was feeling was exactly what I wanted to avoid.”&quot; ## [6] &quot;He began his in-center dialysis treatments with American Renal Associates (ARA) at University Kidney Center in Louisville, Kentucky in August 2013 and transferred to another local ARA facility – University Kidney Center Hikes Lane – in May 2014 since it was closer to his home.&quot; ## [7] &quot;“In a short time, dialysis completely changed me,” he said. His health improved, along with his confidence, encouraging him to start driving himself to and from treatments. Not only did this give him a renewed feeling of independence, but a strong sense of accomplishment, as well.&quot; ## [8] &quot;Now 67 years old, Mr. Beatty says he can do everything he did before, including keeping up with his two granddaughters, playing basketball, among other hobbies, and going on family vacations. In fact, with the help of ARA’s Travel Department, Mr. Beatty can travel stress free. Though he admits traveling while on dialysis can be intimidating, he explained, “All I had to do was show up. ARA’s Travel Team took care of everything.”&quot; ## [9] &quot;Receiving a diagnosis of ESRD can be challenging, but Mr. Beatty’s advice is to take a step back and see dialysis as the miracle it is.&quot; ## [10] &quot;“It took me three years to realize that dialysis was the miracle I was waiting for. I was an extremely sick individual and just a few months on dialysis completely changed me. I don’t know why I waited as long as I did. I could have been enjoying life for the last few years rather than staying home sick. And I honestly haven’t been sick since I started my dialysis treatments!”&quot; ## [11] &quot;Read more patient stories&quot; ## [12] &quot;American Renal Associates operates 240 dialysis clinics in 27 states and Washington D.C., serving more than 17,300 patients with end-stage renal disease in partnership with approximately 400 local nephrologists.&quot; ## [13] &quot;If you have questions, you can call 1-877-99-RENAL (1-877-997-3625) or patients@americanrenal.com&quot; ## [14] &quot;&quot; ## [15] &quot;\\n ©2022 American Renal® Associates. All Rights Reserved.500 Cummings Center, Suite\\n 6550, Beverly, MA, 01915\\n &quot; Okay, that has some junk in it like blank lines and lines that begin with new line characters. # Get rid of elements that are a blank line tmpResult &lt;- tmpResult[tmpResult!=&quot;&quot;] # Get rid of elements that begin with a newline character &quot;\\n&quot; newlines_begin &lt;- sum(grepl(&quot;^\\n&quot;,tmpResult)) if (newlines_begin &gt; 0) { tmpResult &lt;- tmpResult[-grep(&quot;^\\n&quot;,tmpResult)] } tmpResult ## [1] &quot;Mr. Randal Beatty, University Kidney Center Hikes Lane&quot; ## [2] &quot;In April 2010, Randal Beatty was diagnosed with end stage renal disease (ESRD). The diagnosis came as a surprise, and, Mr. Beatty admits, he kept praying for a miracle.&quot; ## [3] &quot;“I heard all those stories about how people feel during dialysis and I didn’t want to deal with it,” said Mr. Beatty. “I didn’t want to lose my freedom and I certainly didn’t want to feel sick all the time.”&quot; ## [4] &quot;So for three years, he waited, hoping to find his miracle. But by August 2013, he knew he waited too long.&quot; ## [5] &quot;“Before my first dialysis treatment, I could feel myself pulling away from everyone, especially my family. I didn’t want to go anywhere or do anything and I was sick all the time. I realized at that point that how I was feeling was exactly what I wanted to avoid.”&quot; ## [6] &quot;He began his in-center dialysis treatments with American Renal Associates (ARA) at University Kidney Center in Louisville, Kentucky in August 2013 and transferred to another local ARA facility – University Kidney Center Hikes Lane – in May 2014 since it was closer to his home.&quot; ## [7] &quot;“In a short time, dialysis completely changed me,” he said. His health improved, along with his confidence, encouraging him to start driving himself to and from treatments. Not only did this give him a renewed feeling of independence, but a strong sense of accomplishment, as well.&quot; ## [8] &quot;Now 67 years old, Mr. Beatty says he can do everything he did before, including keeping up with his two granddaughters, playing basketball, among other hobbies, and going on family vacations. In fact, with the help of ARA’s Travel Department, Mr. Beatty can travel stress free. Though he admits traveling while on dialysis can be intimidating, he explained, “All I had to do was show up. ARA’s Travel Team took care of everything.”&quot; ## [9] &quot;Receiving a diagnosis of ESRD can be challenging, but Mr. Beatty’s advice is to take a step back and see dialysis as the miracle it is.&quot; ## [10] &quot;“It took me three years to realize that dialysis was the miracle I was waiting for. I was an extremely sick individual and just a few months on dialysis completely changed me. I don’t know why I waited as long as I did. I could have been enjoying life for the last few years rather than staying home sick. And I honestly haven’t been sick since I started my dialysis treatments!”&quot; ## [11] &quot;Read more patient stories&quot; ## [12] &quot;American Renal Associates operates 240 dialysis clinics in 27 states and Washington D.C., serving more than 17,300 patients with end-stage renal disease in partnership with approximately 400 local nephrologists.&quot; ## [13] &quot;If you have questions, you can call 1-877-99-RENAL (1-877-997-3625) or patients@americanrenal.com&quot; Next, let’s create a more compact version of the data. We’ll cram it all into a single element. (tmpResult &lt;- paste(tmpResult,collapse=&quot;&quot;)) ## [1] &quot;Mr. Randal Beatty, University Kidney Center Hikes LaneIn April 2010, Randal Beatty was diagnosed with end stage renal disease (ESRD). The diagnosis came as a surprise, and, Mr. Beatty admits, he kept praying for a miracle.“I heard all those stories about how people feel during dialysis and I didn’t want to deal with it,” said Mr. Beatty. “I didn’t want to lose my freedom and I certainly didn’t want to feel sick all the time.”So for three years, he waited, hoping to find his miracle. But by August 2013, he knew he waited too long.“Before my first dialysis treatment, I could feel myself pulling away from everyone, especially my family. I didn’t want to go anywhere or do anything and I was sick all the time. I realized at that point that how I was feeling was exactly what I wanted to avoid.”He began his in-center dialysis treatments with American Renal Associates (ARA) at University Kidney Center in Louisville, Kentucky in August 2013 and transferred to another local ARA facility – University Kidney Center Hikes Lane – in May 2014 since it was closer to his home.“In a short time, dialysis completely changed me,” he said. His health improved, along with his confidence, encouraging him to start driving himself to and from treatments. Not only did this give him a renewed feeling of independence, but a strong sense of accomplishment, as well.Now 67 years old, Mr. Beatty says he can do everything he did before, including keeping up with his two granddaughters, playing basketball, among other hobbies, and going on family vacations. In fact, with the help of ARA’s Travel Department, Mr. Beatty can travel stress free. Though he admits traveling while on dialysis can be intimidating, he explained, “All I had to do was show up. ARA’s Travel Team took care of everything.”Receiving a diagnosis of ESRD can be challenging, but Mr. Beatty’s advice is to take a step back and see dialysis as the miracle it is.“It took me three years to realize that dialysis was the miracle I was waiting for. I was an extremely sick individual and just a few months on dialysis completely changed me. I don’t know why I waited as long as I did. I could have been enjoying life for the last few years rather than staying home sick. And I honestly haven’t been sick since I started my dialysis treatments!”Read more patient storiesAmerican Renal Associates operates 240 dialysis clinics in 27 states and Washington D.C., serving more than 17,300 patients with end-stage renal disease in partnership with approximately 400 local nephrologists.If you have questions, you can call 1-877-99-RENAL (1-877-997-3625) or patients@americanrenal.com&quot; So we could put this logic into a loop and process each of the links programmatically. # Now go to these pages and scrape the text necessary to # build a corpus tmpResult &lt;- vector() textList &lt;- list() for (ii in 1:length(storiesLinks)) { tmpResult &lt;- read_html(storiesLinks[ii]) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() # Get rid of elements that are a blank line tmpResult &lt;- tmpResult[tmpResult!=&quot;&quot;] # Get rid of elements that begin with a newline character &quot;\\n&quot; newlines_begin &lt;- sum(grepl(&quot;^\\n&quot;,tmpResult)) if (newlines_begin &gt; 0) { tmpResult &lt;- tmpResult[-grep(&quot;^\\n&quot;,tmpResult)] } # Let&#39;s collpase all the elements into a single element and then store # it into a list element so we can maintain each patient story separately # This is not necessary but until we figure out what we want to do with # the data then this gives us some options tmpResult &lt;- paste(tmpResult,collapse=&quot;&quot;) textList[[ii]] &lt;- tmpResult } If we did our job correctly then each element of the textList will have text in it corresponding to each patient textList[[1]] ## [1] &quot;Mr. Randal Beatty, University Kidney Center Hikes LaneIn April 2010, Randal Beatty was diagnosed with end stage renal disease (ESRD). The diagnosis came as a surprise, and, Mr. Beatty admits, he kept praying for a miracle.“I heard all those stories about how people feel during dialysis and I didn’t want to deal with it,” said Mr. Beatty. “I didn’t want to lose my freedom and I certainly didn’t want to feel sick all the time.”So for three years, he waited, hoping to find his miracle. But by August 2013, he knew he waited too long.“Before my first dialysis treatment, I could feel myself pulling away from everyone, especially my family. I didn’t want to go anywhere or do anything and I was sick all the time. I realized at that point that how I was feeling was exactly what I wanted to avoid.”He began his in-center dialysis treatments with American Renal Associates (ARA) at University Kidney Center in Louisville, Kentucky in August 2013 and transferred to another local ARA facility – University Kidney Center Hikes Lane – in May 2014 since it was closer to his home.“In a short time, dialysis completely changed me,” he said. His health improved, along with his confidence, encouraging him to start driving himself to and from treatments. Not only did this give him a renewed feeling of independence, but a strong sense of accomplishment, as well.Now 67 years old, Mr. Beatty says he can do everything he did before, including keeping up with his two granddaughters, playing basketball, among other hobbies, and going on family vacations. In fact, with the help of ARA’s Travel Department, Mr. Beatty can travel stress free. Though he admits traveling while on dialysis can be intimidating, he explained, “All I had to do was show up. ARA’s Travel Team took care of everything.”Receiving a diagnosis of ESRD can be challenging, but Mr. Beatty’s advice is to take a step back and see dialysis as the miracle it is.“It took me three years to realize that dialysis was the miracle I was waiting for. I was an extremely sick individual and just a few months on dialysis completely changed me. I don’t know why I waited as long as I did. I could have been enjoying life for the last few years rather than staying home sick. And I honestly haven’t been sick since I started my dialysis treatments!”Read more patient storiesAmerican Renal Associates operates 240 dialysis clinics in 27 states and Washington D.C., serving more than 17,300 patients with end-stage renal disease in partnership with approximately 400 local nephrologists.If you have questions, you can call 1-877-99-RENAL (1-877-997-3625) or patients@americanrenal.com&quot; 1.8 Summary Need some basic HTML and CSS knowledge to find correct elements How to extract text from common elements How to extract text from specific elements Always have to do some text cleanup of data It usually takes multiple times to get it right See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html "],["xml.html", "Chapter 2 XML and JSON 2.1 Finding XPaths 2.2 Example: GeoCoding With Google 2.3 Using JSON 2.4 Using the RJSONIO Package", " Chapter 2 XML and JSON This is where things get a little dicey because some web pages will return XML and JSON in response to inquiries and while these formats seem complicated they are actually doing you a really big favor by doing this since these formats can ususally be easily parsed using various packges. XML is a bit hard to get your head around and JSON is the new kid on the block which is easier to use. Since this isn’t a full-on course lecture I’ll keep it short as to how and why you would want to use these but any time you spend trying to better understand JSON (and XML) the better of you will be when parsing web pages. It’s not such a big deal if all you are going to be parsing is raw text since the mthods we use to do that avoid XML and JSON although cleaning up raw text has its own problems. Let’s revisit the Wikipedia example from the previous section. library(rvest) # Use read_html to fetch the webpage url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_df &lt;- read_html(url) ten_most_populous &lt;- ten_most_df %&gt;% html_nodes(&quot;table&quot;) %&gt;% `[[`(5) %&gt;% html_table() Let’s look at an XML file that has some basic content:   &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;bookstore&gt; &lt;book category=&quot;COOKING&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Everyday Italian&lt;/title&gt; &lt;author&gt;Giada De Laurentiis&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;30.00&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;CHILDREN&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;WEB&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Learning XML&lt;/title&gt; &lt;author&gt;Erik T. Ray&lt;/author&gt; &lt;year&gt;2003&lt;/year&gt; &lt;price&gt;39.95&lt;/price&gt; &lt;/book&gt; &lt;/bookstore&gt; Well we pulled out all tables and then, by experimentation, we isolated table 6 and got the content corresponding to that. But. Is there a more direct way to find the content ? There is. It requires us to install some helper plugins such as the xPath Finder for Firefox and Chrome. In reality there are a number of ways to find the XML Path or CSS Path for an element within a web page but this is a good one to start. Remeber that we want to find the table corresponding to the “10 Most Populous Countries.” So we activate the xPath finder plugin and the highlight the element of interest. This takes some practice to get it right. Once you highlight the desired elment you will see the corresponding XPATH. Here is a screenshot of what I did. We can use the resulting path to directly access the table without first having to first pull out all tables and then trying to find the right one   # Use read_html to fetch the webpage url &lt;- &quot;https://en.wikipedia.org/wiki/World_population&quot; ten_most_populous &lt;- read_html(url) ten_most_df &lt;- ten_most_populous %&gt;% html_nodes(xpath=&#39;/html/body/div[3]/div[3]/div[5]/div[1]/table[4]&#39;) %&gt;% html_table() # We have to get the first element of the list. ten_most_df &lt;- ten_most_df[[1]] ten_most_df ## # A tibble: 10 × 6 ## Rank Country Population `% of world` Date `Source(official or UN)` ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 China 1,411,845,680 17.8% 7 Feb 2022 National population clock[90] ## 2 2 India 1,387,752,092 17.5% 7 Feb 2022 National population clock[91] ## 3 3 United States 333,186,076 4.20% 7 Feb 2022 National population clock[92] ## 4 4 Indonesia 269,603,400 3.40% 1 Jul 2020 National annual projection[93] ## 5 5 Pakistan 220,892,331 2.79% 1 Jul 2020 UN Projection[94] ## 6 6 Brazil 214,319,901 2.70% 7 Feb 2022 National population clock[95] ## 7 7 Nigeria 206,139,587 2.60% 1 Jul 2020 UN Projection[94] ## 8 8 Bangladesh 172,175,096 2.17% 7 Feb 2022 National population clock[96] ## 9 9 Russia 146,748,590 1.85% 1 Jan 2020 National annual estimate[97] ## 10 10 Mexico 127,792,286 1.61% 1 Jul 2020 National annual projection[98] 2.1 Finding XPaths   In addition to Browser Plugins there are standalone tools such as the Oxygen XML Editor which is availabel through the Emory Software Express Website. This is a comprehensive XML editor that will allow you to parse XML and develop paths to locate specific nodes within an XML document. If you find yourself working with websites with lots of XML then this will be useful. The Oxygen editor is free. Let’s look at an XML file that has some basic content:   &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;bookstore&gt; &lt;book category=&quot;COOKING&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Everyday Italian&lt;/title&gt; &lt;author&gt;Giada De Laurentiis&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;30.00&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;CHILDREN&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt; &lt;/book&gt; &lt;book category=&quot;WEB&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Learning XML&lt;/title&gt; &lt;author&gt;Erik T. Ray&lt;/author&gt; &lt;year&gt;2003&lt;/year&gt; &lt;price&gt;39.95&lt;/price&gt; &lt;/book&gt; &lt;/bookstore&gt;         2.2 Example: GeoCoding With Google Let’s run through an example of using the GeoCoding API with Google. They used to provide free access to this service but no more. You have to sign up for an account and get an API key. If you are currently taking one of my classes I probably have arranged for cloud credits that you can use to do Google Geocoding for free. So one way to do this is to create a URL according to the specification given in the Google Geocoding documentation. We need 1) the base Google URL for the Geocoding service, 2) the format of the desired output (XML or JSON), 3) and address for which we want to find the latitude and longitude, and 4) the API key we create at the Google API service. Here is a fully functional URL you can paste into your browser: https://maps.googleapis.com/maps/api/geocode/xml?address=1510+Clifton+Road,+Atlanta,+GA&amp;key=AIzaSyDPwt1Ya79b7lhsZkh75BjCz-GpMKC9ZYw If you paste this into Chome then you get something back like this: So I could create an R function to take care of this kind of thing so I could maybe pass in arbitrary addressess to be geocided. Let’s run through this example and then look at how I parsed the XML file that is returned by the Google GeoCoding API. We will stick with the 1510 Clifton Rd, Atlanta, GA address which corresponds to the Rollins Research Building. First we will see an example of what Google returns in terms of XML. We can use some tools like Oxygen Editor (available free via Emory Software Express) to develop an appropriate XPATH expression to parse out the latitude and longitude information. # https://maps.googleapis.com/maps/api/geocode/xml?address=1510+Clifton+Road,+Atlanta,+GA&amp;key=AIzaSyDPwt1Ya79b7lhsZkh75BjCz-GpMKC9ZYw # https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&amp;key=AIzaSyDPwt1Ya79b7lhsZkh75BjCz-GpMKC9ZYw myGeo &lt;- function(address=&quot;1510 Clifton Rd Atlanta GA&quot;,form=&quot;xml&quot;) { library(XML) library(RCurl) geourl &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/&quot; # You will need to replace this with your OWN key ! key &lt;- &quot;AIzaSyA3ereIVEjA0gPrxLupPLOKFGH_v98KpMA&quot; address &lt;- gsub(&quot; &quot;,&quot;+&quot;,address) add &lt;- paste0(geourl,form,sep=&quot;?address=&quot;) add &lt;- paste0(add,address,&quot;&amp;key=&quot;) geourl &lt;- paste0(add,key) locale &lt;- getURL(geourl) plocal &lt;- xmlParse(locale,useInternalNodes=TRUE) # Okay let&#39;s extract the lat and lon latlon &lt;- getNodeSet(plocal,&quot;/GeocodeResponse/result/geometry/location/descendant::*&quot;) lat &lt;- as.numeric(xmlSApply(latlon,xmlValue))[1] lon &lt;- as.numeric(xmlSApply(latlon,xmlValue))[2] return(c(lat=lat,lng=lon)) } mylocs &lt;- myGeo() lat lng 33.79667 -84.32319 Now. We could have saved the report to a file on our local computer and open it up with Oxygen editor and figure out what the approproate XPATH would be. This is basically what I did. Here is a screenshot of the session. I picked an XPATH expression of //location/descendant::*   We could expand this considerable to process a number of addresses. This is a great example of how once you get a single example working then you can generalize this into a function that will allow you to do the same thing for a much larger numnber of addressess. namevec &lt;- c(&quot;Atlanta GA&quot;, &quot;Birmingham AL&quot;, &quot;Seattle WA&quot;, &quot;Sacramento CA&quot;, &quot;Denver CO&quot;, &quot;LosAngeles CA&quot;, &quot;Rochester NY&quot;) cityList &lt;- lapply(namevec,myGeo,eval=FALSE) # Or to get a data frame cities &lt;- data.frame(city=namevec,do.call(rbind,cityList), stringsAsFactors = FALSE) cities ## city lat lng ## 1 Atlanta GA 33.74900 -84.38798 ## 2 Birmingham AL 33.51859 -86.81036 ## 3 Seattle WA 47.60621 -122.33207 ## 4 Sacramento CA 38.58157 -121.49440 ## 5 Denver CO 39.73924 -104.99025 ## 6 LosAngeles CA 34.05223 -118.24368 ## 7 Rochester NY 43.15658 -77.60885 # Let&#39;s create a Map library(leaflet) m &lt;- leaflet(data=cities) m &lt;- addTiles(m) m &lt;- addMarkers(m,popup=cities$city) ## Assuming &quot;lng&quot; and &quot;lat&quot; are longitude and latitude, respectively # Put up the Map - click on the markers m 2.3 Using JSON JSON is fast becoming the primary interchange format over XML although XML is still well supported. R has a number of packages to ease the parsing of JSON/ documents returned by web pages. Ususally you get back a list which is a native data type in R that can easily be manipulated into a data frame. Most web APIs provide an option for JSON or XML although some only provide JSON. There are rules and regulations about how JSON is formed and we will learn them by example but you can look at the numerous tutotorials on the web to locate definitive references. See http://www.w3schools.com/json/ Here is an XML file that describes some employees. &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;John&lt;/firstName&gt; &lt;lastName&gt;Doe&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Anna&lt;/firstName&gt; &lt;lastName&gt;Smith&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Peter&lt;/firstName&gt; &lt;lastName&gt;Jones&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; And here is the corresposning JSON file: { &quot;employees&quot;:[ {&quot;firstName&quot;:&quot;John&quot;, &quot;lastName&quot;:&quot;Doe&quot;}, {&quot;firstName&quot;:&quot;Anna&quot;, &quot;lastName&quot;:&quot;Smith&quot;}, {&quot;firstName&quot;:&quot;Peter&quot;,&quot;lastName&quot;:&quot;Jones&quot;} ] } It is important to note that the actual information in the document, things like city name, county name, latitude, and longitude are the same as they would be in the comparable XML document. JSON documents are at the heart of the NoSQL“database”called MongoDB JSON can be found within many webpages since it is closely related to JavaScript which is a language strongly related to web pages. JSON is very compact and lighweight which has made it a natural followon to XML so much so that it appears to be replacing XML See http:..www.json.org/ for a full description of the specification An object is an unordered set of name/value pairs. An object begins with (left brace) and ends with (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma). An array is an ordered collection of values. An array begins with [ (left bracket) and ends with ] (right bracket). Values are separated by , (comma). A value can be a string in double quotes, or a number, or true or false or null, or an object or an array. These structures can be nested. A string is a sequence of zero or more Unicode characters, wrapped in double quotes, using backslash escapes. A character is represented as a single character string. Do you remember the Google Geocoding example from before ? We can tell Google to send us back JSON instead of XML just by adjusting the URL accordingly: url &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Rd+Atlanta+GA&amp;key=AIzaSyD0zIyn2ijIqb7OKYTGnAnchXY7zt3VB9Y&quot; 2.4 Using the RJSONIO Package To read/parse this in R we use a package called RJSONIO. There are other packages but this is the one we will be using. Download and install it. There is a function called fromJSON which will parse the JSON file and return a list to contain the data. So we parse lists instead of using XPath. Many people feel this to be easier than trying to construct XPath statments. You will have to decide for yourself. library(RJSONIO) url &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/json?address=1510+Clifton+Road,+Atlanta,+GA&amp;key=AIzaSyD0zIyn2ijIqb7OKYTGnAnchXY7zt3VB9Y&quot; geo &lt;- fromJSON(url) Since what we get back is a list we can directly access whatever we want. We just index into the list. No need for complicated XPATHS. str(geo,3) ## List of 2 ## $ results:List of 1 ## ..$ :List of 6 ## .. ..$ address_components:List of 7 ## .. ..$ formatted_address : chr &quot;1510 Clifton Rd, Atlanta, GA 30322, USA&quot; ## .. ..$ geometry :List of 3 ## .. ..$ place_id : chr &quot;ChIJ5QjdF_oG9YgRWAJzCm19Vf8&quot; ## .. ..$ plus_code : Named chr [1:2] &quot;QMWG+MP Druid Hills, Georgia, United States&quot; &quot;865QQMWG+MP&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;compound_code&quot; &quot;global_code&quot; ## .. ..$ types : chr &quot;street_address&quot; ## $ status : chr &quot;OK&quot; geo$results[[1]]$geometry$location ## lat lng ## 33.79667 -84.32319 Let’s put this into a function that helps us get the information for a number of addresses myGeo &lt;- function(address=&quot;1510 Clifton Rd Atlanta GA&quot;,form=&quot;json&quot;) { library(RJSONIO) geourl &lt;- &quot;https://maps.googleapis.com/maps/api/geocode/&quot; # You will need to replace this with your OWN key ! key &lt;- &quot;AIzaSyA3ereIVEjA0gPrxLiopLKJRPOLH_v89DpMA&quot; address &lt;- gsub(&quot; &quot;,&quot;+&quot;,address) add &lt;- paste0(geourl,form,sep=&quot;?address=&quot;) add &lt;- paste0(add,address,&quot;&amp;key=&quot;) geourl &lt;- paste0(add,key) geo &lt;- fromJSON(geourl) lat &lt;- geo$results[[1]]$geometry$location[1] lng &lt;- geo$results[[1]]$geometry$location[2] return(c(lat,lng)) } Consider the following: namevec &lt;- c(&quot;Atlanta GA&quot;, &quot;Birmingham AL&quot;, &quot;Seattle WA&quot;, &quot;Sacramento CA&quot;, &quot;Denver CO&quot;, &quot;LosAngeles CA&quot;, &quot;Rochester NY&quot;) cityList &lt;- lapply(namevec,myGeo) # Or to get a data frame cities &lt;- data.frame(city=namevec,do.call(rbind,cityList), stringsAsFactors = FALSE) Now we can check out the geocoding cities and then make a map cities ## city lat lng ## 1 Atlanta GA 33.74900 -84.38798 ## 2 Birmingham AL 33.51859 -86.81036 ## 3 Seattle WA 47.60621 -122.33207 ## 4 Sacramento CA 38.58157 -121.49440 ## 5 Denver CO 39.73924 -104.99025 ## 6 LosAngeles CA 34.05223 -118.24368 ## 7 Rochester NY 43.15658 -77.60885 # Let&#39;s create a Map library(leaflet) m &lt;- leaflet(data=cities) m &lt;- addTiles(m) m &lt;- addMarkers(m,popup=cities$city) ## Assuming &quot;lng&quot; and &quot;lat&quot; are longitude and latitude, respectively # Put up the Map - click on the markers m "],["Moreexamples.html", "Chapter 3 More Real Life Examples 3.1 BitCoin Prices 3.2 IMDB 3.3 Faculty Salaries 3.4 Filling Out Forms From a Program 3.5 PubMed", " Chapter 3 More Real Life Examples Okay. This is a tour of some sites that will serve as important examples on how to parse sites. Let’s check the price of bitcoins. You want to be rich don’t you ? 3.1 BitCoin Prices The challenge here is that it’s all one big table and it’s not clear how to adress it. And the owners of the web site will ususally change the format or start using Javascript or HTML5 which will mess things up in the future. One solid approach I frequently use is to simply pull out all the tables and, by experimentation, try to figure out which one has the information I want. This always require some work. library(rvest) url &lt;- &quot;https://coinmarketcap.com/all/views/all/&quot; bc &lt;- read_html(url) bc_table &lt;- bc %&gt;% html_nodes(&#39;table&#39;) %&gt;% html_table() %&gt;% .[[3]] # We get back a one element list that is a data frame str(bc_table,0) ## tibble [200 × 1,001] (S3: tbl_df/tbl/data.frame) bc_table &lt;- bc_table[,c(2:3,5)] head(bc_table) ## # A tibble: 6 × 3 ## Name Symbol Price ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BTCBitcoin BTC $43,023.30 ## 2 ETHEthereum ETH $3,055.21 ## 3 USDTTether USDT $1.00 ## 4 BNBBNB BNB $416.09 ## 5 USDCUSD Coin USDC $0.9995 ## 6 XRPXRP XRP $0.8144 Everything is a character at this point so we have to go in an do some surgery on the data frame to turn the Price into an actual numeric. # The data is &quot;dirty&quot; and has characers in it that need cleaning bc_table &lt;- bc_table %&gt;% mutate(Price=gsub(&quot;\\\\$&quot;,&quot;&quot;,Price)) bc_table &lt;- bc_table %&gt;% mutate(Price=gsub(&quot;,&quot;,&quot;&quot;,Price)) bc_table &lt;- bc_table %&gt;% mutate(Price=round(as.numeric(Price),2)) # There are four rows wherein the Price is missing NA bc_table &lt;- bc_table %&gt;% filter(complete.cases(bc_table)) # Let&#39;s get the Crypto currencies with the Top 10 highest prices top_10 &lt;- bc_table %&gt;% arrange(desc(Price)) %&gt;% head(10) top_10 ## # A tibble: 10 × 3 ## Name Symbol Price ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 WBTCWrapped Bitcoin WBTC 43062. ## 2 BTCBitcoin BTC 43023. ## 3 ETHEthereum ETH 3055. ## 4 BNBBNB BNB 416. ## 5 LTCLitecoin LTC 132. ## 6 SOLSolana SOL 112. ## 7 AVAXAvalanche AVAX 84.1 ## 8 LUNATerra LUNA 56.4 ## 9 DOTPolkadot DOT 21.3 ## 10 MATICPolygon MATIC 1.89 Let’s make a barplot of the top 10 crypto currencies. # Next we want to make a barplot of the Top 10 ylim=c(0,max(top_10$Price)+10000) main=&quot;Top 10 Crypto Currencies in Terms of Price&quot; bp &lt;- barplot(top_10$Price,col=&quot;aquamarine&quot;, ylim=ylim,main=main) axis(1, at=bp, labels=top_10$Symbol, cex.axis = 0.7) grid() So that didn’t work out so well since one of the crypto currencies dominates the others in terms of price. So let’s create a log transformed verion of the plot. # Let&#39;s take the log of the price ylim=c(0,max(log(top_10$Price))+5) main=&quot;Top 10 Crypto Currencies in Terms of log(Price)&quot; bp &lt;- barplot(log(top_10$Price),col=&quot;aquamarine&quot;, ylim=ylim,main=main) axis(1, at=bp, labels=top_10$Symbol, cex.axis = 0.7) grid() 3.2 IMDB Look at this example from IMDb (Internet Movie Database). According to Wikipedia: IMDb (Internet Movie Database)[2] is an online database of information related to films, television programs, home videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, fan and critical reviews, and ratings. We can search or refer to specific movies by URL if we wanted. For example, consider the following link to the “Lego Movie”: http://www.imdb.com/title/tt1490017/ In terms of scraping information from this site we could do that using the rvest package. Let’s say that we wanted to capture the rating information which is 7.8 out of 10. We could use the xPath Tool or the Selector gadet tool to zone in on this information. According to selector gadget we have the following xpath expression: url &lt;- &quot;http://www.imdb.com/title/tt1490017/&quot; lego_movie &lt;- read_html(url) # Scrape the website for the movie rating rating &lt;- lego_movie %&gt;% html_nodes(&quot;.ratingValue span&quot;) %&gt;% html_text() So that gives us what we need albeit in character form. Now it’s a simple matter of parsing out the first rating value: (rating &lt;- as.numeric(rating[1])) ## [1] NA That wasn’t so bad. Let’s see what using the xPath plugin gives us: We get a much longer xpath expression which should provide us with direct access to the value. xp &lt;- &quot;/html/body/div[3]/div/div[2]/div[5]/div[1]/div[2]/div/div[1]/div[2]/div/div[1]/div[1]/div[1]/strong/span&quot; # Scrape the website for the movie rating rating &lt;- lego_movie %&gt;% html_nodes(xpath=xp) %&gt;% html_text() %&gt;% as.numeric() rating ## numeric(0) Let’s access the summary section of the link. We could use Selector Gadget or the xPath plugin. I’ll use the former. mov_summary &lt;- lego_movie %&gt;% html_nodes(&quot;.summary_text&quot;) %&gt;% html_text() mov_summary ## character(0) 3.3 Faculty Salaries In this example we have to parse the main table associated with the results page. Salary Salary url &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey&quot; df &lt;- read_html(url) %&gt;% html_table() %&gt;% `[[`(1) intost &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;) salary &lt;- df %&gt;% separate(InstitutionCategoryState,into=intost,sep=&quot;\\n&quot;) salary ## # A tibble: 10 × 8 ## Institution Category State `Avg. SalaryFul… `Avg. ChangeCon… `CountFull Prof… `Avg. Total Com… ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Auburn Uni… Doctoral ALAB… $132,600 4.4% 407 $170,300 ## 2 Birmingham… Baccala… ALAB… $81,000 0.0% 39 $100,000 ## 3 Huntingdon… Baccala… ALAB… $76,700 0.0% 13 $89,500 ## 4 Jacksonvil… Master’s ALAB… $77,300 N/A 78 $104,100 ## 5 Samford Un… Master ALAB… $105,200 3.2% 130 $133,300 ## 6 Troy Unive… Masters ALAB… $84,500 1.6% 30 $88,500 ## 7 The Univer… Doctoral ALAB… $151,600 2.0% 300 $205,100 ## 8 University… Doctoral ALAB… $139,100 2.8% 191 $164,800 ## 9 University… Doctoral ALAB… $126,400 2.0% 64 $169,200 ## 10 University… Master ALAB… $80,700 2.1% 47 $106,100 ## # … with 1 more variable: Salary EquityFull Professors &lt;dbl&gt; So the default is 10 listings per page but there are many more pages we could process to get more information. If we look at the bottom of the page we can get some clues as to what the URLs are. Here we’ll just process the first two pages since it will be quick and won’t burden the server. # So now we could process multiple pages url &lt;- &#39;https://www.insidehighered.com/aaup-compensation-survey?institution-name=&amp;professor-category=1591&amp;page=1&#39; str1 &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey?&quot; str2 &lt;- &quot;institution-name=&amp;professor-category=1591&amp;page=&quot; intost &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;) salary &lt;- data.frame() # We&#39;ll get just the first two pages for (ii in 1:2) { nurl &lt;- paste(str1,str2,ii,sep=&quot;&quot;) df &lt;- read_html(nurl) tmp &lt;- df %&gt;% html_table() %&gt;% `[[`(1) tmp &lt;- tmp %&gt;% separate(InstitutionCategoryState,into=intost,sep=&quot;\\n&quot;) salary &lt;- rbind(salary,tmp) } salary Look at the URLs at the bottom of the main page to find beginning and ending page numbers. Visually this is easy. Programmatically we could do something like the following: # https://www.insidehighered.com/aaup-compensation-survey?page=1 # https://www.insidehighered.com/aaup-compensation-survey?page=94 # What is the last page number ? We already know the answer - 94 lastnum &lt;- df %&gt;% html_nodes(xpath=&#39;//a&#39;) %&gt;% html_attr(&quot;href&quot;) %&gt;% &#39;[&#39;(103) %&gt;% strsplit(.,&quot;page=&quot;) %&gt;% &#39;[[&#39;(1) %&gt;% &#39;[&#39;(2) %&gt;% as.numeric(.) # So now we could get all pages of the survey str1 &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey?&quot; str2 &lt;- &quot;institution-name=&amp;professor-category=1591&amp;page=&quot; intost &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;) salary &lt;- data.frame() for (ii in 1:lastnum) { nurl &lt;- paste(str1,str2,ii,sep=&quot;&quot;) df &lt;- read_html(nurl) tmp &lt;- df %&gt;% html_table() %&gt;% `[[`(1) tmp &lt;- tmp %&gt;% separate(InstitutionCategoryState,into=intost,sep=&quot;\\n&quot;) salary &lt;- rbind(salary,tmp) Sys.sleep(1) } names(salary) &lt;- c(&quot;Institution&quot;,&quot;Category&quot;,&quot;State&quot;,&quot;AvgSalFP&quot;,&quot;AvgChgFP&quot;, &quot;CntFP&quot;,&quot;AvgTotCompFP&quot;,&quot;SalEquityFP&quot;) salary &lt;- salary %&gt;% mutate(AvgSalFP=as.numeric(gsub(&quot;\\\\$|,&quot;,&quot;&quot;,salary$AvgSalFP))) %&gt;% mutate(AvgTotCompFP=as.numeric(gsub(&quot;\\\\$|,&quot;,&quot;&quot;,salary$AvgTotCompFP))) salary %&gt;% group_by(State,Category) %&gt;% summarize(avg=mean(AvgSalFP)) %&gt;% arrange(desc(avg)) There are some problems: Data is large and scattered across multiple pages We could use above techniques to move from page to page There is a form we could use to narrow criteria But we have to programmatically submit the form rvest (and other packages) let you do this 3.4 Filling Out Forms From a Program Salary Let’s find salaries between $ 150,000 and the default max ($ 244,000) Find the element name associated with “Average Salary” Establish a connection with the form (usually the url of the page) Get a local copy of the form Fill in the value for the “Average Salary” Submit the lled in form Get the results and parse them like above ` So finding the correct element is more challenging. I use Chrome to do this. Just highlight the area over the form and right click to “Insepct” the element. This opens up the developer tools. You have to dig down to find the corrext form and the element name. Here is a screen shot of my activity: Salary url &lt;- &quot;https://www.insidehighered.com/aaup-compensation-survey&quot; # Establish a session mysess &lt;- html_session(url) # Get the form form_unfilled &lt;- mysess %&gt;% html_node(&quot;form&quot;) %&gt;% html_form() form_filled &lt;- form_unfilled %&gt;% set_values(&quot;range-from&quot;=150000) # Submit form results &lt;- submit_form(mysess,form_filled) first_page &lt;- results %&gt;% html_nodes(xpath=expr) %&gt;% html_table() first_page 3.5 PubMed Pubmed provides a rich source of information on published scientific literature. There are tutorials on how to leverage its capabilities but one thing to consider is that MESH terms are a good starting place since the search is index-based. MeSH (Medical Subject Headings) is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed. It’s faster and more accurate so you can first use the MESH browser to generate the appropriate search terms and add that into the Search interface. The MESH browser can be found at https://www.ncbi.nlm.nih.gov/mesh/ What we do here is get the links associated with each publication so we can then process each of those and get the abstract associated with each publication. # &quot;hemodialysis, home&quot; [MeSH Terms] url&lt;-&quot;https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D&quot; # # The results from the search will be of the form: # https://www.ncbi.nlm.nih.gov/pubmed/30380542 results &lt;- read_html(url) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% grep(&quot;/[0-9]{6,6}&quot;,.,value=TRUE) %&gt;% unique(.) results ## [1] &quot;/27061610/&quot; &quot;/30041224/&quot; &quot;/25925822/&quot; &quot;/25925819/&quot; &quot;/28066912/&quot; &quot;/28535526/&quot; &quot;/27545636/&quot; ## [8] &quot;/30041223/&quot; &quot;/26586045/&quot; &quot;/27781373/&quot; So now we could loop through these links and get the abstracts for these results. It looks that there are approximately 20 results per page. As before we would have to dive in to the underlying structure of the page to get the correct HTML pathnames or we could just look for Paragraph elements and pick out the links that way. text.vec &lt;- vector() for (ii in 1:length(results)) { string &lt;- paste0(&quot;https://pubmed.ncbi.nlm.nih.gov&quot;,results[ii]) text.vec[ii] &lt;- read_html(string) %&gt;% html_nodes(&quot;p&quot;) %&gt;% `[[`(7) %&gt;% html_text() } # Eliminate lines with newlines characters final.vec &lt;- gsub(&quot;\\n&quot;,&quot;&quot;,text.vec) final.vec &lt;- gsub(&quot;^\\\\s+&quot;,&quot;&quot;,final.vec) #final.vec &lt;- text.vec[grep(&quot;^\\n&quot;,text.vec,invert=TRUE)] final.vec ## [1] &quot;Pediatric home hemodialysis is infrequently performed despite a growing need globally among patients with end-stage renal disease who do not have immediate access to a kidney transplant. In this review, we expand the scope of the Implementing Hemodialysis in the Home website and associated supplement published previously in Hemodialysis International and offer information tailored to the pediatric population. We describe the experience and outcomes of centers managing pediatric patients, and offer recommendations and practical tools to assist clinicians in providing quotidian dialysis for children, including infrastructural and staffing needs, equipment and prescriptions, and patient selection and training. &quot; ## [2] &quot;Home hemodialysis (HHD) has been available as a modality of renal replacement therapy since the 1960s. HHD allows intensive dialysis such as nocturnal hemodialysis or short daily hemodialysis. Previous studies have shown that patients receiving HHD have an increased survival and better quality of life compared with those receiving in-center conventional HD. However, HHD may increase the risk for specific complications such as vascular access complications, infection, loss of residual kidney function and patient and caregiver burden. In Japan, only 529 patients (0.2% of the total dialysis patients) were on maintenance HHD at the end of 2014. The most commonly perceived barriers to intensive HHD included lack of patient motivation, unwillingness to change from in-center modality, and fear of self-cannulation. However, these barriers can often be overcome by adequate predialysis education, motivational training of patient and caregiver, nurse-assisted cannulation, nurse-led home visits, a well-defined nursing/technical support system for patients, and provision of respite care. &quot; ## [3] &quot;This special supplement of Hemodialysis International focuses on home hemodialysis (HD). It has been compiled by a group of international experts in home HD who were brought together throughout 2013-2014 to construct a home HD \\&quot;manual.\\&quot; Drawing upon both the literature and their own extensive expertise, these experts have helped develop this supplement that now stands as an A-to-Z guide for any who may be unfamiliar or uncertain about how to establish and maintain a successful home HD program. &quot; ## [4] &quot;Prescribing a regimen that provides \\&quot;optimal dialysis\\&quot; to patients who wish to dialyze at home is of major importance, yet there is substantial variation in how home hemodialysis (HD) is prescribed. Geographic location, patient health status and clinical goals, and patient lifestyle and preferences all influence the selection of a prescription for a particular patient-there is no single prescription that provides optimal therapy for all patients, and careful weighing of potential benefit and burden is required for long-term success. This article describes how home HD prescribing patterns have changed over time and provides examples of commonly used home HD prescriptions. In addition, associated clinical outcomes and adequacy parameters as well as criteria for identifying which patients may benefit most from these diverse prescriptions are also presented. &quot; ## [5] &quot;Home hemodialysis (HD) was first introduced in the 1960s with a rapid increase in its use due to inability of dialysis units to accommodate patient demand. A sharp decline was subsequently seen with expanding outpatient dialysis facilities and changes in reimbursement policies. In the last decade, with emerging reports of benefits with home HD and more user-friendly equipment, there has been resurgence in home HD. However, home HD remains underutilized with considerable variations between and within countries. This paper will review the history of home HD, elaborate on its established benefits, identify some of the barriers in uptake of this modality and expand on potential strategies to overcome these barriers. &quot; ## [6] &quot;Home hemodialysis (HD) is undergoing a resurgence. A major driver of this is economics, however, providers are also encouraged by a combination of excellent patient outcomes and patient experiences as well as the development of newer technologies that offer ease of use. Home HD offers significant advantages in flexible scheduling and the practical implementation of extended hours dialysis. This paper explores the reasons why home HD is making a comeback and strives to offer approaches to improve the uptake of this dialysis modality. &quot; ## [7] &quot;The home extracorporeal hemodialysis, which aroused a great interest in the past, has not kept its promises due to the complexity and expectations for family involvement in treatment management. In the United States NxStage One portable system was proposed and designed for home use. In this work we describe, starting from the history of home hemodialysis, the method with NxStage system by comparing it with the conventional HD in 5 patients. The dialysis efficiency was similar between the two treatments, even if home hemodialysis showed a reduction in serum urea, creatinine and phosphorus. At the same time phosphate binders use decreased with an increase in serum calcium while hemoglobin increased reducing doses of erythropoietin. The method was successful in the training of the patients and their partners during hospital training and at home. Patients have shown great enthusiasm at the beginning and during the therapy, which is developed around the users personal needs, being able to decide at its own times during 24 hours according to personal needs, in addition to faster recovery after the dialysis. This method certainly improved the patients&#39; wellness and increased their autonomy. &quot; ## [8] &quot;Most hemodialysis (HD) in Japan is based on the central dialysis fluid delivery system (CDDS). With CDDS, there is an improvement in work efficiency, reduction in cost, and a reduction in regional and institutional differences in dialysis conditions. This has resulted in an improvement in the survival rate throughout Japan. However, as the number of cases with various complications increases, it is necessary to select the optimal dialysis prescription (including hours and frequency) for each individual in order to further improve survival rates. To perform intensive HD, home HD is essential, and various prescriptions have been tried. However, several challenges remain before widespread implementation of home HD can occur. &quot; ## [9] &quot;Home hemodialysis (HD) is a modality of renal replacement therapy that can be safely and independently performed at home by end-stage renal disease (ESRD) patients. Home HD can be performed at the convenience of the patients on a daily basis, every other day and overnight (nocturnal). Despite the great and many perceived benefits of home HD, including the significant improvements in health outcomes and resource utilization, the adoption of home HD has been limited; lack or inadequate pre-dialysis education and training constitute a major barrier. The lack of self-confidence and/or self-efficacy to manage own therapy, lack of family and/or social support, fear of machine and cannulation of blood access and worries of possible catastrophic events represent other barriers for the implementation of home HD besides inadequate competence and/or expertise in caring for home HD patients among renal care providers (nephrologists, dialysis nurses, educators). A well-studied, planned and prepared and carefully implemented central country program supported by adequate budget can play a positive role in overcoming the challenges to home HD. Healthcare authorities, with the increasingly financial and logistic demands and the relatively higher mortality and morbidity rates of the conventional in-center HD, should tackle home HD as an attractive and cost-effective modality with more freedom, quality of life and improvement of clinical outcomes for the ESRD patients. &quot; ## [10] &quot;Home hemodialysis (HHD) is emerging as an important alternate renal replacement therapy. Although there are multiple clinical advantages with HHD, concerns surrounding increased risks of infection in this group of patients remain a major barrier to its implementation. In contrast to conventional hemodialysis, infection related complication represents the major morbidity in this mode of renal replacement therapy. Vascular access related infection is an important cause of infection in this population. Use of central vein catheters and buttonhole cannulation in HHD are important modifiable risk factors for HHD associated infection. Several preventive measures are suggested in the literature, which will require further prospective validation. &quot; Well that was tedious. And we processed only the first page of results. How do we “progrmmatically” hit the “Next” Button at the bottom of the page ? This is complicated by the fact that there appears to be some Javascript at work that we would have to somehow interact with to get the URL for the next page. Unlike with the school salary example it isn’t obvious how to do this. If we hove over the “Next” button we don’t get an associated link. "],["APIs.html", "Chapter 4 APIs 4.1 OMDB 4.2 The omdbapi package 4.3 RSelenium 4.4 EasyPubMed", " Chapter 4 APIs 4.1 OMDB Let’s look at the IMDB page which catalogues lots of information about movies. Just got to the web site and search although here is an example link. https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2 In this case we would like to get the summary information for the movie. So we would use Selector Gadget or some other method to find the XPATH or CSS associated with this element. This pretty easy and doesn’t present much of a problem although for large scale mining of movie data we would run into trouble because IMDB doesn’t really like you to scrape their pages. They have an API that they would like for you to use. url &lt;- &#39;https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2&#39; summary &lt;- read_html(url) %&gt;% html_nodes(xpath=&quot;/html/body/div[2]/main/div/section[1]/section/div[3]/section/section/div[3]/div[2]/div[1]/div[1]/div[2]/span[3]&quot;) %&gt;% html_text() summary But here we go again. We have to parse the desired elements on this page and then what if we wanted to follow other links or set up a general function to search IMDB for other movies of various genres, titles, directors, etc. So as an example on how this works. Paste the URL into any web browser. You must supply your key for this to work. What you get back is a JSON formatted entry corresponding to ”The GodFather”movie. url &lt;- &quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=The+Godfather&quot; library(RJSONIO) url &lt;- &quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=The+Godfather&quot; # Fetch the URL via fromJSON movie &lt;- fromJSON(&quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=The+Godfather&quot;) # We get back a list which is much easier to process than raw JSON or XML str(movie) ## List of 25 ## $ Title : chr &quot;The Godfather&quot; ## $ Year : chr &quot;1972&quot; ## $ Rated : chr &quot;R&quot; ## $ Released : chr &quot;24 Mar 1972&quot; ## $ Runtime : chr &quot;175 min&quot; ## $ Genre : chr &quot;Crime, Drama&quot; ## $ Director : chr &quot;Francis Ford Coppola&quot; ## $ Writer : chr &quot;Mario Puzo, Francis Ford Coppola&quot; ## $ Actors : chr &quot;Marlon Brando, Al Pacino, James Caan&quot; ## $ Plot : chr &quot;The Godfather follows Vito Corleone, Don of the Corleone family, as he passes the mantle to his unwilling son, Michael.&quot; ## $ Language : chr &quot;English, Italian, Latin&quot; ## $ Country : chr &quot;United States&quot; ## $ Awards : chr &quot;Won 3 Oscars. 31 wins &amp; 30 nominations total&quot; ## $ Poster : chr &quot;https://m.media-amazon.com/images/M/MV5BM2MyNjYxNmUtYTAwNi00MTYxLWJmNWYtYzZlODY3ZTk3OTFlXkEyXkFqcGdeQXVyNzkwMjQ&quot;| __truncated__ ## $ Ratings :List of 3 ## ..$ : Named chr [1:2] &quot;Internet Movie Database&quot; &quot;9.2/10&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Source&quot; &quot;Value&quot; ## ..$ : Named chr [1:2] &quot;Rotten Tomatoes&quot; &quot;97%&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Source&quot; &quot;Value&quot; ## ..$ : Named chr [1:2] &quot;Metacritic&quot; &quot;100/100&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Source&quot; &quot;Value&quot; ## $ Metascore : chr &quot;100&quot; ## $ imdbRating: chr &quot;9.2&quot; ## $ imdbVotes : chr &quot;1,742,506&quot; ## $ imdbID : chr &quot;tt0068646&quot; ## $ Type : chr &quot;movie&quot; ## $ DVD : chr &quot;11 May 2004&quot; ## $ BoxOffice : chr &quot;$134,966,411&quot; ## $ Production: chr &quot;N/A&quot; ## $ Website : chr &quot;N/A&quot; ## $ Response : chr &quot;True&quot; movie$Plot ## [1] &quot;The Godfather follows Vito Corleone, Don of the Corleone family, as he passes the mantle to his unwilling son, Michael.&quot; sapply(movie$Ratings,unlist) ## [,1] [,2] [,3] ## Source &quot;Internet Movie Database&quot; &quot;Rotten Tomatoes&quot; &quot;Metacritic&quot; ## Value &quot;9.2/10&quot; &quot;97%&quot; &quot;100/100&quot; Let’s Get all the Episodes for Season 1 of Game of Thrones url &lt;- &quot;http://www.omdbapi.com/?apikey=f7c004c&amp;t=Game%20of%20Thrones&amp;Season=1&quot; movie &lt;- fromJSON(url) str(movie,1) ## List of 5 ## $ Title : chr &quot;Game of Thrones&quot; ## $ Season : chr &quot;1&quot; ## $ totalSeasons: chr &quot;8&quot; ## $ Episodes :List of 10 ## $ Response : chr &quot;True&quot; episodes &lt;- data.frame(do.call(rbind,movie$Episodes),stringsAsFactors = FALSE) episodes ## Title Released Episode imdbRating imdbID ## 1 Winter Is Coming 2011-04-17 1 9.1 tt1480055 ## 2 The Kingsroad 2011-04-24 2 8.8 tt1668746 ## 3 Lord Snow 2011-05-01 3 8.7 tt1829962 ## 4 Cripples, Bastards, and Broken Things 2011-05-08 4 8.8 tt1829963 ## 5 The Wolf and the Lion 2011-05-15 5 9.1 tt1829964 ## 6 A Golden Crown 2011-05-22 6 9.2 tt1837862 ## 7 You Win or You Die 2011-05-29 7 9.2 tt1837863 ## 8 The Pointy End 2011-06-05 8 9.0 tt1837864 ## 9 Baelor 2011-06-12 9 9.6 tt1851398 ## 10 Fire and Blood 2011-06-19 10 9.5 tt1851397 4.2 The omdbapi package Wait a minute. Looks like someone created an R package that wraps all this for us. It is called omdbapi # Use devtools to install devtools::install_github(&quot;hrbrmstr/omdbapi&quot;) library(omdbapi) # The first time you use this you will be prompted to enter your # API key movie_df &lt;- search_by_title(&quot;Star Wars&quot;, page = 2) (movie_df &lt;- movie_df[,-5]) ## Title Year imdbID Type ## 1 Solo: A Star Wars Story 2018 tt3778644 movie ## 2 Star Wars: The Clone Wars 2008–2020 tt0458290 series ## 3 Star Wars: The Clone Wars 2008 tt1185834 movie ## 4 Star Wars: Rebels 2014–2018 tt2930604 series ## 5 Star Wars: Clone Wars 2003–2005 tt0361243 series ## 6 Star Wars: The Bad Batch 2021– tt12708542 series ## 7 The Star Wars Holiday Special 1978 tt0193524 movie ## 8 Star Wars: Visions 2021– tt13622982 series ## 9 Robot Chicken: Star Wars 2007 tt1020990 movie ## 10 Star Wars: Knights of the Old Republic 2003 tt0356070 game # Get lots of info on The GodFather (gf &lt;- find_by_title(&quot;The GodFather&quot;)) ## # A tibble: 3 × 25 ## Title Year Rated Released Runtime Genre Director Writer Actors Plot Language Country Awards ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The … 1972 R 1972-03-24 175 min Crim… Francis… Mario… Marlo… The … English… United… Won 3… ## 2 The … 1972 R 1972-03-24 175 min Crim… Francis… Mario… Marlo… The … English… United… Won 3… ## 3 The … 1972 R 1972-03-24 175 min Crim… Francis… Mario… Marlo… The … English… United… Won 3… ## # … with 12 more variables: Poster &lt;chr&gt;, Ratings &lt;list&gt;, Metascore &lt;chr&gt;, imdbRating &lt;dbl&gt;, ## # imdbVotes &lt;dbl&gt;, imdbID &lt;chr&gt;, Type &lt;chr&gt;, DVD &lt;date&gt;, BoxOffice &lt;chr&gt;, Production &lt;chr&gt;, ## # Website &lt;chr&gt;, Response &lt;chr&gt; # Get the actors from the GodFather get_actors((gf)) ## [1] &quot;Marlon Brando&quot; &quot;Al Pacino&quot; &quot;James Caan&quot; 4.3 RSelenium Sometimes we interact with websites that use Javascript to load more text or comments in a user forum. Here is an example of that. Look at https://www.dailystrength.org/group/dialysis which is a website associated with people wanting to share information about dialysis. If you check the bottom of the pag you will see a button. # https://www.dailystrength.org/group/dialysis library(RSelenium) library(rvest) library(tm) library(SentimentAnalysis) library(wordcloud) url &lt;- &quot;https://www.dailystrength.org/group/dialysis&quot; # The website has a &quot;show more&quot; button that hides most of the patient posts # If we don&#39;t find a way to programmatically &quot;click&quot; this button then we can # only get a few of the posts and their responses. To do this we need to # use the RSelenium package which does a lot of behind the scenes work # See https://cran.r-project.org/web/packages/RSelenium/RSelenium.pdf # http://brazenly.blogspot.com/2016/05/r-advanced-web-scraping-dynamic.html # Open up a connection # rD &lt;- rsDriver() # So, you might have to specify the version of chrome you are using # For someone reason this seems now to be necessary (11/4/19) rD &lt;- rsDriver(browser=c(&quot;chrome&quot;),chromever=&quot;78.0.3904.70&quot;) remDr &lt;- rD[[&quot;client&quot;]] remDr$navigate(url) loadmorebutton &lt;- remDr$findElement(using = &#39;css selector&#39;, &quot;#load-more-discussions&quot;) # Do this a number of times to get more links loadmorebutton$clickElement() # Now get the page with more comments and questions page_source &lt;- remDr$getPageSource() # So let&#39;s parse the contents comments &lt;- read_html(page_source[[1]]) cumulative_comments &lt;- vector() links &lt;- comments %&gt;% html_nodes(css=&quot;.newsfeed__description&quot;) %&gt;% html_node(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) full_links &lt;- paste0(&quot;https://www.dailystrength.org&quot;,links) if (length(grep(&quot;NA&quot;,full_links)) &gt; 0) { full_links &lt;- full_links[-grep(&quot;NA&quot;,full_links)] } ugly_xpath &lt;- &#39;//*[contains(concat( &quot; &quot;, @class, &quot; &quot; ), concat( &quot; &quot;, &quot;comments__comment-text&quot;, &quot; &quot; ))] | //p&#39; for (ii in 1:length(full_links)) { text &lt;- read_html(full_links[ii]) %&gt;% html_nodes(xpath=ugly_xpath) %&gt;% html_text() length(text) &lt;- length(text) - 1 text &lt;- text[-1] text cumulative_comments &lt;- c(cumulative_comments,text) } remDr$close() # stop the selenium server rD[[&quot;server&quot;]]$stop() 4.4 EasyPubMed So there is an R package called EasyPubMed that helps ease the access of data on the Internet. The idea behind this package is to be able to query NCBI Entrez and retrieve PubMed records in XML or TXT format. The PubMed records can be downloaded and saved as XML or text files if desired. According to the package authours, “Data integrity is enforced during data download, allowing to retrieve and save very large number of records effortlessly.” The bottom line is that you can do what you want after that. Let’s look at an example involving home hemodialysis library(easyPubMed) Let’s do some searching my_query &lt;- &#39;hemodialysis, home&quot; [MeSH Terms]&#39; my_entrez_id &lt;- get_pubmed_ids(my_query) my_abstracts &lt;- fetch_pubmed_data(my_entrez_id) my_abstracts &lt;- custom_grep(my_abstracts,&quot;AbstractText&quot;,&quot;char&quot;) my_abstracts[1:3] [1] &quot;Assisted PD (assPD) is an option of home dialysis treatment for dependent end-stage renal patients and worldwide applied in different countries since more than 40 years. China and Germany shares similar trends in demographic development with a growing proportion of elderly referred to dialysis treatment. So far number of patients treated by assPD is low in both countries. We analyze experiences in the implementation process, barriers, and benefits of ass PD in the aging population to provide a model for sustainable home dialysis treatment with PD in both countries. Differences and similarities of different factors (industrial, patient and facility based) which affect utilization of assPD are discussed. AssPD should be promoted in China and Germany to realize the benefits of home dialysis for the aging population by providing a structured model of implementation and quality assurance.&quot; [2] &quot;End-stage renal disease (ESRD) is the final stage of chronic kidney disease in which the kidney is not sufficient to meet the needs of daily life. It is necessary to understand the role of genes expression involved in ESRD patient responses to nocturnal hemodialysis (NHD) and to improve the immunity responsiveness. The aim of this study was to investigate novel immune-associated genes that may play important roles in patients with ESRD.The microarray expression profiles of peripheral blood in patients with ESRD before and after NHD were analyzed by network-based approaches, and then using Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes pathway analysis to explore the biological process and molecular functions of differentially expressed genes. Subsequently, a transcriptional regulatory network of the core genes and the connected transcriptional regulators was constructed. We found that NHD had a significant effect on neutrophil activation and immune response in patients with ESRD.In addition, Our findings suggest that MAPKAPK3, RHOA, ARRB2, FLOT1, MYH9, PRKCD, RHOG, PTPN6, MAPK3, CNPY3, PI3KCG, and PYGL genes maybe potential targets regulated by core transcriptional factors, including ARNT, C/EBPalpha, CEBPA, CREB1, PSG1, DAND5, SP1, GATA1, MYC, EGR2, and EGR3.&quot; [3] &quot;Only a minority of patients with chronic kidney disease treated by hemodialysis are currently treated at home. Until relatively recently, the only type of hemodialysis machine available for these patients was a slightly smaller version of the standard machines used for in-center dialysis treatments. Areas covered: There are now an alternative generation of dialysis machines specifically designed for home hemodialysis. The home dialysis patient wants a smaller machine, which is intuitive to use, easy to trouble shoot, robust and reliable, quick to setup and put away, requiring minimal waste disposal. The machines designed for home dialysis have some similarities in terms of touch-screen patient interfaces, and using pre-prepared cartridges to speed up setting up the machine. On the other hand, they differ in terms of whether they use slower or standard dialysate flows, prepare batches of dialysis fluid, require separate water purification equipment, or whether this is integrated, or use pre-prepared sterile bags of dialysis fluid. Expert commentary: Dialysis machine complexity is one of the hurdles reducing the number of patients opting for home hemodialysis and the introduction of the newer generation of dialysis machines designed for ease of use will hopefully increase the number of patients opting for home hemodialysis.&quot; "],["bagofwords.html", "Chapter 5 Bag of Words Sentiment Analysis 5.1 Workflow 5.2 Simple Example 5.3 tidytext 5.4 Back To The PubMed Example 5.5 BiGrams", " Chapter 5 Bag of Words Sentiment Analysis One we have a collection of text it’s interesting to figure out what it might mean or infer - if anything at all. In text analysis and NLP (Natural Language Processing) we talk about “Bag of Words” to describe a collection or “corpus” of unstructured text. What do we do with a “bag of words” ? Extract meaning from collections of text (without reading !) Detect and analyze patterns in unstructured textual collections Use Natural Language Processing techniques to reach conclusions Discover what ideas occur in text and how they might be linked Determine if the discovered patterns be used to predict behavior ? Identify interesting ideas that might otherwise be ignored 5.1 Workflow Identify and Obtain text (e.g. websites, Twitter, Databases, PDFs, surveys) Create a text ”Corpus”- a structure that contains the raw text Apply transformations: Normalize case (convert to lower case) Remove puncutation and stopwords Remove domain specific stopwords Perform Analysis and Visualizations (word frequency, tagging, wordclouds) Do Sentiment Analysis R has Packages to Help. These are just some of them: QDAP - Quantitative Discourse Package tm - text mining applications within R tidytext - Text Mining using ddplyr and ggplot and tidyverse tools SentimentAnalysis - For Sentiment Analysis However, consider that: Some of these are easier to use than others Some can be kind of a problem to install (e.g. qdap) They all offer similar capabilities We’ll look at tidytext 5.2 Simple Example Find the URL for Lincoln’s March 4, 1865 Speech: url &lt;- &quot;https://millercenter.org/the-presidency/presidential-speeches/march-4-1865-second-inaugural-address&quot; library(rvest) lincoln_doc &lt;- read_html(url) %&gt;% html_nodes(&quot;.view-transcript&quot;) %&gt;% html_text() lincoln_doc ## [1] &quot;TranscriptTranscriptFellow-Countrymen: At this second appearing to take the oath of the Presidential office there is less occasion for an extended address than there was at the first. Then a statement somewhat in detail of a course to be pursued seemed fitting and proper. Now, at the expiration of four years, during which public declarations have been constantly called forth on every point and phase of the great contest which still absorbs the attention and engrosses the energies of the nation, little that is new could be presented. The progress of our arms, upon which all else chiefly depends, is as well known to the public as to myself, and it is, I trust, reasonably satisfactory and encouraging to all. With high hope for the future, no prediction in regard to it is ventured.On the occasion corresponding to this four years ago all thoughts were anxiously directed to an impending civil war. All dreaded it, all sought to avert it. While the inaugural address was being delivered from this place, devoted altogether to saving the Union without war, insurgent agents were in the city seeking to destroy it without war-seeking to dissolve the Union and divide effects by negotiation. Both parties deprecated war, but one of them would make war rather than let the nation survive, and the other would accept war rather than let it perish, and the war came.One-eighth of the whole population were colored slaves, not distributed generally over the Union. but localized in the southern part of it. These slaves constituted a peculiar and powerful interest. All knew that this interest was somehow the cause of the war. To strengthen, perpetuate, and extend this interest was the object for which the insurgents would rend the Union even by war, while the Government claimed no right to do more than to restrict the territorial enlargement of it. Neither party expected for the war the magnitude or the duration which it has already attained. Neither anticipated that the cause of the conflict might cease with or even before the conflict itself should cease. Each looked for an easier triumph, and a result less fundamental and astounding. Both read the same Bible and pray to the same God, and each invokes His aid against the other. It may seem strange that any men should dare to ask a just God&#39;s assistance in wringing their bread from the sweat of other men&#39;s faces, but let us judge not, that we be not judged. The prayers of both could not be answered. That of neither has been answered fully. The Almighty has His own purposes. \\&quot;Woe unto the world because of offenses; for it must needs be that offenses come, but woe to that man by whom the offense cometh.\\&quot; If we shall suppose that American slavery is one of those offenses which, in the providence of God, must needs come, but which, having continued through His appointed time, He now wills to remove, and that He gives to both North and South this terrible war as the woe due to those by whom the offense came, shall we discern therein any departure from those divine attributes which the believers in a living God always ascribe to Him? Fondly do we hope, fervently do we pray, that this mighty scourge of war may speedily pass away. Yet, if God wills that it continue until all the wealth piled by the bondsman&#39;s two hundred and fifty years of unrequited toil shall be sunk, and until every drop of blood drawn with the lash shall be paid by another drawn with the sword, as was said three thousand years ago, so still it must be said \\&quot;the judgments of the Lord are true and righteous altogether.\\&quot;With malice toward none, with charity for all, with firmness in the fight as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nation&#39;s wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.&quot; There are probably lots of words that don’t really “matter” or contribute to the “real” meaning of the speech. word_vec &lt;- unlist(strsplit(lincoln_doc,&quot; &quot;)) word_vec[1:20] ## [1] &quot;TranscriptTranscriptFellow-Countrymen:&quot; &quot;&quot; ## [3] &quot;At&quot; &quot;this&quot; ## [5] &quot;second&quot; &quot;appearing&quot; ## [7] &quot;to&quot; &quot;take&quot; ## [9] &quot;the&quot; &quot;oath&quot; ## [11] &quot;of&quot; &quot;the&quot; ## [13] &quot;Presidential&quot; &quot;office&quot; ## [15] &quot;there&quot; &quot;is&quot; ## [17] &quot;less&quot; &quot;occasion&quot; ## [19] &quot;for&quot; &quot;an&quot; sort(table(word_vec),decreasing = TRUE)[1:10] ## word_vec ## the to and of that for be in it a ## 54 26 24 22 11 9 8 8 8 7 How do we remove all the uninteresting words ? We could do it manaully # Remove all punctuation marks word_vec &lt;- gsub(&quot;[[:punct:]]&quot;,&quot;&quot;,word_vec) stop_words &lt;- c(&quot;the&quot;,&quot;to&quot;,&quot;and&quot;,&quot;of&quot;,&quot;the&quot;,&quot;for&quot;,&quot;in&quot;,&quot;it&quot;, &quot;a&quot;,&quot;this&quot;,&quot;which&quot;,&quot;by&quot;,&quot;is&quot;,&quot;an&quot;,&quot;hqs&quot;,&quot;from&quot;, &quot;that&quot;,&quot;with&quot;,&quot;as&quot;) for (ii in 1:length(stop_words)) { for (jj in 1:length(word_vec)) { if (stop_words[ii] == word_vec[jj]) { word_vec[jj] &lt;- &quot;&quot; } } } word_vec &lt;- word_vec[word_vec != &quot;&quot;] sort(table(word_vec),decreasing = TRUE)[1:10] ## word_vec ## war all be we but God shall was do let ## 11 8 8 6 5 5 5 5 4 4 word_vec[1:30] ## [1] &quot;TranscriptTranscriptFellowCountrymen&quot; &quot;At&quot; ## [3] &quot;second&quot; &quot;appearing&quot; ## [5] &quot;take&quot; &quot;oath&quot; ## [7] &quot;Presidential&quot; &quot;office&quot; ## [9] &quot;there&quot; &quot;less&quot; ## [11] &quot;occasion&quot; &quot;extended&quot; ## [13] &quot;address&quot; &quot;than&quot; ## [15] &quot;there&quot; &quot;was&quot; ## [17] &quot;at&quot; &quot;first&quot; ## [19] &quot;Then&quot; &quot;statement&quot; ## [21] &quot;somewhat&quot; &quot;detail&quot; ## [23] &quot;course&quot; &quot;be&quot; ## [25] &quot;pursued&quot; &quot;seemed&quot; ## [27] &quot;fitting&quot; &quot;proper&quot; ## [29] &quot;Now&quot; &quot;at&quot; 5.3 tidytext So the tidytext package provides some accomodations to convert your body of text into individual tokens which then simplfies the removal of less meaningful words and the creation of word frequency counts. The first thing you do is to create a data frame where the there is one line for each body of text. In this case we have only one long string of text this will be a one line data frame. library(tidytext) library(tidyr) text_df &lt;- data_frame(line = 1:length(lincoln_doc), text = lincoln_doc) text_df ## # A tibble: 1 × 2 ## line text ## &lt;int&gt; &lt;chr&gt; ## 1 1 &quot;TranscriptTranscriptFellow-Countrymen: At this second appearing to take the oath of th… The next step is to breakup each of text lines (we have only 1) into invdividual rows, each with it’s own line. We also want to count the number of times that each word appears. This is known as tokenizing the data frame. token_text &lt;- text_df %&gt;% unnest_tokens(word, text) # Let&#39;s now count them token_text %&gt;% count(word,sort=TRUE) ## # A tibble: 339 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 58 ## 2 to 27 ## 3 and 24 ## 4 of 22 ## 5 it 13 ## 6 that 12 ## 7 war 12 ## 8 all 10 ## 9 for 9 ## 10 in 9 ## # … with 329 more rows But we need to get rid of the “stop words.” It’s a good thing that the tidytext package has a way to filter out the common words that do not significantly contribute to the meaning of the overall text. The stop_words data frame is built into tidytext. Take a look to see some of the words contained therein: data(stop_words) # Sample 40 random stop words stop_words %&gt;% sample_n(40) ## # A tibble: 40 × 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 go SMART ## 2 we SMART ## 3 up snowball ## 4 great onix ## 5 anyone onix ## 6 we&#39;ll snowball ## 7 smallest onix ## 8 changes SMART ## 9 various SMART ## 10 possible onix ## # … with 30 more rows # Now remove stop words from the document tidy_text &lt;- token_text %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; # This could also be done by the following. I point this out only because some people react # negatively to &quot;joins&quot; although fully understanding what joins are can only help you since # much of what the dplyr package does is based on SQL type joins. tidy_text &lt;- token_text %&gt;% filter(!word %in% stop_words$word) tidy_text %&gt;% count(word,sort=TRUE) ## # A tibble: 193 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 war 12 ## 2 god 5 ## 3 union 4 ## 4 offenses 3 ## 5 woe 3 ## 6 address 2 ## 7 ago 2 ## 8 altogether 2 ## 9 answered 2 ## 10 cease 2 ## # … with 183 more rows tidy_text %&gt;% count(word,sort=TRUE) ## # A tibble: 193 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 war 12 ## 2 god 5 ## 3 union 4 ## 4 offenses 3 ## 5 woe 3 ## 6 address 2 ## 7 ago 2 ## 8 altogether 2 ## 9 answered 2 ## 10 cease 2 ## # … with 183 more rows tidy_text %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 2) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() 5.4 Back To The PubMed Example We have around 935 abstracts that we mess with based on our work using the easyPubMed package # Create a data frame out of the cleaned up abstracts library(tidytext) library(dplyr) text_df &lt;- data_frame(line = 1:length(my_abstracts), text = my_abstracts) token_text &lt;- text_df %&gt;% unnest_tokens(word, text) # Many of these words aren&#39;t helpful token_text %&gt;% count(total=word,sort=TRUE) ## # A tibble: 6,936 × 2 ## total n ## &lt;chr&gt; &lt;int&gt; ## 1 the 3062 ## 2 of 2896 ## 3 and 2871 ## 4 in 1915 ## 5 to 1884 ## 6 a 1373 ## 7 dialysis 1365 ## 8 patients 1335 ## 9 home 1281 ## 10 with 1035 ## # … with 6,926 more rows # Now remove stop words data(stop_words) tidy_text &lt;- token_text %&gt;% anti_join(stop_words) # This could also be done by the following. I point this out only because some people react # negatively to &quot;joins&quot; although fully understanding what joins are can only help you since # much of what the dplyr package does is based on SQL type joins. tidy_text &lt;- token_text %&gt;% filter(!word %in% stop_words$word) # Arrange the text by descending word frequency tidy_text %&gt;% count(word, sort = TRUE) ## # A tibble: 6,460 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 dialysis 1365 ## 2 patients 1335 ## 3 home 1281 ## 4 hemodialysis 674 ## 5 hd 463 ## 6 hhd 440 ## 7 patient 395 ## 8 pd 303 ## 9 renal 279 ## 10 study 268 ## # … with 6,450 more rows Some of the most frequently occurring words are in fact “dialysis,” “patients” so maybe we should consider them to be stop words also since we already know quite well that the overall theme is, well, dialysis and kidneys. There are also synonymns and abbreviations that are somewhat redundant such as “pdd,”“pd,”“hhd” so let’s eliminate them also. tidy_text &lt;- token_text %&gt;% filter(!word %in% c(stop_words$word,&quot;dialysis&quot;,&quot;patients&quot;,&quot;home&quot;,&quot;kidney&quot;, &quot;hemodialysis&quot;,&quot;haemodialysis&quot;,&quot;patient&quot;,&quot;hhd&quot;, &quot;pd&quot;,&quot;peritoneal&quot;,&quot;hd&quot;,&quot;renal&quot;,&quot;study&quot;,&quot;care&quot;, &quot;ci&quot;,&quot;chd&quot;,&quot;nhd&quot;,&quot;disease&quot;,&quot;treatment&quot;)) tidy_text %&gt;% count(word, sort = TRUE) ## # A tibble: 6,441 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 therapy 193 ## 2 conventional 191 ## 3 survival 191 ## 4 center 186 ## 5 compared 180 ## 6 clinical 175 ## 7 nocturnal 171 ## 8 outcomes 171 ## 9 quality 171 ## 10 data 161 ## # … with 6,431 more rows Let’s do some plotting of these words library(ggplot2) tidy_text %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 120) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() Okay, it looks like there are numbers in there which might be useful. I suspect that the “95” is probably associated with the idea of a confidence interval. But there are other references to numbers. grep(&quot;^[0-9]{1,3}$&quot;,tidy_text$word)[1:20] ## [1] 9 273 275 284 288 293 296 305 308 387 388 554 614 671 679 680 682 744 758 762 tidy_text_nonum &lt;- tidy_text[grep(&quot;^[0-9]{1,3}$&quot;,tidy_text$word,invert=TRUE),] Okay well I think maybe we have some reasonable data to examine. As you might have realized by now, manipulating data to get it “clean” can be tedious and frustrating though it is an inevitable part of the process. tidy_text_nonum %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 120) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() 5.4.1 How Do You Feel ? The next step is to explore what some of these words might mean. The tidytext package has four dictionaries that help you figure out what sentiment is being expressed by your data frame. # NRC Emotion Lexicon from Saif Mohammad and Peter Turney get_sentiments(&quot;nrc&quot;) %&gt;% sample_n(20) ## # A tibble: 20 × 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 dictionary positive ## 2 defy negative ## 3 banshee sadness ## 4 foreboding anticipation ## 5 rollicking joy ## 6 bereavement sadness ## 7 elated positive ## 8 symphony anticipation ## 9 scoff negative ## 10 circumvention negative ## 11 aggressive negative ## 12 jab anger ## 13 sweet positive ## 14 abuse fear ## 15 pernicious fear ## 16 consecration positive ## 17 ken positive ## 18 excuse negative ## 19 killing anger ## 20 shiver anticipation # the sentiment lexicon from Bing Liu and collaborators get_sentiments(&quot;bing&quot;) %&gt;% sample_n(20) ## # A tibble: 20 × 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 cruelties negative ## 2 concerns negative ## 3 defiantly negative ## 4 snazzy positive ## 5 nobly positive ## 6 paradise positive ## 7 liberation positive ## 8 sued negative ## 9 tangled negative ## 10 lawless negative ## 11 fastest positive ## 12 unraveled negative ## 13 desperately negative ## 14 crowded negative ## 15 fortuitously positive ## 16 get-rich negative ## 17 believable positive ## 18 dripped negative ## 19 smoother positive ## 20 auspicious positive # Tim Loughran and Bill McDonald get_sentiments(&quot;loughran&quot;) %&gt;% sample_n(20) ## # A tibble: 20 × 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 probationary litigious ## 2 recuses litigious ## 3 accomplishment positive ## 4 boosted positive ## 5 covenanting constraining ## 6 delinquencies negative ## 7 arrearage negative ## 8 suspension negative ## 9 adjourns litigious ## 10 renegotiated negative ## 11 recalling negative ## 12 mistakenly negative ## 13 endorsee litigious ## 14 prejudices litigious ## 15 improving positive ## 16 disinterested negative ## 17 problems negative ## 18 wilful litigious ## 19 delists negative ## 20 adversaries negative # Pull out words that correspond to joy nrc_joy &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;joy&quot;) nrc_joy ## # A tibble: 689 × 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 absolution joy ## 2 abundance joy ## 3 abundant joy ## 4 accolade joy ## 5 accompaniment joy ## 6 accomplish joy ## 7 accomplished joy ## 8 achieve joy ## 9 achievement joy ## 10 acrobat joy ## # … with 679 more rows So we will use the nrc sentiment dictionary to see the “sentiment” expressed in our abstracts. bing_word_counts &lt;- tidy_text_nonum %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) %&gt;% count(word,sentiment,sort=TRUE) ## Joining, by = &quot;word&quot; t the positive vs negative words bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() ## Selecting by n Let’s create a word cloud library(wordcloud) # tidy_text_nonum %&gt;% count(word) %&gt;% with(wordcloud(word,n,max.words=90,scale=c(4,.5),colors=brewer.pal(8,&quot;Dark2&quot;))) 5.5 BiGrams Let’s look at bigrams. We need to go back to the cleaned abstracts and pair words to get phrase that might be suggestive of some sentiment text_df &lt;- data_frame(line = 1:length(my_abstracts), text = my_abstracts) dialysis_bigrams &lt;- text_df %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) dialysis_bigrams %&gt;% count(bigram, sort = TRUE) ## # A tibble: 41,738 × 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 in the 382 ## 2 of the 310 ## 3 home dialysis 300 ## 4 home hemodialysis 279 ## 5 of home 195 ## 6 peritoneal dialysis 193 ## 7 associated with 174 ## 8 home hd 153 ## 9 home haemodialysis 144 ## 10 in center 144 ## # … with 41,728 more rows But we have to filter out stop words library(tidyr) bigrams_sep &lt;- dialysis_bigrams %&gt;% separate(bigram,c(&quot;word1&quot;,&quot;word2&quot;),sep=&quot; &quot;) stop_list &lt;- c(stop_words$word,&quot;dialysis&quot;,&quot;patients&quot;,&quot;home&quot;,&quot;kidney&quot;, &quot;hemodialysis&quot;,&quot;haemodialysis&quot;,&quot;treatment&quot;,&quot;patient&quot;,&quot;hhd&quot;, &quot;pd&quot;,&quot;peritoneal&quot;,&quot;hd&quot;,&quot;renal&quot;,&quot;study&quot;,&quot;care&quot;, &quot;ci&quot;,&quot;chd&quot;,&quot;nhd&quot;,&quot;esrd&quot;,&quot;lt&quot;,&quot;95&quot;,&quot;0.001&quot;) bigrams_filtered &lt;- bigrams_sep %&gt;% filter(!word1 %in% stop_list) %&gt;% filter(!word2 %in% stop_list) bigram_counts &lt;- bigrams_filtered %&gt;% count(word1, word2, sort = TRUE) bigrams_united &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) bigrams_united %&gt;% count(bigram, sort = TRUE) %&gt;% print(n=25) ## # A tibble: 11,842 × 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 replacement therapy 71 ## 2 vascular access 65 ## 3 technique failure 54 ## 4 confidence interval 41 ## 5 left ventricular 39 ## 6 blood pressure 36 ## 7 short daily 35 ## 8 clinical outcomes 33 ## 9 thrice weekly 30 ## 10 technique survival 29 ## 11 hazard ratio 26 ## 12 quality improvement 26 ## 13 adverse events 22 ## 14 6 months 21 ## 15 access related 21 ## 16 arteriovenous fistula 21 ## 17 12 months 19 ## 18 ventricular mass 18 ## 19 3 times 15 ## 20 buttonhole cannulation 15 ## 21 cost effective 15 ## 22 observational studies 15 ## 23 retrospective cohort 15 ## 24 cost effectiveness 14 ## 25 daily life 14 ## # … with 11,817 more rows library(tidyquant) bigram_counts %&gt;% filter(n &gt; 30) %&gt;% ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) + geom_tile(alpha = 0.8, color = &quot;white&quot;) + scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) + coord_flip() + theme_tq() + theme(legend.position = &quot;right&quot;) + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + labs(x = &quot;first word in pair&quot;, y = &quot;second word in pair&quot;) "],["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
