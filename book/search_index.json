[
["bagofwords.html", "Chapter 5 Bag of Words Sentiment Analysis 5.1 Workflow 5.2 Simple Example 5.3 tidytext 5.4 Back To The PubMed Example 5.5 BiGrams", " Chapter 5 Bag of Words Sentiment Analysis One we have a collection of text it’s interesting to figure out what it might mean or infer - if anything at all. In text analysis and NLP (Natural Language Processing) we talk about “Bag of Words” to describe a collection or “corpus” of unstructured text. What do we do with a “bag of words” ? Extract meaning from collections of text (without reading !) Detect and analyze patterns in unstructured textual collections Use Natural Language Processing techniques to reach conclusions Discover what ideas occur in text and how they might be linked Determine if the discovered patterns be used to predict behavior ? Identify interesting ideas that might otherwise be ignored 5.1 Workflow Identify and Obtain text (e.g. websites, Twitter, Databases, PDFs, surveys) Create a text ”Corpus”- a structure that contains the raw text Apply transformations: Normalize case (convert to lower case) Remove puncutation and stopwords Remove domain specific stopwords Perform Analysis and Visualizations (word frequency, tagging, wordclouds) Do Sentiment Analysis R has Packages to Help. These are just some of them: QDAP - Quantitative Discourse Package tm - text mining applications within R tidytext - Text Mining using ddplyr and ggplot and tidyverse tools SentimentAnalysis - For Sentiment Analysis However, consider that: Some of these are easier to use than others Some can be kind of a problem to install (e.g. qdap) They all offer similar capabilities We’ll look at tidytext ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 5.2 Simple Example Find the URL for Lincoln’s March 4, 1865 Speech: url &lt;- &quot;https://millercenter.org/the-presidency/presidential-speeches/march-4-1865-second-inaugural-address&quot; library(rvest) ## Loading required package: xml2 lincoln_doc &lt;- read_html(url) %&gt;% html_nodes(&quot;.view-transcript&quot;) %&gt;% html_text() lincoln_doc ## [1] &quot;TranscriptFellow-Countrymen: At this second appearing to take the oath of the Presidential office there is less occasion for an extended address than there was at the first. Then a statement somewhat in detail of a course to be pursued seemed fitting and proper. Now, at the expiration of four years, during which public declarations have been constantly called forth on every point and phase of the great contest which still absorbs the attention and engrosses the energies of the nation, little that is new could be presented. The progress of our arms, upon which all else chiefly depends, is as well known to the public as to myself, and it is, I trust, reasonably satisfactory and encouraging to all. With high hope for the future, no prediction in regard to it is ventured.On the occasion corresponding to this four years ago all thoughts were anxiously directed to an impending civil war. All dreaded it, all sought to avert it. While the inaugural address was being delivered from this place, devoted altogether to saving the Union without war, insurgent agents were in the city seeking to destroy it without war-seeking to dissolve the Union and divide effects by negotiation. Both parties deprecated war, but one of them would make war rather than let the nation survive, and the other would accept war rather than let it perish, and the war came.One-eighth of the whole population were colored slaves, not distributed generally over the Union. but localized in the southern part of it. These slaves constituted a peculiar and powerful interest. All knew that this interest was somehow the cause of the war. To strengthen, perpetuate, and extend this interest was the object for which the insurgents would rend the Union even by war, while the Government claimed no right to do more than to restrict the territorial enlargement of it. Neither party expected for the war the magnitude or the duration which it has already attained. Neither anticipated that the cause of the conflict might cease with or even before the conflict itself should cease. Each looked for an easier triumph, and a result less fundamental and astounding. Both read the same Bible and pray to the same God, and each invokes His aid against the other. It may seem strange that any men should dare to ask a just God&#39;s assistance in wringing their bread from the sweat of other men&#39;s faces, but let us judge not, that we be not judged. The prayers of both could not be answered. That of neither has been answered fully. The Almighty has His own purposes. \\&quot;Woe unto the world because of offenses; for it must needs be that offenses come, but woe to that man by whom the offense cometh.\\&quot; If we shall suppose that American slavery is one of those offenses which, in the providence of God, must needs come, but which, having continued through His appointed time, He now wills to remove, and that He gives to both North and South this terrible war as the woe due to those by whom the offense came, shall we discern therein any departure from those divine attributes which the believers in a living God always ascribe to Him? Fondly do we hope, fervently do we pray, that this mighty scourge of war may speedily pass away. Yet, if God wills that it continue until all the wealth piled by the bondsman&#39;s two hundred and fifty years of unrequited toil shall be sunk, and until every drop of blood drawn with the lash shall be paid by another drawn with the sword, as was said three thousand years ago, so still it must be said \\&quot;the judgments of the Lord are true and righteous altogether.\\&quot;With malice toward none, with charity for all, with firmness in the fight as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nation&#39;s wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.&quot; There are probably lots of words that don’t really “matter” or contribute to the “real” meaning of the speech. word_vec &lt;- unlist(strsplit(lincoln_doc,&quot; &quot;)) word_vec[1:20] ## [1] &quot;TranscriptFellow-Countrymen:&quot; &quot;&quot; ## [3] &quot;At&quot; &quot;this&quot; ## [5] &quot;second&quot; &quot;appearing&quot; ## [7] &quot;to&quot; &quot;take&quot; ## [9] &quot;the&quot; &quot;oath&quot; ## [11] &quot;of&quot; &quot;the&quot; ## [13] &quot;Presidential&quot; &quot;office&quot; ## [15] &quot;there&quot; &quot;is&quot; ## [17] &quot;less&quot; &quot;occasion&quot; ## [19] &quot;for&quot; &quot;an&quot; sort(table(word_vec),decreasing = TRUE)[1:10] ## word_vec ## the to and of that for be in it a ## 54 26 24 22 11 9 8 8 8 7 How do we remove all the uninteresting words ? We could do it manaully # Remove all punctuation marks word_vec &lt;- gsub(&quot;[[:punct:]]&quot;,&quot;&quot;,word_vec) stop_words &lt;- c(&quot;the&quot;,&quot;to&quot;,&quot;and&quot;,&quot;of&quot;,&quot;the&quot;,&quot;for&quot;,&quot;in&quot;,&quot;it&quot;, &quot;a&quot;,&quot;this&quot;,&quot;which&quot;,&quot;by&quot;,&quot;is&quot;,&quot;an&quot;,&quot;hqs&quot;,&quot;from&quot;, &quot;that&quot;,&quot;with&quot;,&quot;as&quot;) for (ii in 1:length(stop_words)) { for (jj in 1:length(word_vec)) { if (stop_words[ii] == word_vec[jj]) { word_vec[jj] &lt;- &quot;&quot; } } } word_vec &lt;- word_vec[word_vec != &quot;&quot;] sort(table(word_vec),decreasing = TRUE)[1:10] ## word_vec ## war all be we but God shall was do let ## 11 8 8 6 5 5 5 5 4 4 word_vec[1:30] ## [1] &quot;TranscriptFellowCountrymen&quot; &quot;At&quot; ## [3] &quot;second&quot; &quot;appearing&quot; ## [5] &quot;take&quot; &quot;oath&quot; ## [7] &quot;Presidential&quot; &quot;office&quot; ## [9] &quot;there&quot; &quot;less&quot; ## [11] &quot;occasion&quot; &quot;extended&quot; ## [13] &quot;address&quot; &quot;than&quot; ## [15] &quot;there&quot; &quot;was&quot; ## [17] &quot;at&quot; &quot;first&quot; ## [19] &quot;Then&quot; &quot;statement&quot; ## [21] &quot;somewhat&quot; &quot;detail&quot; ## [23] &quot;course&quot; &quot;be&quot; ## [25] &quot;pursued&quot; &quot;seemed&quot; ## [27] &quot;fitting&quot; &quot;proper&quot; ## [29] &quot;Now&quot; &quot;at&quot; 5.3 tidytext So the tidytext package provides some accomodations to convert your body of text into individual tokens which then simplfies the removal of less meaningful words and the creation of word frequency counts. The first thing you do is to create a data frame where the there is one line for each body of text. In this case we have only one long string of text this will be a one line data frame. library(tidytext) ## ## Attaching package: &#39;tidytext&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## stop_words library(tidyr) text_df &lt;- data_frame(line = 1:length(lincoln_doc), text = lincoln_doc) ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. text_df ## # A tibble: 1 x 2 ## line text ## &lt;int&gt; &lt;chr&gt; ## 1 1 &quot;TranscriptFellow-Countrymen: At this second appearing to take th… The next step is to breakup each of text lines (we have only 1) into invdividual rows, each with it’s own line. We also want to count the number of times that each word appears. This is known as tokenizing the data frame. token_text &lt;- text_df %&gt;% unnest_tokens(word, text) # Let&#39;s now count them token_text %&gt;% count(word,sort=TRUE) ## # A tibble: 339 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 58 ## 2 to 27 ## 3 and 24 ## 4 of 22 ## 5 it 13 ## 6 that 12 ## 7 war 12 ## 8 all 10 ## 9 for 9 ## 10 in 9 ## # … with 329 more rows But we need to get rid of the “stop words”. It’s a good thing that the tidytext package has a way to filter out the common words that do not significantly contribute to the meaning of the overall text. The stop_words data frame is built into tidytext. Take a look to see some of the words contained therein: data(stop_words) # Sample 40 random stop words stop_words %&gt;% sample_n(40) ## # A tibble: 40 x 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 doing SMART ## 2 having SMART ## 3 best SMART ## 4 give onix ## 5 she SMART ## 6 later SMART ## 7 unless SMART ## 8 with snowball ## 9 likely SMART ## 10 men onix ## # … with 30 more rows # Now remove stop words from the document tidy_text &lt;- token_text %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; # This could also be done by the following. I point this out only because some people react # negatively to &quot;joins&quot; although fully understanding what joins are can only help you since # much of what the dplyr package does is based on SQL type joins. tidy_text &lt;- token_text %&gt;% filter(!word %in% stop_words$word) tidy_text %&gt;% count(word,sort=TRUE) ## # A tibble: 193 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 war 12 ## 2 god 5 ## 3 union 4 ## 4 offenses 3 ## 5 woe 3 ## 6 address 2 ## 7 ago 2 ## 8 altogether 2 ## 9 answered 2 ## 10 cease 2 ## # … with 183 more rows tidy_text %&gt;% count(word,sort=TRUE) ## # A tibble: 193 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 war 12 ## 2 god 5 ## 3 union 4 ## 4 offenses 3 ## 5 woe 3 ## 6 address 2 ## 7 ago 2 ## 8 altogether 2 ## 9 answered 2 ## 10 cease 2 ## # … with 183 more rows tidy_text %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 2) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() 5.4 Back To The PubMed Example We have around 935 abstracts that we mess with based on our work using the easyPubMed package # Create a data frame out of the cleaned up abstracts library(tidytext) library(dplyr) text_df &lt;- data_frame(line = 1:length(my_abstracts), text = my_abstracts) token_text &lt;- text_df %&gt;% unnest_tokens(word, text) # Many of these words aren&#39;t helpful token_text %&gt;% count(total=word,sort=TRUE) ## # A tibble: 6,936 x 2 ## total n ## &lt;chr&gt; &lt;int&gt; ## 1 the 3062 ## 2 of 2896 ## 3 and 2871 ## 4 in 1915 ## 5 to 1884 ## 6 a 1373 ## 7 dialysis 1365 ## 8 patients 1335 ## 9 home 1281 ## 10 with 1035 ## # … with 6,926 more rows # Now remove stop words data(stop_words) tidy_text &lt;- token_text %&gt;% anti_join(stop_words) # This could also be done by the following. I point this out only because some people react # negatively to &quot;joins&quot; although fully understanding what joins are can only help you since # much of what the dplyr package does is based on SQL type joins. tidy_text &lt;- token_text %&gt;% filter(!word %in% stop_words$word) # Arrange the text by descending word frequency tidy_text %&gt;% count(word, sort = TRUE) ## # A tibble: 6,460 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 dialysis 1365 ## 2 patients 1335 ## 3 home 1281 ## 4 hemodialysis 674 ## 5 hd 463 ## 6 hhd 440 ## 7 patient 395 ## 8 pd 303 ## 9 renal 279 ## 10 study 268 ## # … with 6,450 more rows Some of the most frequently occurring words are in fact “dialysis”, “patients” so maybe we should consider them to be stop words also since we already know quite well that the overall theme is, well, dialysis and kidneys. There are also synonymns and abbreviations that are somewhat redundant such as “pdd”,“pd”,“hhd” so let’s eliminate them also. tidy_text &lt;- token_text %&gt;% filter(!word %in% c(stop_words$word,&quot;dialysis&quot;,&quot;patients&quot;,&quot;home&quot;,&quot;kidney&quot;, &quot;hemodialysis&quot;,&quot;haemodialysis&quot;,&quot;patient&quot;,&quot;hhd&quot;, &quot;pd&quot;,&quot;peritoneal&quot;,&quot;hd&quot;,&quot;renal&quot;,&quot;study&quot;,&quot;care&quot;, &quot;ci&quot;,&quot;chd&quot;,&quot;nhd&quot;,&quot;disease&quot;,&quot;treatment&quot;)) tidy_text %&gt;% count(word, sort = TRUE) ## # A tibble: 6,441 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 therapy 193 ## 2 conventional 191 ## 3 survival 191 ## 4 center 186 ## 5 compared 180 ## 6 clinical 175 ## 7 nocturnal 171 ## 8 outcomes 171 ## 9 quality 171 ## 10 data 161 ## # … with 6,431 more rows Let’s do some plotting of these words library(ggplot2) tidy_text %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 120) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() Okay, it looks like there are numbers in there which might be useful. I suspect that the “95” is probably associated with the idea of a confidence interval. But there are other references to numbers. grep(&quot;^[0-9]{1,3}$&quot;,tidy_text$word)[1:20] ## [1] 9 273 275 284 288 293 296 305 308 387 388 554 614 671 679 680 682 ## [18] 744 758 762 tidy_text_nonum &lt;- tidy_text[grep(&quot;^[0-9]{1,3}$&quot;,tidy_text$word,invert=TRUE),] Okay well I think maybe we have some reasonable data to examine. As you might have realized by now, manipulating data to get it “clean” can be tedious and frustrating though it is an inevitable part of the process. tidy_text_nonum %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 120) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() 5.4.1 How Do You Feel ? The next step is to explore what some of these words might mean. The tidytext package has four dictionaries that help you figure out what sentiment is being expressed by your data frame. # NRC Emotion Lexicon from Saif Mohammad and Peter Turney get_sentiments(&quot;nrc&quot;) %&gt;% sample_n(20) ## # A tibble: 20 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 rehabilitation positive ## 2 troublesome anger ## 3 completeness positive ## 4 disgraced negative ## 5 humanity joy ## 6 respecting positive ## 7 architecture trust ## 8 lose disgust ## 9 civil positive ## 10 phantom fear ## 11 swelling fear ## 12 cardiomyopathy fear ## 13 violence anger ## 14 payment negative ## 15 board anticipation ## 16 helmet fear ## 17 prudent trust ## 18 panier positive ## 19 glimmer positive ## 20 cove anticipation # the sentiment lexicon from Bing Liu and collaborators get_sentiments(&quot;bing&quot;) %&gt;% sample_n(20) ## # A tibble: 20 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 stately positive ## 2 loot negative ## 3 leakage negative ## 4 cheer positive ## 5 unfamiliar negative ## 6 pleasure positive ## 7 led positive ## 8 stresses negative ## 9 hearten positive ## 10 clunky negative ## 11 hollow negative ## 12 skeletons negative ## 13 unproductive negative ## 14 catchy positive ## 15 tremendously positive ## 16 navigable positive ## 17 fulminate negative ## 18 beg negative ## 19 bravo positive ## 20 meticulous positive # Tim Loughran and Bill McDonald get_sentiments(&quot;loughran&quot;) %&gt;% sample_n(20) ## # A tibble: 20 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 misjudging negative ## 2 abnormal negative ## 3 restate negative ## 4 dissenting negative ## 5 prosecuted litigious ## 6 risky uncertainty ## 7 inconveniences negative ## 8 deviate uncertainty ## 9 suspect negative ## 10 derogating litigious ## 11 controvert litigious ## 12 misusing negative ## 13 dissolution negative ## 14 reassessment uncertainty ## 15 ineffective negative ## 16 improves positive ## 17 provisoes litigious ## 18 disagreeing negative ## 19 interrogators litigious ## 20 outage negative # Pull out words that correspond to joy nrc_joy &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;joy&quot;) nrc_joy ## # A tibble: 689 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 absolution joy ## 2 abundance joy ## 3 abundant joy ## 4 accolade joy ## 5 accompaniment joy ## 6 accomplish joy ## 7 accomplished joy ## 8 achieve joy ## 9 achievement joy ## 10 acrobat joy ## # … with 679 more rows So we will use the nrc sentiment dictionary to see the “sentiment” expressed in our abstracts. bing_word_counts &lt;- tidy_text_nonum %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) %&gt;% count(word,sentiment,sort=TRUE) ## Joining, by = &quot;word&quot; t the positive vs negative words bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() ## Selecting by n Let’s create a word cloud library(wordcloud) # tidy_text_nonum %&gt;% count(word) %&gt;% with(wordcloud(word,n,max.words=90,scale=c(4,.5),colors=brewer.pal(8,&quot;Dark2&quot;))) 5.5 BiGrams Let’s look at bigrams. We need to go back to the cleaned abstracts and pair words to get phrase that might be suggestive of some sentiment text_df &lt;- data_frame(line = 1:length(my_abstracts), text = my_abstracts) dialysis_bigrams &lt;- text_df %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) dialysis_bigrams %&gt;% count(bigram, sort = TRUE) ## # A tibble: 41,738 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 in the 382 ## 2 of the 310 ## 3 home dialysis 300 ## 4 home hemodialysis 279 ## 5 of home 195 ## 6 peritoneal dialysis 193 ## 7 associated with 174 ## 8 home hd 153 ## 9 home haemodialysis 144 ## 10 in center 144 ## # … with 41,728 more rows But we have to filter out stop words library(tidyr) bigrams_sep &lt;- dialysis_bigrams %&gt;% separate(bigram,c(&quot;word1&quot;,&quot;word2&quot;),sep=&quot; &quot;) stop_list &lt;- c(stop_words$word,&quot;dialysis&quot;,&quot;patients&quot;,&quot;home&quot;,&quot;kidney&quot;, &quot;hemodialysis&quot;,&quot;haemodialysis&quot;,&quot;treatment&quot;,&quot;patient&quot;,&quot;hhd&quot;, &quot;pd&quot;,&quot;peritoneal&quot;,&quot;hd&quot;,&quot;renal&quot;,&quot;study&quot;,&quot;care&quot;, &quot;ci&quot;,&quot;chd&quot;,&quot;nhd&quot;,&quot;esrd&quot;,&quot;lt&quot;,&quot;95&quot;,&quot;0.001&quot;) bigrams_filtered &lt;- bigrams_sep %&gt;% filter(!word1 %in% stop_list) %&gt;% filter(!word2 %in% stop_list) bigram_counts &lt;- bigrams_filtered %&gt;% count(word1, word2, sort = TRUE) bigrams_united &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) bigrams_united %&gt;% count(bigram, sort = TRUE) %&gt;% print(n=25) ## # A tibble: 11,842 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 replacement therapy 71 ## 2 vascular access 65 ## 3 technique failure 54 ## 4 confidence interval 41 ## 5 left ventricular 39 ## 6 blood pressure 36 ## 7 short daily 35 ## 8 clinical outcomes 33 ## 9 thrice weekly 30 ## 10 technique survival 29 ## 11 hazard ratio 26 ## 12 quality improvement 26 ## 13 adverse events 22 ## 14 6 months 21 ## 15 access related 21 ## 16 arteriovenous fistula 21 ## 17 12 months 19 ## 18 ventricular mass 18 ## 19 3 times 15 ## 20 buttonhole cannulation 15 ## 21 cost effective 15 ## 22 observational studies 15 ## 23 retrospective cohort 15 ## 24 cost effectiveness 14 ## 25 daily life 14 ## # … with 1.182e+04 more rows library(tidyquant) ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 bigram_counts %&gt;% filter(n &gt; 30) %&gt;% ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) + geom_tile(alpha = 0.8, color = &quot;white&quot;) + scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) + coord_flip() + theme_tq() + theme(legend.position = &quot;right&quot;) + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + labs(x = &quot;first word in pair&quot;, y = &quot;second word in pair&quot;) "]
]
